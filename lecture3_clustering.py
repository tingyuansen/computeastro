import streamlit as st
import streamlit.components.v1 as components

def show_page():

    # Page Title
    st.title('Unsupervised Learning: Clustering - K-means and Gaussian Mixture Models')

    # # Embed the external HTML page
    # st.info('''
    # Course Slides
    # ''')
    # slideshare_embed_code = """
    #     <iframe src="https://www.slideshare.net/slideshow/embed_code/key/lO9gIfiYtiNP15" 
    #             width="700" height="394" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" 
    #             style="border:0px solid #000; border-width:0px; margin-bottom:0px; max-width: 100%;" allowfullscreen> 
    #     </iframe> 
    # """
    # components.html(slideshare_embed_code, height=412)

    # st.markdown('---')
    
    st.markdown(r'''''')
    st.markdown(r'''In our last lecture, we discussed logistic regression, which is a discriminant function used to separate classes when we have labeled data. Today, we're going to extend that concept to what we call generative models in the context of unsupervised learning. ''')
    st.markdown(r'''''')
    st.markdown(r'''There are two ways to think about logistic regression. One is to treat it as a discriminant function, meaning we're training a discriminative model. The other, as we discussed, is to use Bayes' theorem to recast it as a generative model that captures the distribution of features. We used the example of distinguishing between Portuguese and German sentences. While it's possible to do this classification without speaking the languages, being able to speak the languages makes it much easier, and in machine learning lingo, these are generative models. This idea leads us into today's topic of unsupervised learning.''')
    st.markdown(r'''''')
    st.markdown(r'''In the past two lectures, we focused on supervised learning, where we have input features $X$ and target labels $Y$. Today and in our next lecture, we'll explore unsupervised learning, which has different tasks and objectives. For today, we'll concentrate on clustering objects and density estimation of the feature space. In our next session, we'll cover dimension reduction, specifically focusing on Principal Component Analysis (PCA).''')
    st.markdown(r'''''')
    st.markdown(r'''## Clustering Algorithms: K-means and Gaussian Mixture Models''')
    st.markdown(r'''''')
    st.markdown(r'''We'll discuss two main clustering algorithms today. We'll start with the K-means algorithm, which is simple and scalable but lacks a strong probabilistic foundation. Then we'll move on to Gaussian Mixture Models, which are more statistically rigorous and better suited for probabilistic inference in many scientific applications.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, I'll be honest - I'm not a big fan of K-means because it doesn't align well with the statistical probabilistic approach to inferences that we want to promote in this course. However, understanding K-means, especially its optimization through the Expectation-Maximization algorithm, will help us grasp the more complex Gaussian mixture models. So we'll use K-means as a stepping stone to build intuition.''')
    st.markdown(r'''''')
    st.markdown(r'''## A Simple Example: Kangaroos and Wallabies''')
    st.markdown(r'''''')
    st.markdown(r'''Let's consider a simple example: distinguishing between kangaroos and wallabies based on their length. Imagine we measure the lengths of these animals and plot the distribution. We might observe two peaks: one for shorter wallabies and another for longer kangaroos.''')
    st.markdown(r'''''')
    st.markdown(r'''This scenario highlights key aspects of unsupervised learning. We don't have pre-existing labels, we don't know the number of species (or classes) in advance, and our goal is to identify substructures in the data. In this case, the distribution might be modeled as the sum of two Gaussians, suggesting two dominant species.''')
    st.markdown(r'''''')
    st.markdown(r'''## The Importance of Unsupervised Learning in Science''')
    st.markdown(r'''''')
    st.markdown(r'''Unsupervised learning is crucial in many scientific fields because we often lack labels for our data. Many pursuits involve determining the number of types of objects we have or discovering entirely new classes based on unexplored feature spaces. While unsupervised learning may not always provide ground truth, it's invaluable for building intuition and generating hypotheses in scientific research.''')
    st.markdown(r'''''')
    st.markdown(r'''So, while it might not give us definitive answers, unsupervised learning is a powerful tool. It helps us formulate hypotheses and explore our data in ways that can lead to new discoveries. That's why we're dedicating time to understand these methods.''')
    st.markdown(r'''''')
    st.markdown(r'''## Unsupervised Learning: Features and Dimensionality''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's delve deeper into unsupervised learning and its applications. As we mentioned, the key difference between unsupervised learning, which we'll cover in this lecture and the next, and the supervised learning methods like linear regression and logistic regression that we discussed in the last two lectures, is the absence of target labels $Y$. In unsupervised learning, we only have the features to work with.''')
    st.markdown(r'''''')
    st.markdown(r'''These features, which we denote as $X$, can be high-dimensional. In many scientific contexts, $D$, the number of dimensions, can be very large. For instance, when dealing with spectra, each individual $X$ might represent thousands or even tens of thousands of dimensions. Our goal here is to partition all these features into meaningful clusters.''')
    st.markdown(r'''''')
    st.markdown(r'''To illustrate this concept, let's consider a simple example. Even by eye, you can see that we can separate the ''')
    st.markdown(r'''data points into two distinct clusters. However, the real challenge arises when we deal with higher dimensions - ''')
    st.markdown(r'''three, four, or more - where visualization becomes difficult. This is why we need algorithms to help us perform ''')
    st.markdown(r'''clustering, to identify patterns that might not be immediately apparent to the human eye.''')
    st.markdown(r'''''')
    st.markdown(r'''For the purposes of building intuition, including in today's tutorial, we'll limit ourselves to 2D examples. But ''')
    st.markdown(r'''keep in mind that these methods can be easily generalized to very high dimensions.''')
    st.markdown(r'''''')
    st.markdown(r'''## Application in Galactic Archaeology''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's talk about a particular use case: looking for substructure in the kinematic space of stars. This is an area that's become quite common in galactic archaeology, which is a significant part of modern astrophysics research.''')
    st.markdown(r'''''')
    st.markdown(r'''When we observe stars and analyze their orbits, looking at their energy and angular momenta, we find that the orbits of stars seem to follow some interesting clustering patterns in this space. If you squint your eyes, you might notice various substructures. Of course, it's challenging to see all of these by eye, but researchers have proposed that these substructures are real based on the sub-clustering they observe.''')
    st.markdown(r'''''')
    st.markdown(r'''To confirm these hypotheses, astronomers conduct follow-up studies, such as examining the chemical properties of these stars. These studies have often confirmed that many of these substructures are indeed remnants of merger systems - smaller galaxies or stellar systems that interacted with the proto-Milky Way and subsequently merged with it.''')
    st.markdown(r'''''')
    st.markdown(r'''This example highlights the importance of unsupervised learning in astronomy. A priori, we might not know how many clusters or groups exist in our data. But by separating our features into interesting regions in a generative way, we can build a deeper understanding of the astronomical world we live in.''')
    st.markdown(r'''''')
    st.markdown(r'''## K-means Algorithm: A Simple Clustering Approach''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's delve into the simplest clustering algorithm of them all: the K-means algorithm. We'll use this as a straightforward example to build some intuition about clustering methods in general.''')
    st.markdown(r'''''')
    st.markdown(r'''The fundamental assumption in clustering is that we're trying to find data points that are close to each other. In the next few slides, we'll talk about what we mean by "close" in a more mathematical sense, but for now, let's take this concept at face value.''')
    st.markdown(r'''''')
    st.markdown(r'''It's important to keep in mind that in any clustering task, there are two main things we're trying to optimize. One is the description of the cluster. In the case of K-means, we denote each cluster by its mean, which we call $\mu_k$ (mu sub k). Of course, $\mu_k$ exists in the same dimensional space as our features. If we have two features, the cluster center will also be two-dimensional.''')
    st.markdown(r'''''')
    st.markdown(r'''But besides describing the clusters, perhaps more importantly, we also want to make assignments of data points to clusters. This is a crucial point to remember: clustering is more than just optimizing the description of the clusters; it's also about determining which data points belong to which cluster.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's break down the K-means algorithm at a high level:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Initialization: Since we don't know how to describe the clusters beforehand, we arbitrarily choose some locations in our feature space to serve as initial cluster centers.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Assignment: Given these initial centers, we assign each data point to the nearest cluster center.''')
    st.markdown(r'''''')
    st.markdown(r'''3. Update: After making these assignments, we update the description of each cluster. Specifically, we recalculate the cluster centers as the mean of all data points assigned to that cluster.''')
    st.markdown(r'''''')
    st.markdown(r'''4. Iteration: We then iterate between the assignment and update steps. We reassign points based on the updated cluster centers, then update the centers based on the new assignments.''')
    st.markdown(r'''''')
    st.markdown(r'''5. Convergence: We continue this process until we reach convergence. Convergence here means that further repetitions of this process no longer change either the centers of the clusters or the assignment of data points to clusters.''')
    st.markdown(r'''''')
    st.markdown(r'''This iterative process highlights that we're optimizing two types of variables: the cluster descriptions (centers) and the data point assignments. We alternate between optimizing these two aspects until we reach a stable solution.''')
    st.markdown(r'''''')
    st.markdown(r'''## K-means in Practice: A Simple Example''')
    st.markdown(r'''''')
    st.markdown(r'''Now that we've covered the basic concept of K-means, let's build some intuition by looking at a simple case. Imagine we have a dataset represented in 2D space, and we want to separate these points into two clusters. How would we proceed with K-means?''')
    st.markdown(r'''''')
    st.markdown(r'''The first step in K-means is to initialize the cluster centers. While you could theoretically choose any location within the feature space, a practical approach is to select two data points and designate them as initial cluster centers. Let's call these centers $\mu_1$ and $\mu_2$.''')
    st.markdown(r'''''')
    st.markdown(r'''Once we have our initial centers, we move on to the assignment step. For each data point, we calculate its distance to each cluster center and assign it to the closest one. For example, if we take this particular data point [gesturing to a point on the slide], we'd calculate its distance to both the red cross ($\mu_1$) and the green cross ($\mu_2$). If it's closer to the green cross, we assign it to the green cluster, and vice versa.''')
    st.markdown(r'''''')
    st.markdown(r'''Of course, our initial cluster centers are unlikely to be optimal. So, the next step is to update them. In K-means, we do this by calculating the mean location of all points assigned to each cluster and setting this as the new cluster center. This process is then repeated: we reassign points based on the new centers, then update the centers based on the new assignments, iterating until convergence.''')
    st.markdown(r'''''')
    st.markdown(r'''## Challenges with K-means: Local Minima''')
    st.markdown(r'''''')
    st.markdown(r'''This approach seems intuitive, and we'll discuss in later slides why it's mathematically sound. However, there's a catch: this optimization process can fall into local minima. Let me illustrate this with an example.''')
    st.markdown(r'''''')
    st.markdown(r'''[Referring to the slide] Let's say we initialize our centers here and here within this blob of data. After the assignment step, we might find that half the points are closer to the green center and half to the yellow. We then update our centers and repeat the process. After some iterations, it might converge to something like what we see on the right of this slide. Clearly, this isn't an optimal clustering.''')
    st.markdown(r'''''')
    st.markdown(r'''When we say "not optimal," we need to define our objective function. In clustering, a natural objective is to minimize the sum of distances between each point and its assigned cluster center. In this suboptimal result, some data points are assigned to the cyan cluster but are very far from its center. The situation seems even worse for the yellow cluster.''')
    st.markdown(r'''''')
    st.markdown(r'''However, if we start with a different initialization, we might get much better results. [Referring to the next slide] If we initialize our centers in more appropriate locations, after some iterations, we end up with clusters that match our intuitive expectations. This result is more optimal because the members of each cluster are relatively close to their respective centers.''')
    st.markdown(r'''''')
    st.markdown(r'''This sensitivity to initialization means that with K-means, we often need to run the algorithm multiple times with different starting points. Fortunately, this isn't a major drawback, primarily because K-means is very computationally efficient. The main operation is calculating distances between points and cluster centers, which has a complexity of $K * N$, where $K$ is the number of clusters and $N$ is the number of data points. This is a relatively cheap operation computationally, avoiding more expensive $O(N^2)$ or $O(K^2)$ calculations.''')
    st.markdown(r'''''')
    st.markdown(r'''In practice, this means we can afford to run K-means multiple times with different initializations, choosing the result that gives us the lowest overall within-cluster distance sum. This approach helps us avoid getting stuck in poor local minima and increases our chances of finding a good clustering solution.''')
    st.markdown(r'''''')
    st.markdown(r'''## Mathematical Formulation of K-means''')
    st.markdown(r'''''')
    st.markdown(r'''Now that we've seen a pictorial view of K-means, let's dive deeper into its characteristics, mathematical formulation, and optimization process.''')
    st.markdown(r'''''')
    st.markdown(r'''First, let's note some key observations about K-means:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Linear Boundaries: You might have noticed that the boundaries between clusters are always somewhat linear. This is very similar to logistic regression, which isn't too surprising. In K-means, we're assuming that points are isotropically close to their cluster centers, essentially assuming diagonal covariances. Even though we don't necessarily assume the two clusters have the same covariances, if we further assume that the distances have the same covariances, we get back to something very close to logistic regression. This is why the boundary is also a linear line.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Generative vs. Discriminative: There's a key difference here. In logistic regression, we have a discriminative model trying to describe just the boundary. But in K-means, it's more like a generative model because we're trying to describe the distribution of the data, the distribution of the features in the X space.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's formulate the K-means algorithm mathematically. Remember, we need to optimize two things: the cluster assignments and the properties of the clusters (their centers).''')
    st.markdown(r'''''')
    st.markdown(r'''For cluster assignments, we use one-hot encoding, similar to what we introduced in logistic regression. For the nth data point, we have a K-dimensional vector where only one entry is 1 and the rest are 0. The position of the 1 indicates which cluster the data point belongs to. For example, [0, 1, 0] would mean the data point belongs to cluster 2 out of 3 clusters.''')
    st.markdown(r'''''')
    st.markdown(r'''This is what we call "hard assignment" - each point belongs to exactly one cluster. Taking the example of wallabies and kangaroos, we want to say if this is a true kangaroo or a true wallaby. We don't want data points with a 50-50 split or even an 80-20 split.''')
    st.markdown(r'''''')
    st.markdown(r'''Next, let's define our objective function:''')
    st.markdown(r'''''')
    st.markdown(r'''$J = \sum_{n=1}^N \sum_{k=1}^K r_{nk} ||x_n - \mu_k||^2$''')
    st.markdown(r'''''')
    st.markdown(r'''This might look intimidating, but it's straightforward. We're summing over all data points ($N$) and all clusters ($K$), calculating the squared distance between each point and its assigned cluster center. The $r_{nk}$ term ensures we only consider the distance to the assigned cluster.''')
    st.markdown(r'''''')
    st.markdown(r'''If this seems a bit too jargony, think back to the pictorial view we showed earlier. Even though we sum over $k$, it's actually quite redundant because for any given $n$, only one $k$ value is non-zero and the rest are zero. This is just a compact way to write what we're trying to optimize.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, why is this optimization tricky? It might look like a simple chi-squared optimization, but it's not. We're optimizing both $\mu_k$ (cluster centers) and $r_{nk}$ (assignments) simultaneously. We have $N * K$ degrees of freedom for $r_{nk}$, and the hard assignment constraint ($r_{nk}$ must be 0 or 1) makes simple gradient descent problematic.''')
    st.markdown(r'''''')
    st.markdown(r'''## Expectation Maximization (EM) Algorithm for K-means''')
    st.markdown(r'''''')
    st.markdown(r'''Despite these challenges, we can use an algorithm called Expectation Maximization (EM) to find a good solution. The key insight is that if we fix $\mu_k$, optimizing $r_{nk}$ becomes straightforward, and vice versa.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's break down the EM algorithm for K-means:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Expectation Step (E-step):''')
    st.markdown(r'''   Fix $\mu_k$ and optimize $r_{nk}$. This is simple:''')
    st.markdown(r'''   $r_{nk} = \{1 \mathrm{ if } k = \argmin_j ||x_n - \mu_j||^2''')
    st.markdown(r'''           0 \mathrm{{} otherwise{}}$''')
    st.markdown(r'''   We're just assigning each point to its nearest cluster center.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Maximization Step (M-step):''')
    st.markdown(r'''   Fix $r_{nk}$ and optimize $\mu_k$. We do this by differentiating $J$ with respect to $\mu_k$ and setting it to zero:''')
    st.markdown(r'''   $\mu_k = (\sum_n r_{nk} x_n) / (\sum_n r_{nk})$''')
    st.markdown(r'''   This is just the mean of all points assigned to cluster $k$.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's break this down further. If we fix $r_{nk}$, optimizing $\mu_k$ is straightforward. The loss function is quadratic with respect to $\mu_k$, which means it's convex. To find the optimal point, we differentiate $J$ with respect to $\mu_k$ and set the derivative to zero. The derivative of a quadratic function is simple - it becomes two times the linear term. Setting this to zero and rearranging gives us the formula above.''')
    st.markdown(r'''''')
    st.markdown(r'''This might look complicated with the summations and derivatives, but it's actually quite simple. We're taking all the data points that belong to cluster $k$ (where $r_{nk}$ is non-zero), summing their locations, and dividing by the number of members in the cluster. It's just the cluster mean, as we saw in our pictorial explanation.''')
    st.markdown(r'''''')
    st.markdown(r'''We iterate between these two steps until convergence. This process, which seemed intuitive in our pictorial explanation, is actually optimizing our defined objective function.''')
    st.markdown(r'''''')
    st.markdown(r'''A few additional notes:''')
    st.markdown(r'''- We're using Euclidean distance here, but you could use other distance metrics if appropriate for your data.''')
    st.markdown(r'''- The name "Expectation Maximization" comes from statistical theory, but for our purposes, you can think of it as simply iterating between these two optimization steps.''')
    st.markdown(r'''''')
    st.markdown(r'''This formulation of K-means demonstrates why it's such an elegant and computationally efficient algorithm. By alternating between assignment and update steps, we can find a good clustering solution even in high-dimensional spaces, despite the complexity of the optimization problem.''')
    st.markdown(r'''''')
    st.markdown(r'''## Application of K-means: Data Compression''')
    st.markdown(r'''''')
    st.markdown(r'''Even though k-means seems to be a very simple algorithm, it actually has quite a lot of applications. One application is data compression. Let's say I have a photo made up of $N$ pixels. For each pixel, I have three floats representing the RGB values, which I need to save as float64. This is very expensive because I'm saving $N$ pixels $\times 3 \times$ float64. ''')
    st.markdown(r'''''')
    st.markdown(r'''But there is a better way to compress the information if you are willing to sacrifice some fidelity, and that is by k-means. Treating each pixel as a data point in the 3D RGB space, I can do a clustering of $K$ clusters. Essentially, I'm finding $K$ groups of colors in the RGB space. ''')
    st.markdown(r'''''')
    st.markdown(r'''After clustering, I have the mean vector, which is the mean color of each group of pixels. I will color the pixels with their cluster's mean color. You can see there's only one shade of blue instead of a clear gradient.''')
    st.markdown(r'''''')
    st.markdown(r'''By doing this, we are really compressing the information by a lot. The only floats we need to save are the mean vectors of the $K$ clusters ($K \times 3 \times$ float64). This is a very small number. We still need to save an array of $N$ pixels, but for each pixel we only need to save the cluster assignment. With 10 clusters, we can save that as an integer (4 bits).''')
    st.markdown(r'''''')
    st.markdown(r'''For most pixels, we can save the information with a very small number of bits, yet it gives a very good representation of the image. You can argue that this is somewhat an artistic view. It's a fun application - with just a few lines of code, you can start doing this type of thing at home.''')
    st.markdown(r'''''')
    st.markdown(r'''## Introduction to Gaussian Mixture Models''')
    st.markdown(r'''''')
    st.markdown(r'''That concludes our discussion of k-means. Next, we'll move on to something more advanced called Gaussian mixture models. While k-means is very good, it's a very simple model. For one, it gives hard assignments, meaning there's no probabilistic interpretation. A key aspect of modern statistical methods in astronomy is to provide a probabilistic view of our data and models.''')
    st.markdown(r'''''')
    st.markdown(r'''The simplest probabilistic model we can use for clustering is a mixture model. A mixture model means that instead of describing the distribution of features $x$ with a single Gaussian, which would be quite limiting, we can write it as the sum of different Gaussians:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(x) = \sum_{k=1}^K \pi_k \mathcal{N}(x | \mu_k, \Sigma_k)$''')
    st.markdown(r'''''')
    st.markdown(r'''Let's say we have a distribution of features $x_1$ and $x_2$ that looks like a complex, multi-modal shape. Fitting this with a single Gaussian would not do a very good job. But if we are allowed to describe it by the sum of three Gaussians, it will probably do a pretty good job.''')
    st.markdown(r'''''')
    st.markdown(r'''### The Importance of Weights in GMMs''')
    st.markdown(r'''''')
    st.markdown(r'''One catch though - you might naively think you only need to define the mean vectors (locations of the Gaussians) and the covariances (widths and tilts of the Gaussians). But you also need to define the amplitudes of the Gaussians. ''')
    st.markdown(r'''''')
    st.markdown(r'''Looking at the 3D space of $x_1$, $x_2$, and $p(x_1, x_2)$, the different Gaussian modes can have different heights or relative importance in your data space. Therefore, not only do you need to optimize the means and covariances, but also the weights (relative importance) of the individual modes.''')
    st.markdown(r'''''')
    st.markdown(r'''Going back to the mathematics, we assume the distribution of $x$ in this high-dimensional space can be written as a sum of Gaussians. Each Gaussian is characterized by a mean and covariance that we do not know a priori, just like in k-means where we do not know the cluster centers a priori but try to optimize them. We also have the weights $\pi_k$ representing the importance of each mode.''')
    st.markdown(r'''''')
    st.markdown(r'''### Constraint on Weights''')
    st.markdown(r'''''')
    st.markdown(r'''One catch - you might think you can just write this down and it's fine. But as we will see, the loss function optimizes the likelihood, meaning we try to evaluate the data points $x_n$ using this presumed distribution. This leads to a question.''')
    st.markdown(r'''''')
    st.markdown(r'''You might think relative weights like $\pi_k = [0.2, 0.5, 0.3]$ are the same as $\pi_k = [2, 5, 3]$. Yes, in the relative sense they show the correct relative importance. But they would give very different values when evaluating $p(x)$. ''')
    st.markdown(r'''''')
    st.markdown(r'''If you let the $\pi_k$ be unconstrained, in principle you can take $\pi_k$ to infinity and make the likelihood larger and larger. This is not surprising, because $\pi_k$ represents the relative importance of individual modes, like separating a birthday cake. ''')
    st.markdown(r'''''')
    st.markdown(r'''You need to first tell me how big the birthday cake is. It doesn't matter how big the cake is as long as you fix a size. Then we can talk about how we partition it. But if you do not fix the size, it's an ill-defined question.''')
    st.markdown(r'''''')
    st.markdown(r'''In the canonical way, we assume the sum of the weights to be one, meaning we have a cake of size one. This can be any fixed value. Most of the derivations, actually all of them, will be the same as long as this is a fixed value. The specific value is not important as long as we fix the size of the summation of all the weights.''')
    st.markdown(r'''''')
    st.markdown(r'''## Optimization of Gaussian Mixture Models''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's dive into the optimization of Gaussian Mixture Models (GMMs). As we've shown many times in this course, the key idea is to optimize the likelihood. Our goal here is to find the correct models that will optimize the observation.''')
    st.markdown(r'''''')
    st.markdown(r'''In GMMs, we have a set of parameters to optimize: the weights $\pi$, the cluster centers $\mu$, and the cluster covariances $\Sigma$. We've observed a bunch of data points ${x_n}$, and we're trying to optimize the chances of actually observing this data given our model.''')
    st.markdown(r'''''')
    st.markdown(r'''The likelihood for a single data point is simply the evaluation of that data point using our proposed distribution. For multiple data points, assuming independent observations, the joint likelihood is the product of individual likelihoods:''')
    st.markdown(r'''''')
    st.markdown(r'''$p({x_n} | \pi, \mu, \Sigma) = \Pi_{n=1}^N p(x_n | \pi, \mu, \Sigma)$''')
    st.markdown(r'''''')
    st.markdown(r'''However, as we've learned, optimizing products can lead to numerical issues. So, we take the log of the joint distribution, giving us the log-likelihood:''')
    st.markdown(r'''''')
    st.markdown(r'''$\ln p({x_n} | \pi, \mu, \Sigma) = \Sigma_{n=1}^N \ln(\Sigma_{k=1}^K \pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k))$''')
    st.markdown(r'''''')
    st.markdown(r'''This optimization is very tricky because we're dealing with a sum of different Gaussians, and all the parameters have sums of sums. Maximizing this log-likelihood is much more complex than for a single Gaussian. The summation over all components makes the problem highly non-convex, and there's no closed-form analytic solution.''')
    st.markdown(r'''''')
    st.markdown(r'''### Degrees of Freedom in GMMs''')
    st.markdown(r'''''')
    st.markdown(r'''Let's count the degrees of freedom we're dealing with:''')
    st.markdown(r'''- For $\pi_k$: $K$ scalars, so $K$ degrees of freedom''')
    st.markdown(r'''- For $\mu_k$: $K$ $d$-dimensional vectors, so $K * d$ degrees of freedom''')
    st.markdown(r'''- For $\Sigma_k$: $K$ $d×d$ matrices, so $K * d * d$ degrees of freedom''')
    st.markdown(r'''''')
    st.markdown(r'''This high complexity, especially from the covariances, is why GMMs don't scale well to very high-dimensional spaces.''')
    st.markdown(r'''''')
    st.markdown(r'''We could apply gradient descent as we've learned, but that requires some optimization tricks due to the non-convexity. Instead, just like in K-means clustering, the expectation-maximization (EM) technique comes in handy here.''')
    st.markdown(r'''''')
    st.markdown(r'''### Expectation-Maximization for GMMs''')
    st.markdown(r'''''')
    st.markdown(r'''Let's see how this works by differentiating our log-likelihood with respect to $\mu_k$:''')
    st.markdown(r'''''')
    st.markdown(r'''$0 = -\Sigma_{n=1}^N (\pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k)) / (\Sigma_j \pi_j \mathcal{N}(x_n | \mu_j, \Sigma_j)) * \Sigma_k^{-1} (x_n - \mu_k)$''')
    st.markdown(r'''''')
    st.markdown(r'''This looks like a complex non-linear optimization. However, we can simplify it by defining a new variable, $\gamma_{nk}$:''')
    st.markdown(r'''''')
    st.markdown(r'''$\gamma_{nk} = (\pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k)) / (\sum_j \pi_j \mathcal{N}(x_n | \mu_j, \Sigma_j))$''')
    st.markdown(r'''''')
    st.markdown(r'''With this simplification, our optimization equation becomes:''')
    st.markdown(r'''''')
    st.markdown(r'''$0 = \sum_{n=1}^N \gamma_{nk} (x_n - \mu_k)$''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's delve deeper into how we update the parameters in Gaussian Mixture Models (GMMs). We've already discussed the concept of $\gamma_{nk}$, which is similar to $r_{nk}$ in K-means, but with a crucial difference.''')
    st.markdown(r'''''')
    st.markdown(r'''The role of $\gamma_{nk}$ is very similar to $r_{nk}$, denoting the probability that data point $n$ belongs to cluster $k$. We call it the "response" or "responsibility" of cluster $k$ for explaining data point $n$. However, unlike $r_{nk}$ in K-means which is binary (0 or 1), $\gamma_{nk}$ in GMMs can take any fractional value between 0 and 1. This is why we say K-means has "hard" cluster assignments, while GMMs have "soft" assignments.''')
    st.markdown(r'''''')
    st.markdown(r'''It's easy to check that $\sum_k \gamma_{nk} = 1$ for any given $n$. This follows directly from the definition of $\gamma_{nk}$, as the numerator and denominator cancel out when summed over $k$.''')
    st.markdown(r'''''')
    st.markdown(r'''This soft assignment leads to some interesting properties. When calculating the new mean for a cluster, a data point that is partially assigned to multiple clusters only contributes a fraction of its value to each cluster, proportional to its assignment probabilities. Another thing to note is that, strictly speaking, none of the $\gamma_{nk}$ values are exactly zero in GMMs. They can be extremely small, but not strictly zero. Therefore, all data points actually contribute to the update of the mean, even if only slightly.''')
    st.markdown(r'''''')
    st.markdown(r'''### Updating Parameters in GMMs''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's look at how we update the parameters. For the means, if we fix $\gamma_{nk}$, the optimal $\mu_k$ is given by:''')
    st.markdown(r'''''')
    st.markdown(r'''$\mu_k = (\sum_{n=1}^N \gamma_{nk}x_n) / (\sum_{n=1}^N \gamma_{nk})$''')
    st.markdown(r'''''')
    st.markdown(r'''This is very similar to what we saw in K-means, just with soft assignments instead of hard ones.''')
    st.markdown(r'''''')
    st.markdown(r'''For updating the covariances, we differentiate the log-likelihood with respect to $\Sigma_k$. This leads to the update rule:''')
    st.markdown(r'''''')
    st.markdown(r'''$\Sigma_k = (\sum_{n=1}^N \gamma_{nk}(x_n - \mu_k)(x_n - \mu_k)^T) / (\sum_{n=1}^N \gamma_{nk})$''')
    st.markdown(r'''''')
    st.markdown(r'''This is intuitively clear. The term $(x_n - \mu_k)(x_n - \mu_k)^T$ is exactly how you would calculate the covariance. In the 1D case, to calculate the standard deviation, you take the data minus the mean and square it. In the multivariate case, it's the same idea, just now you subtract the mean in the vector space and do the dot product by transposing the vectors.''')
    st.markdown(r'''''')
    st.markdown(r'''Just like with $\mu_k$, different data points have different weightings for this new covariance. If a data point has a low probability of belonging to a certain cluster, it shouldn't contribute much to the dispersion of that cluster. This is why we weight all these covariances by the individual $\gamma_{nk}$ and then divide by the sum of the weights.''')
    st.markdown(r'''''')
    st.markdown(r'''### Lagrange Multipliers''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's talk about updating $\pi_k$. This is slightly trickier because we have a constraint to respect: $\sum_k \pi_k = 1$. We can't just differentiate the log-likelihood with respect to $\pi_k$ and set it to zero, because $\pi_k$ needs to satisfy this constraint.''')
    st.markdown(r'''''')
    st.markdown(r'''This is where we need to use a technique called the Lagrange multiplier method. If you haven't learned this in your calculus courses, don't worry - it's still quite straightforward. The basic idea is this: if you have a function $f(x)$ that you want to optimize, but $x$ needs to satisfy a constraint $g(x) = 0$, then you introduce an auxiliary variable $\lambda$ (the Lagrange multiplier).''')
    st.markdown(r'''''')
    st.markdown(r'''Instead of optimizing $f(x)$, you optimize $L = f(x) - \lambda g(x)$. You treat both $\lambda$ and $x$ as parameters to optimize. In our case, $f(x)$ is our log-likelihood, $x$ represents $\pi_k$, and $g(x)$ is the constraint $(\sum_k \pi_k - 1 = 0)$.''')
    st.markdown(r'''''')
    st.markdown(r'''For $\pi_k$, we have the constraint $\sum_k \pi_k = 1$. To handle this, we use the Lagrange multiplier method. We optimize the likelihood while including the constraint with an auxiliary variable $\lambda$. Our total objective function becomes:''')
    st.markdown(r'''''')
    st.markdown(r'''$L = \ln p(\{x_n\} | \pi, \mu, \Sigma) - \lambda(\sum_k \pi_k - 1)$''')
    st.markdown(r'''''')
    st.markdown(r'''Now we differentiate this with respect to $\pi_k$ and set it to zero:''')
    st.markdown(r'''''')
    st.markdown(r'''$\partial L/\partial \pi_k = \sum_n (1/p(x_n)) * \mathcal{N}(x_n | \mu_k, \Sigma_k) - \lambda = 0$''')
    st.markdown(r'''''')
    st.markdown(r'''The first term comes from differentiating the log-likelihood, which is a sum of logs of sums with a linear term in $\pi_k$. The second term is simply $\lambda$ from differentiating the constraint term.''')
    st.markdown(r'''''')
    st.markdown(r'''Next, we multiply both sides by $\pi_k$ and sum over $k$:''')
    st.markdown(r'''''')
    st.markdown(r'''$\sum_k \pi_k * \sum_n (1/p(x_n)) * \mathcal{N}(x_n | \mu_k, \Sigma_k) - \lambda\sum_k \pi_k = 0$''')
    st.markdown(r'''''')
    st.markdown(r'''We know that $\sum_k \pi_k = 1$, and we can show that the first term also equals $N$ (the number of data points). This means $\lambda$ must equal $-N$.''')
    st.markdown(r'''''')
    st.markdown(r'''Substituting $-N$ for $\lambda$ and multiplying only by $\pi_k$ (without summing over $k$), we get:''')
    st.markdown(r'''''')
    st.markdown(r'''$\pi_k * \sum_n (1/p(x_n)) * \mathcal{N}(x_n | \mu_k, \Sigma_k) = N\pi_k$''')
    st.markdown(r'''''')
    st.markdown(r'''Rearranging, we arrive at the update rule for $\pi_k$:''')
    st.markdown(r'''''')
    st.markdown(r'''$\pi_k = (1/N) * \sum_n \gamma_{nk}$''')
    st.markdown(r'''''')
    st.markdown(r'''This result is intuitively clear: to update the weights of individual modes, we sum up all the responsibilities. For example, if we want to know the relative importance of one mode compared to another, we sum up all the probabilities of data points belonging to that mode and divide by the total number of samples.''')
    st.markdown(r'''''')
    st.markdown(r'''### The EM Algorithm for GMMs''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's put all of this together in the Expectation-Maximization algorithm:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Expectation Step (E-step):''')
    st.markdown(r'''   Given the current parameter estimates ($\pi$, $\mu$, $\Sigma$), we compute the responsibilities:''')
    st.markdown(r'''   ''')
    st.markdown(r'''   $\gamma_{nk} = (\pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k)) / (\sum_j \pi_j \mathcal{N}(x_n | \mu_j, \Sigma_j))$''')
    st.markdown(r'''''')
    st.markdown(r'''   This is a Bayesian interpretation of how we should do the assignment. The ratio of likelihoods tells us how much each mode is claiming a particular data point.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Maximization Step (M-step):''')
    st.markdown(r'''   Given the responsibilities, we update the parameters:''')
    st.markdown(r'''''')
    st.markdown(r'''   $\mu_k = (\sum_n \gamma_{nk}x_n) / (\sum_n \gamma_{nk})$''')
    st.markdown(r'''   $\Sigma_k = (\sum_n \gamma_{nk}(x_n - \mu_k)(x_n - \mu_k)^T) / (\sum_n \gamma_{nk})$''')
    st.markdown(r'''   $\pi_k = (1/N) * \sum_n \gamma_{nk}$''')
    st.markdown(r'''''')
    st.markdown(r'''   These updates are intuitive extensions of K-means:''')
    st.markdown(r'''   - For $\mu_k$, we take the weighted sum of all data points, with weights being the responsibilities.''')
    st.markdown(r'''   - For $\Sigma_k$, we calculate the covariances using a weighted sum of all data points.''')
    st.markdown(r'''   - For $\pi_k$, we sum up all the "mode-ness" of the objects and divide by the total number $N$.''')
    st.markdown(r'''''')
    st.markdown(r'''We iterate between these two steps until convergence.''')
    st.markdown(r'''''')
    st.markdown(r'''### Practical Application of GMMs''')
    st.markdown(r'''''')
    st.markdown(r'''Let's walk through the practical application of Gaussian Mixture Models, or GMMs, and discuss why they're so significant, especially in astronomy. Imagine we have a scatter of data points. In GMMs, we define cluster properties not just by their mean location, but by their entire distribution. We typically visualize this with a circle representing one standard deviation of the distribution. ''')
    st.markdown(r'''''')
    st.markdown(r'''Now, given our current assumption about the clusters, we first calculate the likelihood of each data point belonging to each cluster. Let's say we're clustering two types of astronomical objects - we might have one cluster for one type and another for the other type. We assign fractional memberships to each point. This is what we call the expectation step, where we use the ratio of likelihoods to make these soft assignments.''')
    st.markdown(r'''''')
    st.markdown(r'''After that, we move to the maximization step. Here, we update the means, covariances, and weights of our clusters based on these fractional assignments. We keep iterating between these two steps until we converge on a solution.''')
    st.markdown(r'''''')
    st.markdown(r'''One interesting aspect of GMMs is that they often find the correct solution even with initializations that might cause other methods to get stuck. This is because the soft assignments in GMMs make the loss function smoother, making it less likely to get trapped in local minima. That said, it's still good practice to run multiple initializations and keep the one with the maximum likelihood.''')
    st.markdown(r'''''')
    st.markdown(r'''### Advantages of GMMs in Astronomy''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's discuss why GMMs are often preferable to simpler clustering methods like K-means. First, GMMs use what we call soft assignments. The $\gamma_{nk}$ in our equations can take any value between 0 and 1, unlike the hard assignments in K-means where a point either belongs to a cluster or it doesn't. This allows for more nuanced cluster memberships.''')
    st.markdown(r'''''')
    st.markdown(r'''Perhaps more importantly, GMMs are what we call generative models. This is crucial because it means we're not just clustering data - we're learning the underlying distribution. Once we have that, we can actually generate new data points that fit that distribution. In astronomy, this capability can be particularly useful for simulations and predictions.''')
    st.markdown(r'''''')
    st.markdown(r'''This generative aspect leads to another key advantage: outlier detection. Because GMMs give us a full probability distribution, we can identify data points that are poorly described by our model. In astronomy, this is incredibly valuable. A significant part of the excitement in our field comes from finding new, unexpected objects - things that don't fit our current understanding. GMMs give us a powerful tool for this kind of discovery.''')
    st.markdown(r'''''')
    st.markdown(r'''GMMs are also more flexible when it comes to cluster shapes. Simpler methods often assume spherical clusters, but GMMs can capture elliptical and more complex shapes through their covariance matrices. This often allows them to better represent real-world astronomical data distributions.''')
    st.markdown(r'''''')
    st.markdown(r'''### Advanced Techniques and Future Directions''')
    st.markdown(r'''''')
    st.markdown(r'''It's worth noting that while tools like GMMs are powerful, they do have limitations, especially when we're dealing with high-dimensional data like spectra. This is where more advanced techniques, including deep learning, come into play. These methods offer even more powerful generative models that can capture highly non-trivial distributions.''')
    st.markdown(r'''''')
    st.markdown(r'''In astronomy, the real excitement of machine learning, particularly advanced techniques like deep learning, lies in their potential to find outliers and capture complex, high-dimensional distributions that classical methods struggle with. It's important to distinguish between simply applying machine learning tools to astronomical data and doing research to merge advanced machine learning ideas with astronomical problems. The latter is where we're seeing some of the most exciting developments in our field.''')
    st.markdown(r'''''')
    st.markdown(r'''### Determining the Optimal Number of Clusters''')
    st.markdown(r'''''')
    st.markdown(r'''An important question to consider is: How do we determine the best number of clusters, or $K$, in our Gaussian Mixture Models? $K$ is a hyperparameter that we often need to define beforehand, but choosing the right $K$ isn't always straightforward.''')
    st.markdown(r'''''')
    st.markdown(r'''There's no universally correct answer, but there are some approaches we can consider. First, it's important to understand what happens as we increase $K$. For any given $K$, we can calculate the final best likelihood of the model. As $K$ increases, the likelihood will generally improve because our model becomes more expressive. If we were to simply choose the $K$ that gives us the best likelihood, we might keep increasing $K$ indefinitely. But of course, this approach runs the risk of overfitting our data.''')
    st.markdown(r'''''')
    st.markdown(r'''So how do we strike a balance? There are some established methods in statistics for this, namely the Bayesian Information Criterion (BIC) and the Akaike Information Criterion (AIC). These methods are conceptually similar to regularization terms in other machine learning contexts. They calculate the log likelihood, but as you increase $K$, they also introduce a penalty term. This penalty discourages unnecessarily complex models unless the increased complexity is really needed to explain the data.''')
    st.markdown(r'''''')
    st.markdown(r'''While BIC and AIC are statistically elegant and rigorous under certain conditions, those assumptions are often too strong for the kind of data we work with in astronomy. ''')
    st.markdown(r'''''')
    st.markdown(r'''For practical astronomical work, a straightforward approach often works well: plot the log likelihood of the best model as a function of $K$. As you do this, you'll typically see that even though the log likelihood keeps increasing with $K$, it starts to plateau at some point. Look for what we call the "elbow" or "knee" of this curve. This turning point often indicates a good choice for $K$.''')
    st.markdown(r'''''')
    st.markdown(r'''This approach, while perhaps less mathematically rigorous than BIC or AIC, often works well in practice for astronomical data. It allows you to visually identify the point where adding more clusters starts to yield diminishing returns in terms of model fit.''')
    st.markdown(r'''''')
    st.markdown(r'''Remember, in many astronomical contexts, the concept of a true $K$ might not even make sense. Instead, we're looking for a model complexity that balances goodness of fit with simplicity and interpretability. This visual method helps you make that judgment in a way that's often more intuitive and practical for real-world astronomical data analysis.''')
    st.markdown(r'''''')
    st.markdown(r'''To wrap up, we've covered one key task in unsupervised learning: clustering and density approximation. We started with K-means, which is simple but limited, and then moved to GMMs, which offer a more probabilistic approach. We've seen that even for these relatively simple models, optimization requires more sophisticated techniques like expectation-maximization, rather than just gradient descent.''')
    st.markdown(r'''''')

if __name__ == '__main__':
    show_page()