import streamlit as st
from streamlit_app import navigation_menu

def show_page():
    st.set_page_config(page_title="Comp Astro",
                       page_icon="https://raw.githubusercontent.com/teaghan/astronomy-12/main/images/tutor_favicon.png", layout="wide")

    # # Page Title
    st.title('Supervised Learning: Regression - Linear Regression')

    navigation_menu()

    # # Embed the external HTML page
    # st.info('''
    # Course Slides
    # ''')
    # slideshare_embed_code = """
    #     <iframe src="https://www.slideshare.net/slideshow/embed_code/key/pJMT9Lqvkq3jFH" 
    #             width="700" height="394" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" 
    #             style="border:0px solid #000; border-width:0px; margin-bottom:0px; max-width: 100%;" allowfullscreen> 
    #     </iframe> 
    # """
    # components.html(slideshare_embed_code, height=412)

    # st.markdown('---')
    
    st.markdown(r'''''')
    st.markdown(r'''## Fundamental Concepts and Course Overview''')
    st.markdown(r'''''')
    st.markdown(r'''Welcome to our first main lecture on machine learning in astronomy. Let's start with some fundamental concepts, including the distinction between supervised and unsupervised learning. We'll also cover cross-validation - a crucial skill for evaluating and selecting the best models.''')
    st.markdown(r'''''')
    st.markdown(r'''Our journey begins with Bayesian statistics, which provides the essential framework for understanding the rest of this course. While machine learning is often treated as a standalone subject, we're taking a different approach. We'll explore the statistical foundations of various machine learning methods, giving you a deeper, more unified understanding of these topics.''')
    st.markdown(r'''''')
    st.markdown(r'''More importantly, the philosophy behind Bayesian statistics (and by extension, astrostatistics) offers a general framework for understanding and modeling the Universe. This philosophical approach forms the bedrock of how we approach data modeling.''')
    st.markdown(r'''''')
    st.markdown(r'''Fair warning: we're diving into the deep end with Bayesian statistics right off the bat. If this is new territory for you, it might feel like a steep learning curve. Don't worry, though - you'll get a bird's-eye view of everything we'll be covering, which will help you navigate the details as we progress.''')
    st.markdown(r'''''')
    st.markdown(r'''## Course Resources''')
    st.markdown(r'''''')
    st.markdown(r'''To illustrate the application of Bayesian statistics, we'll focus on linear models. Now, don't be fooled by their seemingly simple appearance of "fitting a line." When viewed through the lens of Bayesian statistics, linear models reveal themselves as a rich and nuanced field of study. This will set the tone for the rest of our lectures.''')
    st.markdown(r'''''')
    st.markdown(r'''For those who need a refresher or want to brush up on their math skills, I recommend the book "Mathematics for Machine Learning." It's an excellent resource for the basics. For those seeking a more in-depth understanding, the textbook from Bishop is highly recommended. It's a timeless textbook that covers all the key ideas in classical machine learning, which forms a significant part of our course content.''')
    st.markdown(r'''''')
    st.markdown(r'''## Classical Machine Learning: The Foundation''')
    st.markdown(r'''''')
    st.markdown(r'''This course focuses on the classical aspects of machine learning, allowing us to provide more statistical rigor to the subject. I often like to compare machine learning to music. Yes, there are many exciting methods like neural networks (which we will touch upon in this course) that are like K-pop. But to truly appreciate these "pop" methods, a solid foundation in "classical" machine learning can enhance your understanding and appreciation of more complex applications. It's not just about random strings put together; there's a structure and logic to it, much like in music. It might be challenging to learn Bach or Beethoven right away, so to speak, before being able to "play" with some pop music, but I assure you that this approach will be rewarded in the long run.''')
    st.markdown(r'''''')
    st.markdown(r'''## Defining Machine Learning''')
    st.markdown(r'''''')
    st.markdown(r'''So, what is machine learning? If you recall your basic programming, i.e., explicit programming (not machine learning), if you have input $X$, you will output some value $Y$. If you see another input $X$, you will output another value $Y$. This is classical, very explicit programming. ''')
    st.markdown(r'''''')
    st.markdown(r'''But what machine learning does is try to build an understanding of a world view based on the data we have without explicitly coding it. Importantly, the goal here is to build such a "world view" based only on the data. This is different from "modeling" where we are explicitly given hard-coded rules.''')
    st.markdown(r'''''')
    st.markdown(r'''However, it's crucial to understand that there's no such thing as "pure learning" or "pure modeling." Everything we do in research falls somewhere between learning and modeling. Ultimately, what we try to do here is to build a "world view" and understanding of the universe from the data. In many cases, there's a "data-driven" part (i.e., learning), but there's also an assumption part (i.e., modeling).''')
    st.markdown(r'''''')
    st.markdown(r'''For instance, when we do linear regression (which we'll cover today), we're learning from the data, but we're also modeling by assuming the underlying data follows a hyperplane. This blend of learning and modeling is present in all areas of scientific research. It's not a matter of either doing machine learning or not doing it; rather, it's always a spectrum where we combine elements of both learning from data and making assumptions about the underlying structure.''')
    st.markdown(r'''''')
    st.markdown(r'''## Taxonomy of Machine Learning''')
    st.markdown(r'''''')
    st.markdown(r'''Let's dive into the taxonomy of machine learning and its applications in astronomy. Machine Learning is broadly categorized into three main types: Supervised Learning, Unsupervised Learning, and Reinforcement Learning. In this series of lectures, we will not talk about reinforce learning, as it is a more advanced topic, and also it still have rather limited application in astronomy. But we will cover extensively supervised learning and unsupervised learning.''')
    st.markdown(r'''''')
    st.markdown(r'''### Supervised Learning''')
    st.markdown(r'''''')
    st.markdown(r'''In supervised learning, we're given a dataset containing samples, where we have input features and target outputs we're trying to predict. Let's use a real-world example to illustrate this: understanding the real estate market. In this scenario, our input feature might be the square footage of a house, and our target output would be the price of the house. The task is to predict the price of a residence given its square footage. This helps us build a "world view" of the real estate market, as we assume house prices aren't random but have a relationship with their features.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, you might be wondering what type of models will allow us to build such a relation. As we'll see, one of the simplest models would be linear regression, which involves fitting a line to the data. Obviously, in this case, as shown in the figure, a straight line wouldn't be a good fit. But here's the interesting part: linear regression extends beyond just fitting a linear line. The trick, as we'll see later in the lecture, is that we can also build more complex so-called "features." For example, we could take the square of our input. This gives us a more complex model, yet it still falls within the context of linear regression.''')
    st.markdown(r'''''')
    st.markdown(r'''We can even extend this to cases with multiple features, like number of rooms, year built, or energy ratio. This is what we call multi-dimensional regression, where we have multiple input features all contributing to our prediction of the output.''')
    st.markdown(r'''''')
    st.markdown(r'''### Supervised Learning in Astronomy: The M-sigma Relation''')
    st.markdown(r'''''')
    st.markdown(r'''But let's move beyond this mundane example. Linear regression is also widely applied in astronomy. An excellent example is the M-sigma relation. This shows a relationship between the mass of a galaxy's central black hole and the mass of the galaxy itself, particularly its central bulge.''')
    st.markdown(r'''''')
    st.markdown(r'''This relation is significant for two reasons. First, there's the practical application. It's much easier to measure the galaxy or bulge mass than the mass of the central supermassive black hole. By understanding this relationship, we can infer difficult-to-measure properties from easier-to-measure ones. This is incredibly useful in astronomy, where direct measurements can be challenging or impossible.''')
    st.markdown(r'''''')
    st.markdown(r'''Second, and perhaps more intriguingly, are the theoretical implications. The existence of this relationship is puzzling and has intrigued astronomers for decades. It leads us to understand that there's a co-evolution of the black hole and the galaxy, which was really surprising at first. Think about it: black holes are located so centrally in the galaxy, yet the formation of the entire galaxy directly affects the black hole's formation with a high precision of inference power. This has led to much of our understanding of black hole growth, though that's beyond the scope of today's lecture.''')
    st.markdown(r'''''')
    st.markdown(r'''### Classification in Astronomy''')
    st.markdown(r'''''')
    st.markdown(r'''Let's expand our discussion on machine learning in astronomy. While we've focused on regression tasks where our target variable is continuous, supervised learning encompasses much more. Often, we're dealing with classification problems where our target variable is discrete, representing different categories like stars, galaxies, or types of celestial objects. You might wonder, "Why not just treat these discrete variables as a regression task?" Well, in our next lecture, we'll dive into why this approach can lead to some tricky issues and why we need specialized classification techniques.''')
    st.markdown(r'''''')
    st.markdown(r'''A prime example of classification in astronomy is star-galaxy separation. It's crucial in many observations to distinguish between stars and galaxies accurately. Why? Because misclassification can lead to some pretty significant errors. Imagine mistaking a high-redshift galaxy for a nearby M-dwarf star – you'd end up with vastly different scientific conclusions!''')
    st.markdown(r'''''')
    st.markdown(r'''### Unsupervised Learning''')
    st.markdown(r'''''')
    st.markdown(r'''In this series of lecture, we will also talk about unsupervised learning. The key difference here is that we're not separating our data into input features and target outputs. Instead, we're on a hunt for inherent structures or patterns in the data. Unsupervised learning mainly falls into two categories:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Dimensionality Reduction: This is all about compressing information from high-dimensional data into fewer dimensions while keeping the essential features. In this course, we'll cover Principal Component Analysis (PCA), a go-to technique for this task.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Clustering: Here, we're looking to find natural groupings within the data.''')
    st.markdown(r'''''')
    st.markdown(r'''These unsupervised techniques are incredibly powerful in astronomy. For instance, by applying clustering to star orbits (plotting energy vs. angular momentum), astronomers have uncovered distinct groups of stars that likely came from different smaller galaxies that merged with the Milky Way.''')
    st.markdown(r'''''')
    st.markdown(r'''## Applications of Unsupervised Learning in Astronomy''')
    st.markdown(r'''''')
    st.markdown(r'''On dimension redunction, I've got a personal anecdote here – one of my undergraduate projects involved applying PCA to stellar chemistry. It's fascinating how this seemingly simple analysis of a 25-dimensional space with just 200 stars led to significant insights about stellar nucleosynthesis. And here's another cool example: using PCA to analyze quasar spectra. By compressing information from many pixels into a few principal components, we can start to decipher the spectra of these voracious, feeding black holes.''')
    st.markdown(r'''''')
    st.markdown(r'''## Course Outline''')
    st.markdown(r'''''')
    st.markdown(r'''Looking ahead, here's our game plan for the upcoming lectures: Today, we're tackling regression (supervised learning). Next, we'll dive into classification (also supervised learning). After that, we'll explore unsupervised learning, covering both clustering and dimensionality reduction.''')
    st.markdown(r'''''')
    st.markdown(r'''This progression will give you a taste of the different tasks we care about in astronomy. We're starting with the basics, but don't underestimate their power. As we've seen, even simple techniques like PCA can lead to cutting-edge research in astronomy.''')
    st.markdown(r'''''')
    st.markdown(r'''After that, we'll also cover some modern techniques like neural networks, exploring their applications in both supervised and unsupervised learning. However, we'll see that these modern techniques sometimes have limitations when it comes to uncertainty quantification. This realization will lead us to some more classical models that don't suffer from these issues, such as advanced topics like Gaussian processes – what I like to call the "Bach or Beethoven" of machine learning. ''')
    st.markdown(r'''''')
    st.markdown(r'''As we delve into Gaussian processes, we'll discover that unlike linear regression or classification, sampling what we call the posterior for these advanced techniques can be quite tricky, especially for Gaussian process classification. This challenge will lead us to lectures on sampling techniques, including MCMC.''')
    st.markdown(r'''''')
    st.markdown(r'''All of this is to say that while each lecture introduces individual techniques, they're all interconnected. Over the course of this series, you'll gradually uncover these connections. ''')
    st.markdown(r'''''')
    st.markdown(r'''Finally, for completeness, we'll touch on large language models like GPT, discussing not just how to use them, but also how they work behind the scenes and how you might fine-tune them for astronomical applications.''')
    st.markdown(r'''''')
    st.markdown(r'''## Evaluating Machine Learning Models''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's dive into the heart of machine learning, starting with supervised learning. One of the fundamental questions we need to address is: How do we know if our machine learning model is working effectively? Let's break this down with a simple example.''')
    st.markdown(r'''''')
    st.markdown(r'''Imagine we have some data with input $X$, and we want to build a straightforward model using polynomial fitting. The key question here is: How many orders of polynomial should we use? This is where the concept of a "good fit" comes into play.''')
    st.markdown(r'''''')
    st.markdown(r'''If we look at our data points, represented as yellow dots on a graph, we might see different scenarios:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Underfitting: This occurs when we use too few parameters or a very rigid model. The model is too simple to capture the underlying pattern in the data.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Overfitting: On the other extreme, if we use too many parameters, our model might start fitting the noise in the data rather than the true underlying pattern.''')
    st.markdown(r'''''')
    st.markdown(r'''3. Good fit: This is the sweet spot where our model captures the true pattern without being overly influenced by noise.''')
    st.markdown(r'''''')
    st.markdown(r'''### Quantifying Model Performance''')
    st.markdown(r'''''')
    st.markdown(r'''But how do we quantify what constitutes a good fit versus a bad fit? The simplest approach is to split our data into training and testing sets. In the polynomial case, you'd expect that for the training data, increasing the degrees of freedom (i.e., using higher-order polynomials) will keep improving your fit. However, there's a catch: at some point, even though you're improving your fit on the training data, you'll start overfitting to the noise in your data.''')
    st.markdown(r'''''')
    st.markdown(r'''This is where the test set comes in handy. By evaluating your model on data it hasn't seen during training, you can get a better idea of its true performance. However, there's a slight logical flaw in using just training and testing sets: you're still using your test data to determine the best order of polynomial. In practice, especially in astronomy where we often have limited data, this might not be a huge issue. But if you want to be methodologically rigorous, you want to ensure that the true test dataset isn't influencing your choice of model complexity.''')
    st.markdown(r'''''')
    st.markdown(r'''### The Three-Way Split: Training, Validation, and Testing''')
    st.markdown(r'''''')
    st.markdown(r'''This is why many practitioners recommend splitting your data into three sets: training, validation, and testing. Here's how it works:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Training set: This is used to fit the polynomial (or any model).''')
    st.markdown(r'''''')
    st.markdown(r'''2. Validation set: This is used to check the trend of what we call the loss function and to select the best model.''')
    st.markdown(r'''''')
    st.markdown(r'''3. Test set: This final set is used to evaluate the chosen model's performance on completely unseen data.''')
    st.markdown(r'''''')
    st.markdown(r'''A common split might be 60% for training, 20% for validation, and 20% for testing, but these proportions can vary based on your specific situation and the amount of data available.''')
    st.markdown(r'''This approach allows you to make decisions about model complexity (like the degree of polynomial to use) based on the validation set, while keeping the test set completely untouched until you're ready for a final evaluation of your chosen model.''')
    st.markdown(r'''''')
    st.markdown(r'''## Introduction to Bayesian Statistics''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's dive into Bayesian statistics, which is fundamental for this lecture. Let's start with some basic probability concepts.''')
    st.markdown(r'''''')
    st.markdown(r'''### Basic Probability Concepts''')
    st.markdown(r'''''')
    st.markdown(r'''To understand Bayesian statistics, we need two basic ingredients: the sum rule and the product rule. These pertain to cases where we have two random variables, $X$ and $Y$.''')
    st.markdown(r'''''')
    st.markdown(r'''#### The Sum Rule''')
    st.markdown(r'''''')
    st.markdown(r'''The sum rule, which gives us the marginal distribution of $X$, $p(X)$, is obtained by integrating over all possible values of $Y$:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(x) = \int p(x, y) dy$''')
    st.markdown(r'''''')
    st.markdown(r'''Think of it this way: imagine $X$ as the chance that I bring an umbrella, and $Y$ as the chance that it rains. If I want to know the probability of bringing an umbrella today, regardless of whether it rains or not, that would be the marginal distribution of $X$.''')
    st.markdown(r'''''')
    st.markdown(r'''#### The Product Rule''')
    st.markdown(r'''''')
    st.markdown(r'''Besides the sum rule, we also have the product rule of probability, which states that the joint probability of $X$ and $Y$ is equal to the probability of $X$ given $Y$, multiplied by the probability of $Y$:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(x, y) = p(x | y) p(y)$''')
    st.markdown(r'''''')
    st.markdown(r'''Similarly, we can swap the order:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(x, y) = p(y | x) p(x)$''')
    st.markdown(r'''''')
    st.markdown(r'''### Bayes' Theorem''')
    st.markdown(r'''''')
    st.markdown(r'''Interestingly, if we equate these two (because they're technically the same thing), we arrive at Bayes' theorem:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(x | y) = \dfrac{p(y | x) p(x)}{p(y)}$''')
    st.markdown(r'''''')
    st.markdown(r'''This is particularly interesting because it tells us that we can understand the probability of $X$ given $Y$ from the probability of $Y$ given $X$ with some other specified terms. This is critical because it means that even if we only know the probability in a single conditional direction, we can infer the reverse direction.''')
    st.markdown(r'''''')
    st.markdown(r'''This simple equation is the foundation of Bayesian statistics, named after Thomas Bayes. In astronomy, Bayesian statistics is widely used, but to understand why, we need to grasp the difference between the Bayesian and frequentist approaches.''')
    st.markdown(r'''''')
    st.markdown(r'''### Bayesian vs. Frequentist Approaches''')
    st.markdown(r'''''')
    st.markdown(r'''#### The Frequentist Approach''')
    st.markdown(r'''''')
    st.markdown(r'''The frequentist approach is intuitive: as the name suggests, to determine if a dice is fair, you'd roll it many times and plot the histogram of results. If it forms a uniform distribution, the dice is fair. But here's the catch - what if we can't repeat the experiment?''')
    st.markdown(r'''''')
    st.markdown(r'''This is precisely the situation we face in astronomy. We have only one universe, one Milky Way. We can't "roll the dice" multiple times. This was the thought experiment that led Bayes to ask: what if we can't repeat the experiment? Can we still understand the truth we live in? From the Bayesian theorem above, the answer appears to be yes, because we can let $X$ be the observation and $Y$ be the truth, allowing us to reverse the process, but with a catch.''')
    st.markdown(r'''''')
    st.markdown(r'''#### The Bayesian Perspective''')
    st.markdown(r'''''')
    st.markdown(r'''The key catch here is that in this formalism, we are treating both the observation and the "truth" as random variables, and the latter is profound. Treating our knowledge of the ground truth (i.e., $p(Y=truth|X=data)$) as a random variable means that we admit our understanding will always be imperfect due to finite observations. This means that whatever we do, we'll never know the exact truth. Our models themselves should be uncertain.''')
    st.markdown(r'''''')
    st.markdown(r'''This idea is profound. It doesn't mean Bayes didn't believe there is an underlying truth governing the universe. Rather, it acknowledges that because we live in one universe with one realization, it's impossible for us to be absolutely certain about anything. But we can refine the uncertainty of our truth as the data presents itself. This forms the foundation of science itself. Science isn't just a collection of facts; it's a process of constantly updating our beliefs and admitting that our models are always imperfect. This is how we make progress.''')
    st.markdown(r'''''')
    st.markdown(r'''### The Simplicity and Power of Bayes' Theorem''')
    st.markdown(r'''''')
    st.markdown(r'''The math behind Bayes' theorem is simple, which is why it's amusing when people say they do research in "Bayesian statistics," which itself is nothing to brag about -- especially in astronomy, where in almost all cases, the only "type" of statistics we could do would be Bayesian statistics. Even Thomas Bayes thought it was such a tautology that he initially didn't publish it. The power of Bayesian inference isn't in the math, but in the deep understanding of how we should view the universe given our limited observations.''')
    st.markdown(r'''''')
    st.markdown(r'''### Applying Bayes' Theorem to Statistical Modeling''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's apply Bayes' theorem to our statistical modeling. If we consider our data ($\mathcal{D}$) and model parameters ($w$) as random variables, we can write Bayes' theorem as:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(w | \mathcal{D}) = \frac{p(\mathcal{D} | w) p(w)}{p(\mathcal{D})}$''')
    st.markdown(r'''''')
    st.markdown(r'''Let's break this down:''')
    st.markdown(r'''''')
    st.markdown(r'''1. $p(w | \mathcal{D})$ is the posterior: Given our observed data, what is our belief about the model parameters? This is what we're ultimately trying to determine.''')
    st.markdown(r'''''')
    st.markdown(r'''2. $p(\mathcal{D} | w)$ is the likelihood: Assuming our model is correct, what's the probability of observing our data?''')
    st.markdown(r'''''')
    st.markdown(r'''3. $p(w)$ is the prior: Before seeing any data, what's our belief about the model parameters?''')
    st.markdown(r'''''')
    st.markdown(r'''4. $p(\mathcal{D})$ is the evidence: This is the probability of the data under all possible model parameters. It's often treated as a normalization constant.''')
    st.markdown(r'''''')
    st.markdown(r'''In practice, we often work with the proportional form:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(w | \mathcal{D}) \propto p(\mathcal{D} | w) p(w)$''')
    st.markdown(r'''''')
    st.markdown(r'''This formulation is profound because it puts the data and the model on equal footing as random variables. It allows us to update our beliefs about the model based on observed data, even when we can't repeat an experiment (as is often the case in astronomy).''')
    st.markdown(r'''''')
    st.markdown(r'''### Bayesian Inference in Practice''')
    st.markdown(r'''''')
    st.markdown(r'''For example, in the historical debate about whether the Earth or the Sun is at the center of the solar system, Bayesian inference would ask: Given our observations (data), what's the probability of each model being true? We can calculate this by considering the likelihood of our observations under each model, combined with our prior beliefs.''')
    st.markdown(r'''''')
    st.markdown(r'''The key to applying Bayesian inference is defining the likelihood function. This function describes how probable our observed data is under different model parameters. By combining this with our prior beliefs, we can update our understanding of the model parameters.''')
    st.markdown(r'''''')
    st.markdown(r'''This approach is particularly powerful in astronomy where we often can't repeat experiments. We're not trying to determine an absolute "truth," but rather updating our beliefs based on available evidence. Each term in Bayes' theorem has an intuitive meaning, allowing us to incorporate both our prior knowledge and new data into our models.''')
    st.markdown(r'''''')
    st.markdown(r'''## Introduction to Linear Regression''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's move on to a specific application of this framework: linear regression, or fitting a line to data. You might think, "Why focus on something as simple as fitting a line?" But there are several reasons why this is a powerful and important technique:''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's discuss why linear regression, despite its simplicity, is so powerful and widely used in astronomy:''')
    st.markdown(r'''''')
    st.markdown(r'''### The Power of Linear Regression''')
    st.markdown(r'''''')
    st.markdown(r'''Occam's Razor: As Einstein said, "Everything should be made as simple as possible, but not simpler." Linear regression embodies this principle, offering a straightforward approach to understanding complex systems.''')
    st.markdown(r'''''')
    st.markdown(r'''Analytical Solution: One of the beauties of linear regression is that it has an exact solution. This means we can solve it directly, without relying on iterative or approximate methods.''')
    st.markdown(r'''''')
    st.markdown(r'''Well-understood Statistical Behavior: In the world of astrostatistics, we must acknowledge that we never have infinite data. Linear regression allows us to understand the statistical behavior of our model, including uncertainties and confidence intervals.''')
    st.markdown(r'''''')
    st.markdown(r'''Flexibility through Feature Engineering: While the model itself is linear, we can make it highly expressive by transforming our input features. For example, if we're modeling real estate prices, we don't have to use just the size of the house. We can use the square of the size, the size multiplied by the lot size, or the size multiplied by the year built. This allows us to capture non-linear relationships while still benefiting from the simplicity of linear regression.''')
    st.markdown(r'''''')
    st.markdown(r'''Interpretability: Linear models are often easier to interpret than more complex models, which is crucial in scientific applications.''')
    st.markdown(r'''''')
    st.markdown(r'''### Applications in Astronomy''')
    st.markdown(r'''''')
    st.markdown(r'''Widely Applicable in Astronomy: Many important relationships in astronomy can be modeled using linear regression. For example:''')
    st.markdown(r'''''')
    st.markdown(r'''a) The M-sigma relation, which relates the mass of a galaxy's central black hole to the velocity dispersion of the galaxy.''')
    st.markdown(r'''''')
    st.markdown(r'''b) The Kennicutt-Schmidt law, which relates the density of star formation to the surface density of molecular gas. This can be expressed as:''')
    st.markdown(r'''''')
    st.markdown(r'''$\Sigma_{SFR} = \alpha \cdot \Sigma_{mol}^\beta$''')
    st.markdown(r'''''')
    st.markdown(r'''Which, when log-transformed, becomes a linear relationship:''')
    st.markdown(r'''''')
    st.markdown(r'''$\log(\Sigma_{SFR}) = \gamma + \beta \cdot \log(\Sigma_{mol})$''')
    st.markdown(r'''''')
    st.markdown(r'''This last example illustrates how power laws, which are common in astronomy, can be transformed into linear relationships through logarithms. ''')
    st.markdown(r'''''')
    st.markdown(r'''### Formalizing Linear Regression''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's formalize the problem setting for linear regression. We assume our output $y$, a scalar, can be written as a linear combination of our input features:''')
    st.markdown(r'''''')
    st.markdown(r'''$y = w^T \cdot x + \varepsilon$''')
    st.markdown(r'''''')
    st.markdown(r'''Here, $w$ is our vector of model parameters (weights), $x$ is our vector of input features, and $\varepsilon$ represents noise or error. For notational convenience, we often include a constant term (bias) by adding a '1' to our feature vector, allowing us to write this as a simple dot product of $w$ and $x$.''')
    st.markdown(r'''''')
    st.markdown(r'''The key question is: how do we optimize these model parameters? While there are many ad hoc approaches, we'll start with a concrete basis in Bayesian inference. This framework will allow us to not only find the best parameters but also understand their uncertainties and make probabilistic predictions.''')
    st.markdown(r'''''')
    st.markdown(r'''### Mathematical Foundations of Linear Regression''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's dive deeper into the mathematics of linear regression and how we can use the principle of maximum likelihood estimation to find the optimal parameters.''')
    st.markdown(r'''''')
    st.markdown(r'''First, let's define our linear model:''')
    st.markdown(r'''''')
    st.markdown(r'''$y(x, w) = \sum_{j=0}^{M-1} w_j \cdot \phi_j(x) = w^T \cdot \phi(x)$''')
    st.markdown(r'''''')
    st.markdown(r'''Where:''')
    st.markdown(r'''$w = (w_0, ..., w_{M-1})^T$ is our vector of parameters''')
    st.markdown(r'''$\phi(x) = (\phi_0(x), ..., \phi_{M-1}(x))^T$ is our vector of basis functions''')
    st.markdown(r'''''')
    st.markdown(r'''By convention, $\phi_0(x) = 1$, making $w_0$ the bias parameter''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's assume our observations are noisy:''')
    st.markdown(r'''''')
    st.markdown(r'''$t = y(x, w) + \varepsilon$, where $\varepsilon \sim \mathcal{N}(0, \sigma^2)$''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's consider how we move from a single observation to multiple observations:''')
    st.markdown(r'''For a single observation, the likelihood of observing $t$ given input $x$ is:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(t | x, w, \sigma^2) = \mathcal{N}(t | y(x, w), \sigma^2)$''')
    st.markdown(r'''''')
    st.markdown(r'''Where $\mathcal{N}(x | \mu, \sigma^2)$ is the normal (Gaussian) distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''But that's only one data point. What do we do when we have $N$ data points? Here's where we make an important assumption: we assume that all our data points are independent.''')
    st.markdown(r'''''')
    st.markdown(r'''We apply the same principle to our data points. If we have $n$ independent data points, then the combined likelihood is just the product of all the individual likelihoods for each $x_n$ and $t_n$ pair.''')
    st.markdown(r'''''')
    st.markdown(r'''To make this easier to work with, we take the logarithm of both sides. Since log is a monotonic function, it doesn't change the location of the maximum:''')
    st.markdown(r'''''')
    st.markdown(r'''$\ln p(t | X, w, \sigma^2) = \sum_{n=1}^N \ln \mathcal{N}(t_n | w^T \cdot \phi(x_n), \sigma^2)$''')
    st.markdown(r'''''')
    st.markdown(r'''Expanding this out:''')
    st.markdown(r'''''')
    st.markdown(r'''$\ln p(t | X, w, \sigma^2) = \dfrac{N}{2} \cdot \ln(\dfrac{1}{\sigma^2}) - \dfrac{N}{2} \cdot \ln(2\pi) - \dfrac{1}{2\sigma^2} \cdot \sum_{n=1}^N (t_n - w^T \cdot \phi(x_n))^2$''')
    st.markdown(r'''''')
    st.markdown(r'''Now, if we want to find the $w$ that maximizes this likelihood, we only need to focus on the part that depends on $w$:''')
    st.markdown(r'''''')
    st.markdown(r'''$\arg\max_w \ln p(t | X, w, \sigma^2) = \arg\min_w \sum_{n=1}^N \dfrac{1}{2} \cdot (t_n - w^T \cdot \phi(x_n))^2$''')
    st.markdown(r'''''')
    st.markdown(r'''The right-hand side of this equation is equivalent to minimizing the sum of squared errors, which we often denote as $E(w)$ or $\chi^2$ (chi-squared).''')
    st.markdown(r'''''')
    st.markdown(r'''This derivation shows us that maximizing the likelihood under our assumptions is equivalent to minimizing the sum of squared errors, something that is often assumed in astronomy, but not quite often explained, and all these simply come from optimization the likelihood.''')
    st.markdown(r'''''')
    st.markdown(r'''### Analytical Solution for Linear Regression''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's delve into the analytical solution for the linear model. We can express our error function $E(w)$ in matrix form:''')
    st.markdown(r'''''')
    st.markdown(r'''$E(w) = \dfrac{1}{2} \cdot (t - \Phi w)^T \cdot (t - \Phi w)$''')
    st.markdown(r'''''')
    st.markdown(r'''Where:''')
    st.markdown(r'''- $t = (t_1, ..., t_n)^T$ is our target matrix''')
    st.markdown(r'''- $\Phi$ is our design matrix, with rows corresponding to $\phi(x_n)$ for each data point''')
    st.markdown(r'''''')
    st.markdown(r'''To find the optimal $w$, we differentiate $E(w)$ with respect to $w$ and set it to zero:''')
    st.markdown(r'''''')
    st.markdown(r'''$\nabla_w E(w) = \Phi^T \cdot (t - \Phi w) = 0$''')
    st.markdown(r'''''')
    st.markdown(r'''Solving this equation gives us the maximum likelihood estimate:''')
    st.markdown(r'''''')
    st.markdown(r'''$w_{ML} = (\Phi^T \cdot \Phi)^{-1} \cdot \Phi^T \cdot t$''')
    st.markdown(r'''''')
    st.markdown(r'''This solution is powerful for several reasons:''')
    st.markdown(r'''''')
    st.markdown(r'''1. It's an exact solution, not an approximation.''')
    st.markdown(r'''2. It can handle high-dimensional feature spaces efficiently.''')
    st.markdown(r'''3. It's computationally inexpensive for most practical problems.''')
    st.markdown(r'''''')
    st.markdown(r'''The most computationally expensive part is inverting $(\Phi^T \cdot \Phi)$, but this is typically manageable because it's an $m \times m$ matrix, where $m$ is the number of features. Even with millions of data points, as long as the number of features is reasonable (say, up to a few hundred), this inversion is computationally feasible.''')
    st.markdown(r'''''')
    st.markdown(r'''### Estimating Noise''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's consider the noise term. We've been assuming homogeneous noise (constant $\sigma^2$ for all observations), but what if we don't know its value? We can estimate it from the data as well.''')
    st.markdown(r'''''')
    st.markdown(r'''Recall our log-likelihood function:''')
    st.markdown(r'''''')
    st.markdown(r'''$\ln p(t | X, w, \beta) = \dfrac{N}{2} \cdot \ln \beta - \dfrac{N}{2} \cdot \ln(2\pi) - \beta \cdot \dfrac{1}{2} \cdot \sum_{n=1}^N (t_n - w^T \cdot \phi(x_n))^2$''')
    st.markdown(r'''''')
    st.markdown(r'''Where $\beta = \dfrac{1}{\sigma^2}$.''')
    st.markdown(r'''''')
    st.markdown(r'''If we differentiate this with respect to $\beta$ and set it to zero, we get:''')
    st.markdown(r'''''')
    st.markdown(r'''$\dfrac{1}{\beta_{ML}} = \sigma^2_{ML} = \dfrac{1}{N} \cdot \sum_{n=1}^N (t_n - w_{ML}^T \cdot \phi(x_n))^2$''')
    st.markdown(r'''''')
    st.markdown(r'''This gives us an estimate of the noise variance based on the residuals of our model.''')
    st.markdown(r'''''')
    st.markdown(r'''It's worth noting that so far, we've been focusing on maximum likelihood estimation. This approach assumes that the data alone is sufficient to determine our model parameters. However, in a fully Bayesian treatment, we might want to incorporate prior knowledge about $w$.''')
    st.markdown(r'''''')
    st.markdown(r'''The goal of the Bayesian approach is that it provides not just point estimates, but a full probabilistic model. This allows us to quantify uncertainties in our predictions and model parameters, which is crucial in scientific applications like astronomy.''')
    st.markdown(r'''''')
    st.markdown(r'''As we will see below, if we assume a Gaussian prior on $w$, we can still obtain an analytical solution (known as the Maximum A Posteriori or MAP estimate). This leads to what's known as Ridge regression or L2 regularization. For more complex priors, we might need to resort to sampling methods (which we'll cover in lecture 11B) to fully characterize the posterior distribution of $w$.''')
    st.markdown(r'''''')
    st.markdown(r'''## Bayesian Linear Regression''')
    st.markdown(r'''Now, let's delve deeper into the Bayesian perspective of linear regression. We've been focusing on maximizing the likelihood $p(t | X, w, \sigma^2)$, but in a full Bayesian treatment, we're interested in the posterior distribution $p(w | \mathcal{D})$, where $\mathcal{D} = (X, t, \sigma^2)$.''')
    st.markdown(r'''''')
    st.markdown(r'''### Gaussian Prior and Posterior''')
    st.markdown(r'''''')
    st.markdown(r'''To get a proper posterior, we need to specify a prior on $w$. Let's assume a Gaussian prior:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(w) = \mathcal{N}(w | m_0, S_0)$''')
    st.markdown(r'''''')
    st.markdown(r'''where $m_0$ is the prior mean and $S_0$ is the prior covariance matrix.''')
    st.markdown(r'''''')
    st.markdown(r'''The posterior then becomes a product of Gaussians:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(w | t, X, \sigma^2) \propto p(t | w, X, \sigma^2) p(w)$''')
    st.markdown(r'''$\propto \mathcal{N}(t | w^T \Phi, \sigma^2I) \mathcal{N}(w | m_0, S_0)$''')
    st.markdown(r'''''')
    st.markdown(r'''Let's denote the posterior as $p(w | t, X, \sigma^2) \equiv \mathcal{N}(w | m_n, S_n)$. Using the result for the product of Gaussians, we can derive:''')
    st.markdown(r'''''')
    st.markdown(r'''$S_n = (S_0^{-1} + \dfrac{1}{\sigma^2}\Phi^T \Phi)^{-1}$''')
    st.markdown(r'''$m_n = S_n(S_0^{-1}m_0 + \dfrac{1}{\sigma^2}\Phi^T t)$''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's consider a simpler prior where $m_0 = 0$ and $S_0^{-1} = \dfrac{1}{\eta^2}I$, i.e., $p(w) = \mathcal{N}(w | 0, \eta^2I)$. This gives us:''')
    st.markdown(r'''''')
    st.markdown(r'''$S_n = (\dfrac{1}{\eta^2} + \dfrac{1}{\sigma^2}\Phi^T \Phi)^{-1}$''')
    st.markdown(r'''$m_n = (\dfrac{\sigma^2}{\eta^2} + \Phi^T \Phi)^{-1} \Phi^T t$''')
    st.markdown(r'''''')
    st.markdown(r'''### Unified View of Estimation Methods''')
    st.markdown(r'''''')
    st.markdown(r'''This formulation provides a unified view of maximum likelihood, $\chi^2$ minimization, and Bayesian inference. If we assume very precise observations ($\sigma^2 \to 0$) or an infinitely broad prior ($\eta \to \infty$), we recover the maximum likelihood solution:''')
    st.markdown(r'''''')
    st.markdown(r'''$m_n = (\Phi^T \Phi)^{-1} \Phi^T t = w_{ML}$''')
    st.markdown(r'''''')
    st.markdown(r'''The key insight from Bayesian statistics is that both the data and the parameters are treated as random variables, meaning our estimate of $w$ itself is uncertain. In other words, I don't just know the best $W$ (i.e., all the slopes that could explain the data), I also know the distribution of $W$. So I know the posterior of $W$.''')
    st.markdown(r'''''')
    st.markdown(r'''### Connection to Ridge Regression''')
    st.markdown(r'''''')
    st.markdown(r'''We can also view this from the perspective of ridge regression or regularization. Taking the log of the posterior:''')
    st.markdown(r'''''')
    st.markdown(r'''$\ln p(w | t, X, \sigma^2) = \ln p(t | w, X, \sigma^2) + \ln p(w) + const.$''')
    st.markdown(r'''$= -\dfrac{1}{2\sigma^2} ||t - \Phi w||^2 - \dfrac{1}{2\eta^2} ||w||^2 + const.$''')
    st.markdown(r'''''')
    st.markdown(r'''The second term acts as a regularizer, penalizing non-zero $w$ and favoring sparser solutions. The value of $\eta$ acts as a hyperparameter controlling the strength of regularization.''')
    st.markdown(r'''''')
    st.markdown(r'''This term might look familiar to those who have worked with neural networks. To prevent overfitting, one common trick is to include a penalty term, often called L2 regularization. Many things we take for granted in machine learning, like regularization, actually come from the Bayesian perspective of including prior beliefs. This regularizes our models.''')
    st.markdown(r'''''')
    st.markdown(r'''In practice, this means that if I want to fit a polynomial and include regularization terms, I can force it to use fewer coefficients, making the model more rigid. Bayesian linear regression gives you a way to tune this rigidity and flexibility, helping you find the best solutions for your particular conditions.''')
    st.markdown(r'''''')
    st.markdown(r'''This Bayesian treatment of linear regression is powerful because it provides not just point estimates, but full distributions. Think about applications like the M-sigma relation or the Kennicutt-Schmidt law in astronomy. Instead of just saying "the slope is -1.5," you can say "the slope is -1.5 ± 0.21," providing a measure of uncertainty that's crucial in scientific work.''')
    st.markdown(r'''''')
    st.markdown(r'''### Posterior Predictive Distribution''')
    st.markdown(r'''''')
    st.markdown(r'''Now that we've derived the posterior for our model parameters, we can also get the posterior predictive distribution for new inputs. This is where the power of Bayesian linear regression really shines. For a new input $x^*$, we can calculate:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(y^* | x^*, \mathcal{D}) = \int p(y^* | x^*, w) p(w | \mathcal{D}) dw$''')
    st.markdown(r'''''')
    st.markdown(r'''This integral is analytically tractable for linear regression with Gaussian priors, resulting in:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(y^* | x^*, \mathcal{D}) = \mathcal{N}(y^* | \phi(x^*)^T m_n, \sigma^2 + \phi(x^*)^T S_n \phi(x^*))$''')
    st.markdown(r'''''')
    st.markdown(r'''This gives us not just a point prediction, but a full distribution over possible $y^*$ values for the new input $x^*$, capturing our uncertainty.''')
    st.markdown(r'''''')
    st.markdown(r'''The beauty of this approach is its expansiveness and simplicity. In linear algebra, this becomes just one line of code. The Gaussian distribution's properties make these calculations tractable - the product and integral of Gaussians remain Gaussian.''')
    st.markdown(r'''''')
    st.markdown(r'''## Tutorial: Inferring Stellar Temperatures''')
    st.markdown(r'''''')
    st.markdown(r'''Let me illustrate why this is so important, not just in industry but also in astronomy. Consider a practical example we'll explore in the tutorial: inferring stellar temperatures from spectra. In this case, our input $X$ is the spectrum, which is very high-dimensional - we might have 7,000 pixels. But the dimensionality doesn't matter because in linear regression, everything is analytic. Our output $Y$ is the temperature we're trying to infer.''')
    st.markdown(r'''''')
    st.markdown(r'''Here's how we'd approach this:''')
    st.markdown(r'''1. Split the data into training and test sets to ensure fair comparisons.''')
    st.markdown(r'''2. Use a subset of stars with known temperatures as our training data.''')
    st.markdown(r'''3. Apply our Bayesian linear regression model to infer temperatures for the other spectra.''')
    st.markdown(r'''''')
    st.markdown(r'''Even if we simplify to just the maximum likelihood solution, it's still powerful. With just one line of code, you can say, "I can get the temperature of the star, and it's 5000 ± 130 Kelvin." For those working with stellar spectra, you'll appreciate the significance of this, especially given the complex pipelines often built for this purpose.''')
    st.markdown(r'''''')
    st.markdown(r'''When we plot the true temperatures against our predicted ones, including error bars, you'll see they follow quite well, with the one-sigma error bars generally encompassing the scatter. Some outliers exist, which isn't surprising given that the relationship between temperature and spectrum isn't perfectly linear.''')
    st.markdown(r'''''')
    st.markdown(r'''## Conclusion''')
    st.markdown(r'''''')
    st.markdown(r'''What's remarkable is that we've achieved this with such a simple model. We're only using $x$ and $x$-squared as features, so instead of $n$-pixel features, we have $2n$-pixel features. But the dimensionality increase doesn't matter - it's still just one line of code to solve. In fact, there are more lines of code to calculate the error than to solve the main problem!''')
    st.markdown(r'''''')
    st.markdown(r'''This approach should always be your first line of attack for any regression problem. Instead of spending months building a complex pipeline, you can quickly check the feasibility and get a sense of the uncertainties involved with this one-liner.''')
    st.markdown(r'''''')
    st.markdown(r'''In next week's lecture, we'll move on to classification, still within the linear and Bayesian regime, extending these principles to categorical data. Remember, the power of this method lies in its simplicity, analytical tractability, and ability to quantify uncertainty - all crucial aspects in scientific work, in astronomy and beyond.''')
    st.markdown(r'''''')
    st.markdown(r'''''')

#if __name__ == '__main__':
show_page()