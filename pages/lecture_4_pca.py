import streamlit as st
from streamlit_app import navigation_menu

def show_page():

    # Page Title
    st.title('Unsupervised Learning: Dimension Reduction - Principal Component Analysis')
    navigation_menu()

    # # Embed the external HTML page
    # st.info('''
    # Course Slides
    # ''')
    # slideshare_embed_code = """
    #     <iframe src="https://www.slideshare.net/slideshow/embed_code/key/rZdRwkSEXE2eR1" 
    #             width="700" height="394" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" 
    #             style="border:0px solid #000; border-width:0px; margin-bottom:0px; max-width: 100%;" allowfullscreen> 
    #     </iframe> 
    # """
    # components.html(slideshare_embed_code, height=412)

    # st.markdown('---')
    
    st.markdown(r'''''')
    st.markdown(r'''Welcome back to our third lecture in this series on machine learning in astronomy. Today, we'll explore another crucial aspect of unsupervised learning: dimension reduction. In our previous sessions, we discussed supervised learning, focusing on the mapping from $x$ to $y$, and we began our exploration of unsupervised learning through clustering and density estimation using Gaussian Mixture Models (GMM). Now, we'll delve into the concept of dimension reduction, with a particular focus on Principal Component Analysis (PCA).''')
    st.markdown(r'''''')
    st.markdown(r'''## The Fundamentals of PCA''')
    st.markdown(r'''''')
    st.markdown(r'''PCA is a fundamental tool for dimension reduction. While many of you may be familiar with this concept, the underlying mathematics is more complex than it might initially appear. The algorithm itself is straightforward, but it's essential to understand the mathematical principles behind it.''')
    st.markdown(r'''''')
    st.markdown(r'''The core idea of dimension reduction, especially in PCA, is to project data from a high-dimensional space onto a lower-dimensional manifold. To comprehend this process, we need to discuss vector projection and clearly define our objectives. As with any machine learning technique, we must establish a loss function. In PCA, we make the assumption that the directions with maximum variance contain the most information, and we'll explore how to formulate this mathematically.''')
    st.markdown(r'''''')
    st.markdown(r'''Once we understand the mathematical foundations, we'll see that the algorithm relies on two key steps: calculating the covariance of the data and diagonalizing matrices. This process allows us to determine the directions of interest. However, it does require a solid understanding of linear algebra, particularly concepts such as diagonalization, eigenvectors, and eigenvalues. If these topics are unfamiliar, I recommend reviewing them. The 3Blue1Brown video series on linear algebra provides an excellent visual explanation of these concepts.''')
    st.markdown(r'''''')
    st.markdown(r'''## Motivation for Dimension Reduction in Astronomy''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's discuss the motivation behind dimension reduction. Many datasets, both within and outside of astronomy, are high-dimensional. Images and spectra can have thousands of pixels, while the elemental abundances of stars might have dozens of dimensions. It's challenging to understand or visualize the collective behavior or properties from such high-dimensional data. While we can plot individual examples, discerning overall trends and properties becomes difficult. Dimension reduction helps us compress this information to better understand the underlying data structure.''')
    st.markdown(r'''''')
    st.markdown(r'''There are several benefits to dimension reduction. Primarily, it addresses the issue of storage efficiency, as high-dimensional data is expensive to store. In astronomy, we often encounter overcomplete data, meaning that many dimensions (pixels or properties) are highly correlated. This stems from the fact that astrophysical processes are governed by relatively simple principles. While astrophysics might seem complex to the public, it's actually one of the more straightforward sciences because its guiding principles are often low-dimensional.''')
    st.markdown(r'''''')
    st.markdown(r'''Consider, for example, the prediction of binary star orbits. These can be determined using a set of equations based on fundamental physical principles. In contrast, predicting the motion of a complex system like a human being is far more challenging. In astrophysics, most phenomena we observe result from a finite set of physical processes that generate the observations. Consequently, they are, by definition, relatively low-dimensional. This gives us hope that we can project our high-dimensional data into a more physically meaningful, low-dimensional space.''')
    st.markdown(r'''''')
    st.markdown(r'''## A Basic Example of Dimension Reduction''')
    st.markdown(r'''''')
    st.markdown(r'''Let's examine a basic example of dimension reduction. Imagine we have two features, $x_1$ and $x_2$. If most of the variance is in $x_1$, with $x_2$ showing only small variations, we could potentially retain most of the information by keeping only the value of $x_1$ and disregarding $x_2$. This approach assumes that information is primarily contained in the variability of the objects, which is often a reasonable assumption.''')
    st.markdown(r'''''')
    st.markdown(r'''In practice, however, the situation is usually more complex. Often, all dimensions contain informative data, but they are highly correlated. A relevant example is real estate pricing. If we plot house prices against their areas, we expect to see a strong correlation – larger houses tend to be more expensive. In this case, both pieces of information are informative, but knowing one allows us to predict the other with reasonable accuracy. This suggests we could potentially store this information in a single dimension rather than two.''')
    st.markdown(r'''''')
    st.markdown(r'''The challenge lies in capturing not just the values themselves, but also the relationship between them – the direction of the correlation or degeneracy. This is where PCA becomes particularly useful. Instead of working with the original axes, we can rotate our coordinate system to align with the direction of maximum variance. In this new frame, we might call the primary axis $x_1$ and the secondary axis $x_2$. By storing the value along $x_1$ and discarding $x_2$, we achieve two goals simultaneously: we compress the information and determine the direction of the degeneracy through our principal component.''')
    st.markdown(r'''''')
    st.markdown(r'''## Applications of PCA in Astronomy''')
    st.markdown(r'''''')
    st.markdown(r'''Now that we've covered the basics of Principal Component Analysis (PCA), let's explore its practical applications in astronomy. Despite its simplicity, PCA remains a powerful tool with numerous cutting-edge applications in our field.''')
    st.markdown(r'''''')
    st.markdown(r'''Here is an example from my own study. The study conducted about a decade ago utilized PCA on a dataset of just 200 stars - this was before the advent of large surveys like GALAH. What's remarkable is that the core analysis was accomplished with merely five lines of code, yet the paper has garnered significant attention in the field.''')
    st.markdown(r'''''')
    st.markdown(r'''In this study, the objective wasn't data compression per se, given the relatively small dataset of 300 stars across 20 dimensions. Instead, the focus was on a fascinating aspect of stellar chemistry: the high correlation among various chemical properties of stars. This correlation stems from the fact that most stellar chemical compositions are inherited from previous generations of supernovae.''')
    st.markdown(r'''''')
    st.markdown(r'''The hypothesis was that if there were only a limited number of supernova types, we should observe stellar chemical compositions forming a low-dimensional manifold in the high-dimensional space of elemental abundances. To illustrate this concept, consider a scenario where only one type of supernova exists, always producing the same relative elemental ratios. In this case, as more supernovae occur, newly formed stars would only populate a single direction in this chemical space.''')
    st.markdown(r'''''')
    st.markdown(r'''When we consider multiple supernova types, we begin to see a manifold forming in the 30-dimensional space of elemental abundances. By analyzing the directions of the principal components, we can infer information about the diversity of supernova types contributing to galactic chemical evolution.''')
    st.markdown(r'''''')
    st.markdown(r'''One of the key findings in the paper was evidence supporting the existence of what we term "hypernovae" - highly energetic supernovae that contribute a distinct direction in this chemical space. This discovery added a new dimension, quite literally, to our understanding of stellar nucleosynthesis and galactic chemical enrichment.''')
    st.markdown(r'''''')
    st.markdown(r'''While this work is now a decade old, the application of PCA in astronomy continues to evolve and yield important results. A more recent example involves a slightly more advanced technique called latent factor analysis. This method was employed in a study published recently, focusing on quasar continua extraction - a topic some of you might be exploring in your own research.''')
    st.markdown(r'''''')
    st.markdown(r'''In practice, analyzing quasar spectra is more complex than our earlier stellar abundance example. We're not only dealing with the observations and the intrinsic quasar continuum but also absorption from the intergalactic medium. The paper I'm referring to combined three techniques that we'll cover in upcoming lectures: PCA, Gaussian processes, and neural networks. By integrating these methods, impressive results were achieved in reconstructing quasar continua.''')
    st.markdown(r'''''')
    st.markdown(r'''This recent work underscores an important point: while today's lecture focuses on one of the simplest techniques in our toolkit - PCA, implementable in just a few lines of code - these fundamental methods remain incredibly powerful when applied thoughtfully to well-defined problems in astronomy.''')
    st.markdown(r'''''')
    st.markdown(r'''## Mathematical Foundations of PCA''')
    st.markdown(r'''''')
    st.markdown(r'''Now that we've explored some applications of Principal Component Analysis (PCA) in astronomy, let's delve into its mathematical foundations. We'll set up the problem formally and discuss some key insights that make PCA such a powerful tool.''')
    st.markdown(r'''''')
    st.markdown(r'''In our PCA setup, we're dealing solely with features $X$, without corresponding labels $Y$. We assume our dataset is independent and identically distributed (IID). This means that while individual data points may differ, they are all drawn from the same underlying distribution. To illustrate this concept, consider rolling a die multiple times. Each roll is independent, and while the outcomes vary, they all come from the same uniform distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''Formally, we consider an IID dataset $x = \{x_1, ..., x_n\}$, where $x_n \in \mathbb{R}^D$. A crucial assumption for PCA is that our data is centered with a mean of zero. This condition is necessary for many of the proofs we'll encounter. In practice, if your data isn't centered (which is common in astronomy - think of spectra where flux is always positive), you can simply shift each variable to have a mean of zero.''')
    st.markdown(r'''''')
    st.markdown(r'''The objective of PCA is to find a projection $z$ of our data $x$, where $z$ has a significantly lower dimensionality than $x$, while remaining as similar as possible to the original data. This projection should capture the most important information in our dataset.''')
    st.markdown(r'''''')
    st.markdown(r'''A key insight of PCA is that retaining the most information after data compression is equivalent to capturing the largest amount of variance in the low-dimensional representation. Consider our earlier example of a 2D dataset where most of the variance is along $x_1$. Intuitively, we'd want to keep the $x_1$ direction and discard $x_2$, as $x_1$ contains more information in terms of variance.''')
    st.markdown(r'''''')
    st.markdown(r'''However, it's important to note that this approach might not always be appropriate in astronomical applications. There may be cases where the smaller variances are of greater interest due to known physical mechanisms. In such scenarios, you might need to reweight your features accordingly.''')
    st.markdown(r'''''')
    st.markdown(r'''The general goal of PCA is to find a rotation of our coordinate system that captures both the degeneracies in our data and its intrinsic dimensionality. The challenge lies in determining this projection a priori. While it's straightforward in two dimensions, finding the correct projection vectors becomes increasingly difficult as we move to higher-dimensional spaces, which are common in astronomical datasets.''')
    st.markdown(r'''''')
    st.markdown(r'''## Mathematical Formulation of PCA''')
    st.markdown(r'''''')
    st.markdown(r'''Mathematically, we're seeking a direction, represented by a vector $b$, onto which we can project our data. This projection should preserve the maximum variance of the original dataset. The key is to find this $b$ in an algorithmic way that's independent of the dimensionality of our data.''')
    st.markdown(r'''''')
    st.markdown(r'''To formalize this, we can express our objective as finding a projection matrix $B$ that maximizes the variance of our projected data $z = Bx$, subject to certain constraints. This optimization problem forms the core of PCA.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's start with a quick revision on vector projection. If you were at my colloquium earlier, you'll remember we can use dot products here, which is very helpful. To project a vector $x$ onto a vector $b$ in a $D$-dimensional space, we use:''')
    st.markdown(r'''''')
    st.markdown(r'''$\hat{x} = (b^Tx)b$''')
    st.markdown(r'''''')
    st.markdown(r'''Here, $b^Tx$ gives us the scalar coefficient, and $b$ provides the direction.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, something that will be crucial in this lecture is the covariance of the data. The data covariance essentially tells us how much spread we have in the data. By definition, for a mean-zero random variable, the covariance matrix is:''')
    st.markdown(r'''''')
    st.markdown(r'''$S = \dfrac{1}{N} \sum_{n=1}^N x_nx_n^T = Cov(X, X) \in \mathbb{R}^{D\times D}$''')
    st.markdown(r'''''')
    st.markdown(r'''This $S$ is actually an $R^{D\times D}$ matrix. Each time you do this dot product, you get a matrix because you're doing a column dot row operation.''')
    st.markdown(r'''''')
    st.markdown(r'''Okay, so our goal here, mathematically speaking, is to find a set of $b$'s that allows us to decompose our vector $x$. We want to write:''')
    st.markdown(r'''''')
    st.markdown(r'''$x = \sum_{d=1}^D (b^d \cdot x)b^d = \sum_{m=1}^M (b_m \cdot x)b_m + \sum_{j=M+1}^D (b_j \cdot x)b_j$''')
    st.markdown(r'''''')
    st.markdown(r'''The first sum here is what we want to keep - it's our projection onto the lower-dimensional manifold. The second sum is what we'll discard. Our challenge is figuring out how to find this set of $b^d$ vectors.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's denote $z_m = b_m^Tx$ as the coefficient of the $m$-th projection, with $b_1, ..., b_m \in \mathbb{R}^D$.''')
    st.markdown(r'''''')
    st.markdown(r'''## Maximum Variance Formalism''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's talk about the maximum variance formalism and how we want to formulate our loss function. We'll start by focusing on finding the first principal component. We want to find a direction in our D-dimensional space, let's call it $b_1$, onto which we can project all our data such that it keeps the most variance.''')
    st.markdown(r'''''')
    st.markdown(r'''Mathematically, we want to maximize:''')
    st.markdown(r'''''')
    st.markdown(r'''$V_1 \equiv \mathbb{V}[z_1] = \dfrac{1}{N} \sum_{n=1}^N z_{1n}^2$''')
    st.markdown(r'''''')
    st.markdown(r'''where $z_1 = b_1^Tx$ is the first coordinate of our low-dimensional representation.''')
    st.markdown(r'''''')
    st.markdown(r'''So, how do we find $b_1$ based on this? We can substitute the definition of $z_1$ into our variance equation:''')
    st.markdown(r'''''')
    st.markdown(r'''$V_1 = \dfrac{1}{N} \sum_{n=1}^N (b_1^Tx_n)^2 = \dfrac{1}{N} \sum_{n=1}^N b_1^Tx_nx_n^Tb_1 = b_1^T(\dfrac{1}{N} \sum_{n=1}^N x_nx_n^T)b_1 = b_1^TSb_1$''')
    st.markdown(r'''''')
    st.markdown(r'''Here, $S$ is our data covariance matrix that we defined earlier.''')
    st.markdown(r'''''')
    st.markdown(r'''We want to maximize $V_1 = b_1^TSb_1$, but we have a constraint: the norm of $b_1$ should be 1. In other words, we want to find:''')
    st.markdown(r'''''')
    st.markdown(r'''$\max b_1^TSb_1$ subject to $||b_1||^2 = 1$''')
    st.markdown(r'''''')
    st.markdown(r'''To do this, we use the method of Lagrange multipliers. We introduce an auxiliary variable, $\lambda_1$, and form what we call the Lagrangian:''')
    st.markdown(r'''''')
    st.markdown(r'''$\mathcal{L}(\lambda_1, b_1) = b_1^TSb_1 + \lambda_1(1 - b_1^Tb_1)$''')
    st.markdown(r'''''')
    st.markdown(r'''Now, we want to optimize this term with respect to both $b_1$ and $\lambda_1$. Just like in mechanics, we do this by differentiating the Lagrangian with respect to these variables and setting the derivatives to zero.''')
    st.markdown(r'''''')
    st.markdown(r'''If we differentiate with respect to $b_1$, we get:''')
    st.markdown(r'''''')
    st.markdown(r'''$\frac{\partial\mathcal{L}}{\partial b_1} = 2b_1^TS - 2\lambda_1b_1^T$''')
    st.markdown(r'''''')
    st.markdown(r'''And if we differentiate with respect to $\lambda_1$, we get:''')
    st.markdown(r'''''')
    st.markdown(r'''$\frac{\partial\mathcal{L}}{\partial\lambda_1} = 1 - b_1^Tb_1$''')
    st.markdown(r'''''')
    st.markdown(r'''Now, this second term is quite straightforward. If we set it to zero, it just gives us our original constraint that the norm of $b_1$ is 1.''')
    st.markdown(r'''''')
    st.markdown(r'''The first term is more interesting. If we set it to zero, we have:''')
    st.markdown(r'''''')
    st.markdown(r'''$b_1^TS = \lambda_1b_1^T$''')
    st.markdown(r'''''')
    st.markdown(r'''We can take the transpose of the whole equation. Remember, for a scalar like $\lambda_1$, the transpose doesn't change anything. So we get:''')
    st.markdown(r'''''')
    st.markdown(r'''$S^Tb_1 = \lambda_1b_1$''')
    st.markdown(r'''''')
    st.markdown(r'''Now, $S$ is our covariance matrix, which is symmetric by definition. So $S = S^T$, and our equation becomes:''')
    st.markdown(r'''''')
    st.markdown(r'''$Sb_1 = \lambda_1b_1$''')
    st.markdown(r'''''')
    st.markdown(r'''Let's recap what we've done here. We started not knowing what $b_1$ should be, but wanting to optimize the variance of our projected data. Through this process, we've found that to satisfy our optimization, $b_1$ needs to satisfy this equation.''')
    st.markdown(r'''''')
    st.markdown(r'''For those of you who've taken linear algebra, this should look very familiar. When we have a matrix (in this case, $S$) multiplied by a vector ($b_1$) equaling that same vector multiplied by a scalar ($\lambda_1$), what we have is an eigenvector equation. The vector $b_1$ that satisfies this is an eigenvector of $S$, and $\lambda_1$ is the corresponding eigenvalue.''')
    st.markdown(r'''''')
    st.markdown(r'''So, we've determined that $b_1$ must be an eigenvector of our covariance matrix $S$. But which one? We have multiple eigenvectors, so which one will maximize our variance?''')
    st.markdown(r'''''')
    st.markdown(r'''Well, now that we know $b_1$ is an eigenvector, we can substitute $Sb_1 = \lambda_1b_1$ back into our original variance equation:''')
    st.markdown(r'''''')
    st.markdown(r'''$V_1 = b_1^TSb_1 = b_1^T(\lambda_1b_1) = \lambda_1b_1^Tb_1 = \lambda_1$''')
    st.markdown(r'''''')
    st.markdown(r'''Remember, $b_1^Tb_1 = 1$ because of our normalization constraint.''')
    st.markdown(r'''''')
    st.markdown(r'''What this tells us is that the variance of our data projected onto this one-dimensional subspace is equal to the eigenvalue associated with the eigenvector $b_1$.''')
    st.markdown(r'''''')
    st.markdown(r'''Therefore, to maximize the variance of our low-dimensional representation, we should choose the eigenvector associated with the largest eigenvalue of the data covariance matrix. This eigenvector is what we call the first principal component.''')
    st.markdown(r'''''')
    st.markdown(r'''## Proof by Induction for Higher-Order Principal Components''')
    st.markdown(r'''''')
    st.markdown(r'''Okay, let's talk about proof by induction. This is a very useful trick to prove a sequence of statements. Here's how it works:''')
    st.markdown(r'''''')
    st.markdown(r'''1. First, you show that the statement is true for the base case, usually $n = 1$ or $n = 0$. We've already done this for PCA.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Then you have the inductive step. You assume the statement is true for $n = m - 1$, and prove it's true for $n = m$.''')
    st.markdown(r'''''')
    st.markdown(r'''This approach is quite logical. If we prove the base case and the inductive step, we can show that the statement is true for all $m$. That's what we want to do here.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's recap what we're trying to prove. We've already proven the base case. Now we want to prove the induction case. Here's what we're saying:''')
    st.markdown(r'''''')
    st.markdown(r'''If the first $m-1$ eigenvectors we should keep are the ones corresponding to the $m-1$ largest eigenvalues of $S$, then the next one we should keep is the $m$-th eigenvector. We want to show that this $m$-th direction will indeed maximize the remaining variance in the data.''')
    st.markdown(r'''''')
    st.markdown(r'''I know this sounds complicated, but let's break it down with some math.''')
    st.markdown(r'''''')
    st.markdown(r'''To make things more compact, we can represent our data $X$ as a matrix in $\mathbb{R}^{D \times N}$ space, where each column is a data point. If we want to project all the data onto one direction, we can do the same dot product as before, but instead of using one column at a time, we'll dot the whole rectangular matrix with $b_1$.''')
    st.markdown(r'''''')
    st.markdown(r'''If we've already taken $m-1$ components, the projection onto these components will be:''')
    st.markdown(r'''''')
    st.markdown(r'''$\sum_{i=1}^{m-1} b_i(b_i^T X)$''')
    st.markdown(r'''''')
    st.markdown(r'''The remaining data (the part not explained by these $m-1$ components) will be:''')
    st.markdown(r'''''')
    st.markdown(r'''$\hat{X} = X - \sum_{i=1}^{m-1} b_i(b_i^T X)$''')
    st.markdown(r'''''')
    st.markdown(r'''This can also be written more compactly as:''')
    st.markdown(r'''''')
    st.markdown(r'''$\hat{X} = X - BB^T X$''')
    st.markdown(r'''''')
    st.markdown(r'''where $B$ is a matrix with the first $m-1$ eigenvectors as its columns.''')
    st.markdown(r'''''')
    st.markdown(r'''What we're trying to do here is find the direction that will maximize the extraction of variance from $\hat{X}$. We'll call the covariance of this new transformed data $\hat{S}$. For the induction step to be true, we want to show that the direction we should keep is indeed the $m$-th eigenvector of the original $S$.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's take a step back and summarize:''')
    st.markdown(r'''1. We're assuming the first $m-1$ components are correct.''')
    st.markdown(r'''2. We've projected our data onto this $(m-1)$-dimensional subspace.''')
    st.markdown(r'''3. We have the remaining information, which we call $\hat{X}$.''')
    st.markdown(r'''4. We calculate the covariance of $\hat{X}$, which we call $\hat{S}$.''')
    st.markdown(r'''5. We want to show that among the remaining directions, we should take the one that corresponds to the $m$-th eigenvector of the original data covariance $S$.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's look at this new covariance matrix $\hat{S}$. There are some mathematical properties we need to use here. I know there are lots of bits and pieces of math involved, and it can be challenging to put them all together. But don't worry if it's not immediately clear - these concepts can be revisited and studied further if needed.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's dive into the nitty-gritty of our proof. We want to take the covariance of $\hat{X}$, which we know can be written as $X - BB^TX$. One nice property of the covariance operation is that it's bilinear. This means we can expand it out into different quadratic terms and cross terms. It might seem a bit fuzzy, but if you write out the definition of covariance in terms of the sum over $n$ from 1 to $N$ and do the dot product, you'll see this is true.''')
    st.markdown(r'''''')
    st.markdown(r'''So, let's break this down step by step:''')
    st.markdown(r'''''')
    st.markdown(r'''1. First term: $Cov(X, X)$''')
    st.markdown(r'''   This is just our original covariance matrix $S$. Easy enough.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Last term: $Cov(BB^TX, BB^TX)$''')
    st.markdown(r'''   This is the tricky one, so let's start here. Because covariance is bilinear, we can pull out constant matrices from both sides. So, $Cov(AX, BY) = ACov(X, Y)B^T$. Applying this, we get:''')
    st.markdown(r'''''')
    st.markdown(r'''   $Cov(BB^TX, BB^TX) = BB^TCov(X, X)BB^T = BB^TSBB^T$''')
    st.markdown(r'''''')
    st.markdown(r'''   Now, remember what $B$ is - it's a matrix whose columns are the $m-1$ eigenvectors we've already extracted. These are eigenvectors of $S$, which means $SB = \Lambda B$, where $\Lambda$ is a diagonal matrix of eigenvalues.''')
    st.markdown(r'''''')
    st.markdown(r'''   So, $BB^TSBB^T = BB^T\Lambda BB^T = \Lambda BB^T$''')
    st.markdown(r'''''')
    st.markdown(r'''   The last step is because $B^TB = I$ (identity matrix). Why? Because $B$ contains eigenvectors of a symmetric, positive definite matrix (our covariance matrix $S$), which are orthogonal and normalized.''')
    st.markdown(r'''''')
    st.markdown(r'''3. Middle term: $-2Cov(X, BB^TX)$''')
    st.markdown(r'''   This one's easier. We can pull out the constant $BB^T$:''')
    st.markdown(r'''''')
    st.markdown(r'''   $-2Cov(X, BB^TX) = -2Cov(X, X)BB^T = -2SBB^T = -2\Lambda BB^T$''')
    st.markdown(r'''''')
    st.markdown(r'''Putting it all together, we get:''')
    st.markdown(r'''''')
    st.markdown(r'''$\hat{S} = Cov(\hat{X}, \hat{X}) = S - \Lambda BB^T$''')
    st.markdown(r'''''')
    st.markdown(r'''Now, here's where the magic happens. It's actually pretty easy to check that $\hat{S}$ has the same eigenvectors as $S$. Why? If we multiply this expression by any eigenvector of $S$, the $BB^T$ term will either give us the vector back (for the first $m-1$ eigenvectors) or zero (for the rest), and $S$ will give us the eigenvalue times the vector. ''')
    st.markdown(r'''''')
    st.markdown(r'''But there's a key difference. For the first $m-1$ eigenvectors, when we do $\hat{S}b$, we get $\lambda b - \lambda b = 0$. Their eigenvalues in $\hat{S}$ have become zero! ''')
    st.markdown(r'''''')
    st.markdown(r'''So what's the largest eigenvalue of $\hat{S}$? It's the $m$-th largest eigenvalue of $S$, corresponding to the $m$-th eigenvector of $S$. We've effectively "used up" the first $m-1$ eigenvalues, leaving the $m$-th as the largest remaining one.''')
    st.markdown(r'''''')
    st.markdown(r'''And there you have it! We've shown that after accounting for the first $m-1$ principal components, the $m$-th principal component is indeed the $m$-th eigenvector of our original covariance matrix $S$. Proof complete. QED.''')
    st.markdown(r'''''')
    st.markdown(r'''I know this was a lot of math, and it might seem a bit hairy. But the key idea is actually quite intuitive: as we extract each principal component, we're removing that direction of variation from our data, leaving the next most important direction as the one to choose next.''')
    st.markdown(r'''''')
    st.markdown(r'''This proof gives us confidence that PCA is doing exactly what we want it to do: capturing the most important directions of variation in our data, one at a time, in order of importance. In astronomical applications, this means we can represent complex, high-dimensional data (like spectra or multi-wavelength observations) using just a few key components, making our analyses more tractable and often revealing underlying physical processes.''')
    st.markdown(r'''''')
    st.markdown(r'''Any questions on this proof? I know it's dense, but understanding it really helps solidify your grasp of what PCA is doing under the hood.''')
    st.markdown(r'''''')
    st.markdown(r'''## Practical Implementation of PCA''')
    st.markdown(r'''''')
    st.markdown(r'''If all this seems too mathematical, don't worry too much about it. Let's focus on what you really need to do to implement PCA:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Take your data.''')
    st.markdown(r'''2. Calculate the covariance matrix. That's just one line of NumPy.''')
    st.markdown(r'''3. Do the diagonalization. That's another line of NumPy.''')
    st.markdown(r'''4. Keep the first $m$ eigenvectors, and you're done.''')
    st.markdown(r'''''')
    st.markdown(r'''That's the main point. If you want to project the data to keep the most variance, it's as simple as taking the covariance of the data, doing the diagonalization, and keeping the $m$ eigenvectors that correspond to the largest eigenvalues.''')
    st.markdown(r'''''')
    st.markdown(r'''But there's still one question to answer: how do we choose $m$? If we take $m$ equal to $D$ (the original number of dimensions), we're not doing any dimension reduction at all. Of course, we want $m$ to be smaller than $D$.''')
    st.markdown(r'''''')
    st.markdown(r'''Luckily, to determine how many components to keep, we've already done all the necessary calculations. Why? Because we've shown that when we project onto the $m$-th principal component, the variance captured by this projection is $\lambda_m$ (the $m$-th eigenvalue). And since all the eigenvectors are orthogonal (because $S$ is symmetric), we can just sum up the variances from all the directions.''')
    st.markdown(r'''''')
    st.markdown(r'''So, if we want to know how much variance we've captured up to the $m$-th dimension, we just sum the variances from 1 to $m$:''')
    st.markdown(r'''''')
    st.markdown(r'''$V_m = \sum_{i=1}^m \lambda_i$''')
    st.markdown(r'''''')
    st.markdown(r'''This gives us the variance we've kept by stopping at $m$. We also know the total variance - it's just the sum of all eigenvalues up to $D$:''')
    st.markdown(r'''''')
    st.markdown(r'''$V_D = \sum_{i=1}^D \lambda_i$''')
    st.markdown(r'''''')
    st.markdown(r'''Put differently, if we want to know how much variance we've lost by compressing the data, we can sum from $m+1$ to $D$:''')
    st.markdown(r'''''')
    st.markdown(r'''$J_m = \sum_{j=m+1}^D \lambda_j = V_D - V_m$''')
    st.markdown(r'''''')
    st.markdown(r'''Instead of using these absolute quantities, it's often more useful to look at relative values. We can define:''')
    st.markdown(r'''''')
    st.markdown(r'''- Relative variance captured: $V_m/V_D$''')
    st.markdown(r'''- Relative variance lost by compression: $1 - V_m/V_D$''')
    st.markdown(r'''''')
    st.markdown(r'''## PCA in Practice: The MNIST Example''')
    st.markdown(r'''''')
    st.markdown(r'''Okay, let's recap what we've covered today. To run PCA:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Start with your data points.''')
    st.markdown(r'''2. Shift the data to have a mean of zero.''')
    st.markdown(r'''3. If you have different dimensions, you might need to normalize the units. This is a choice you need to make. Is 1 Kelvin more important than 1 gram? Or is 10 Kelvin equal to 1 gram? That's up to you, but the easiest way is to just divide each dimension by its standard deviation.''')
    st.markdown(r'''4. Calculate the data covariance matrix.''')
    st.markdown(r'''5. Do the diagonalization to find the eigenvectors and eigenvalues.''')
    st.markdown(r'''6. Choose how many components to keep based on the cumulative variance explained.''')
    st.markdown(r'''7. Project your data onto these principal components.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's look at a concrete example that you're probably familiar with: the MNIST dataset. MNIST is a small sample set with 60,000 examples of handwritten digits. It's often used for digit classification tasks. Each image is 28 by 28 pixels, giving us a total of 784 pixels. This means we can treat each observation as a 784-dimensional vector.''')
    st.markdown(r'''''')
    st.markdown(r'''I want to use MNIST not just as an example of PCA, but also to address a common misconception in astronomy. There's a tendency to equate machine learning with big data, as if they're synonymous. We often hear, "Why do we do machine learning? Big data. Why do we need big data? Machine learning." But the reality is, the Venn diagram between these two concepts doesn't overlap as much as you might think.''')
    st.markdown(r'''''')
    st.markdown(r'''You can absolutely do big data without deep learning. Classical data science and statistics can do a pretty good job dealing with big data. Conversely, you can do machine learning without big data.''')
    st.markdown(r'''''')
    st.markdown(r'''MNIST is a perfect example of this. It's still widely used, even in cutting-edge research. If you want to understand the theoretical underpinnings of neural networks, people still look at MNIST. And MNIST is not big data - you could attach the entire dataset to an email without issue. So you're definitely not doing big data, but you are still doing machine learning. These are very different concepts, but somehow we tend to mix them up.''')
    st.markdown(r'''''')
    st.markdown(r'''Okay, back to MNIST. Let's say we take just the images of the digit 8, run PCA on this subset, do the diagonalization, and calculate the cumulative sum of the eigenvalues. We get something like this [referring to the graph]. What this tells us is that if we want to keep 50 dimensions out of the 784, we'll capture about 70% of the variance. If we keep up to about 100 vectors, we'll preserve about 90% of the variance.''')
    st.markdown(r'''''')
    st.markdown(r'''By plotting these cumulative sums of the eigenvalues, we get a sense of how many dimensions we need to keep to reconstruct our data with good fidelity, and how much information we're throwing away (assuming that information equates to variance).''')
    st.markdown(r'''''')
    st.markdown(r'''This example also shows us that even seemingly simple datasets like MNIST are actually much more complex than many astronomical datasets. Look at how slowly the variance grows - even out of the 784 dimensions, you need to keep about 150 to capture most of the variance. That's about one-fifth of the original dimensions.''')
    st.markdown(r'''''')
    st.markdown(r'''Contrast this with quasar spectra. There, you start with 7,000 dimensions, but you can compress it down to just five dimensions and still capture almost all the variance. This really drives home how simple astronomy can be in terms of intrinsic dimensionality.''')
    st.markdown(r'''''')
    st.markdown(r'''This simplicity in astronomical data is actually a good thing. It means that despite the vast amounts of data we collect, the underlying physical processes are often governed by a relatively small number of parameters. This is why techniques like PCA can be so powerful in astronomy - they help us cut through the noise and focus on the most important aspects of our data.''')
    st.markdown(r'''''')
    st.markdown(r'''## Practical Considerations for High-Dimensional Data''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's wrap up our discussion on PCA with some crucial practical considerations, especially for high-dimensional data.''')
    st.markdown(r'''''')
    st.markdown(r'''To perform PCA, we need to calculate the data covariance matrix and then diagonalize it. This matrix is $D$ by $D$, where $D$ is the number of features. As we've mentioned, $D$ can be very large. For example, if you're working with a spectrum that has 10,000 pixels, diagonalizing this matrix has a complexity of $D^3$. That's going to take a while - you might have time to watch a YouTube video while it's running!''')
    st.markdown(r'''''')
    st.markdown(r'''But here's an interesting situation that often comes up in classical machine learning: sometimes the number of data points, $N$, is smaller than $D$. This rarely happens in modern deep learning because you typically need lots of data to train a neural network. But in classical machine learning, you might encounter cases where the data is sparse. For instance, you might want to run PCA on just five spectra, even if each spectrum has 10,000 pixels.''')
    st.markdown(r'''''')
    st.markdown(r'''In this case, you really don't want to diagonalize the $D$ by $D$ matrix. Instead, you want to find a trick to diagonalize an $N$ by $N$ matrix. Turns out, this is possible.''')
    st.markdown(r'''''')
    st.markdown(r'''Why? Well, if $N$ is very small, the $D$ by $D$ matrix can't be full rank. In fact, most of its eigenvalues have to be zero. If you only have $N$ data points, you only need to keep at most $N$ vectors to capture all the information.''')
    st.markdown(r'''''')
    st.markdown(r'''So, is there an $N$ by $N$ matrix that will give us essentially the same results? The challenge is that we've shown that we really want to diagonalize the $S$ matrix, which is $D$ by $D$, and its eigenvalues have physical meaning. So if we substitute it with another matrix, we need to ensure it has the same eigenvalues.''')
    st.markdown(r'''''')
    st.markdown(r'''Here's the trick: Instead of $S = \dfrac{1}{N}XX^T$, we can look at $\dfrac{1}{N}X^TX$. This is an $N$ by $N$ matrix, but it shares the same non-zero eigenvalues as $S$. This might seem like magic, but it's actually a common trick you'll see again when we talk about Gaussian processes. It's called the kernel trick.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's dive deeper into why this works and how we can recover our original eigenvectors. Here's the step-by-step derivation:''')
    st.markdown(r'''''')
    st.markdown(r'''1. We start with our original eigenvalue equation for $S$:''')
    st.markdown(r'''''')
    st.markdown(r'''   $Sb_m = \lambda_mb_m$''')
    st.markdown(r'''''')
    st.markdown(r'''2. Substituting $S = \dfrac{1}{N}XX^T$, we get:''')
    st.markdown(r'''''')
    st.markdown(r'''   $\dfrac{1}{N}XX^Tb_m = \lambda_mb_m$''')
    st.markdown(r'''''')
    st.markdown(r'''3. Now, let's multiply both sides by $X^T$ from the left:''')
    st.markdown(r'''''')
    st.markdown(r'''   $\dfrac{1}{N}X^TXX^Tb_m = \lambda_mX^Tb_m$''')
    st.markdown(r'''''')
    st.markdown(r'''4. Let's define $c_m \equiv X^Tb_m$. This gives us:''')
    st.markdown(r'''''')
    st.markdown(r'''   $\dfrac{1}{N}X^TXc_m = \lambda_mc_m$''')
    st.markdown(r'''''')
    st.markdown(r'''5. This is our new eigenvalue equation in the $N$-dimensional space. Now, let's go back to the original space by multiplying both sides by $X$ from the left:''')
    st.markdown(r'''''')
    st.markdown(r'''   $\dfrac{1}{N}XX^TXc_m = \lambda_mXc_m$''')
    st.markdown(r'''''')
    st.markdown(r'''6. Look at the left side. $XX^T$ is our original $S$ matrix multiplied by $N$. So we can rewrite this as:''')
    st.markdown(r'''''')
    st.markdown(r'''   $S(Xc_m) = \lambda_m(Xc_m)$''')
    st.markdown(r'''''')
    st.markdown(r'''And there we have it! This last equation shows that $Xc_m$ is indeed an eigenvector of our original covariance matrix $S$, with the same eigenvalue $\lambda_m$.''')
    st.markdown(r'''''')
    st.markdown(r'''This derivation proves that even though we're working in the $N$-dimensional space (which is smaller when $N < D$), we can recover the eigenvectors of our original $D$-dimensional space simply by multiplying our $N$-dimensional eigenvectors by $X$.''')
    st.markdown(r'''''')
    st.markdown(r'''This trick is incredibly useful because it allows us to work with a much smaller matrix ($N$ x $N$ instead of $D$ x $D$) when we have fewer data points than dimensions, which is often the case in astronomical spectroscopy or other high-dimensional data scenarios.''')
    st.markdown(r'''''')
    st.markdown(r'''In practice, this means you can:''')
    st.markdown(r'''1. Form the matrix $X^TX$ (which is $N \times N$)''')
    st.markdown(r'''2. Find its eigenvectors $c_m$''')
    st.markdown(r'''3. Multiply each $c_m$ by $X$ to get the corresponding eigenvector of $S$''')
    st.markdown(r'''''')
    st.markdown(r'''This process gives you the same non-zero eigenvalues and the corresponding eigenvectors as if you had worked with the full $D \times D$ matrix, but with potentially much less computational effort.''')
    st.markdown(r'''''')
    st.markdown(r'''## Summary: Key Steps of PCA''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's summarize the key steps of PCA, incorporating this trick for high-dimensional data:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Standardize the data: For each dimension $d$, subtract the mean and divide by the standard deviation. This step is optional, depending on whether it makes sense to consider variance in different units, or if you want to make all dimensions "democratic".''')
    st.markdown(r'''''')
    st.markdown(r'''2. Calculate the covariance matrix: ''')
    st.markdown(r'''   - If $N > D$: $S = \dfrac{1}{N}XX^T$''')
    st.markdown(r'''   - If $N < D$: Use $\dfrac{1}{N}X^TX$ instead''')
    st.markdown(r'''''')
    st.markdown(r'''3. Find the eigenvalues and eigenvectors:''')
    st.markdown(r'''   - If $N > D$: Calculate for $S$ directly''')
    st.markdown(r'''   - If $N < D$: Calculate for $\dfrac{1}{N}X^TX$, then multiply the eigenvectors by $X$ to get the eigenvectors of $S$''')
    st.markdown(r'''''')
    st.markdown(r'''4. Rank the eigenvectors by their eigenvalues. Calculate the cumulative sum of the eigenvalues to decide where to truncate the dimension.''')
    st.markdown(r'''''')
    st.markdown(r'''5. Let $B$ be the matrix of the chosen eigenvectors.''')
    st.markdown(r'''''')
    st.markdown(r'''6. Project your data onto the new basis: $\hat{X} = B(B^TX)$.''')
    st.markdown(r'''''')
    st.markdown(r'''7. Finally, if you standardized your data in step 1, you might want to shift it back to the original scale: $\hat{x}^d \leftarrow \hat{x}^d * \sigma^d + \mu^d$ for each dimension $d$.''')
    st.markdown(r'''''')
    st.markdown(r'''In practice, you'll often use Singular Value Decomposition (SVD), which has a complexity of $N^2D$ or $D^2N$, depending on which is smaller. This is usually a good compromise and works well for most practical cases. However, in extreme cases where $N$ is really large or very small, or $D$ is very large or very small, the trick we've discussed here might still be more efficient.''')
    st.markdown(r'''''')
    st.markdown(r'''And there you have it! That's PCA in a nutshell, including how to handle high-dimensional data efficiently. We've covered a lot of ground today:''')
    st.markdown(r'''''')
    st.markdown(r'''- The concept of dimension reduction''')
    st.markdown(r'''- Vector projection''')
    st.markdown(r'''- The maximum variance perspective''')
    st.markdown(r'''- Data covariance matrices''')
    st.markdown(r'''- Eigenvalues and eigenvectors''')
    st.markdown(r'''- How to implement PCA, even for high-dimensional data''')
    st.markdown(r'''- The kernel trick for cases where $N < D$''')
    st.markdown(r'''''')
    st.markdown(r'''Remember, while the math can get complex, the core idea of PCA is simple: find the directions in your data that capture the most variance. This powerful technique can help you understand the structure of your data, reduce noise, and often reveal underlying physical processes in astronomical datasets.''')
    st.markdown(r'''''')
    st.markdown(r'''In your future research, you'll find that PCA is an invaluable tool in your data analysis toolkit. It's a perfect example of how a relatively simple mathematical concept can lead to profound insights in astronomy.''')
    st.markdown(r'''''')

#if __name__ == '__main__':
show_page()