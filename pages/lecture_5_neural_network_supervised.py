import streamlit as st
from streamlit_app import navigation_menu

def show_page():
    st.set_page_config(page_title="Comp Astro",
                       page_icon="https://raw.githubusercontent.com/teaghan/astronomy-12/main/images/tutor_favicon.png", layout="wide")

    # Page Title
    st.title('Neural Networks - Supervised Learning')
    navigation_menu()

    # Embed the external HTML page
    # st.info('''
    # Course Slides
    # ''')
    # slideshare_embed_code = """
    #     <iframe src="https://www.slideshare.net/slideshow/embed_code/key/KjMfH7sEIc7437" 
    #             width="700" height="394" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" 
    #             style="border:0px solid #000; border-width:0px; margin-bottom:0px; max-width: 100%;" allowfullscreen> 
    #     </iframe> 
    # """
    # components.html(slideshare_embed_code, height=412)

    # st.markdown('---')
    
    st.markdown(r'''''')
    st.markdown(r'''Welcome back, everyone. Let's begin by recapping what we've covered in the previous lectures.''')
    st.markdown(r'''''')
    st.markdown(r'''We've discussed supervised and unsupervised learning in their basic forms. For individual tasks, we introduced some of the most widely used and classical methods: regression and classification for supervised learning, and density estimation and dimension reduction for unsupervised learning.''')
    st.markdown(r'''''')
    st.markdown(r'''From this lecture onwards, we'll start diving into more advanced methods, while still anchoring ourselves in the realm of supervised and unsupervised tasks. This lecture and the next will focus on neural networks. Today, we'll cover neural networks in the context of supervised tasks, and in the following lecture, we'll explore their application to unsupervised tasks. After that, we'll discuss Gaussian processes, again tackling supervised tasks. As you can see, we're following a pattern of revisiting similar problem domains but exploring them with different methods, each offering varying breadth and depth.''')
    st.markdown(r'''''')
    st.markdown(r'''## The Basics of Neural Networks''')
    st.markdown(r'''''')
    st.markdown(r'''Today's topic is neural networks. I always feel a bit hesitant when teaching Neural Networks 101 because it's easy to give people the wrong impression. You might think, "Is this what all the hype is about? Just a few neurons strung together, and magic happens?" That's not the impression I want to give today.''')
    st.markdown(r'''''')
    st.markdown(r'''When we talk about deep learning, we're mostly referring to neural networks, and it's true that much of the current machine learning hype centers around them. However, the field itself is incredibly broad and deep. We'll touch on some exciting research, but we need to start with the basics. It's important to keep in mind that what we discuss today barely scratches the surface of deep learning.''')
    st.markdown(r'''''')
    st.markdown(r'''The basic concept we'll explore is how neural networks address the limitations of classical methods. Once we understand some classical methods, we can discuss their limitations, and neural networks naturally emerge as a solution. We'll also revisit some concepts from earlier lectures, such as weight decay in neural networks, which is similar to the regularization we saw in the first two lectures, and gradient descent, which we touched upon in logistic regression.''')
    st.markdown(r'''''')
    st.markdown(r'''The main focus of today's lecture will be on autodifferentiation and backpropagation. These form the basis of neural networks and explain their current popularity.''')
    st.markdown(r'''''')
    st.markdown(r'''## Limitations of Classical Methods''')
    st.markdown(r'''''')
    st.markdown(r'''Let's start by discussing the limitations of classical methods like linear regression and logistic regression. In these approaches, we assume that the output $y$ depends linearly on the input features $x$, or some transformation of those features, $\phi(x)$. Mathematically, this is expressed as:''')
    st.markdown(r'''''')
    st.markdown(r'''For regression: $y = w^T \phi(x)$''')
    st.markdown(r'''For classification: $y = \sigma(w^T \phi(x))$, where $\sigma$ is the logistic function''')
    st.markdown(r'''''')
    st.markdown(r'''The advantage of linear regression is that everything is purely analytic, from the maximum likelihood solution to the regularized solution, posterior, and predictive distribution. It's also convex, making optimization straightforward. Classification is slightly more complex, but since the sigmoid function is monotonic, gradient descent still works quite efficiently.''')
    st.markdown(r'''''')
    st.markdown(r'''However, there's a significant problem with this assumption. It's a very strong assumption because we predefine the features $\phi(x)$. It's challenging to come up with a $\phi(x)$ that will fit the output in the form of $w^T \phi(x)$ for arbitrary problems.''')
    st.markdown(r'''''')
    st.markdown(r'''## Neural Networks as a Solution''')
    st.markdown(r'''''')
    st.markdown(r'''Neural networks aim to solve this limitation by considering the feature itself as a learnable function with parameters. This approach is more flexible, changing fixed features $\phi(x)$ to parameterized features $\phi(x, w)$. These features are learned through optimization, which is conceptually straightforward but has only recently become widely adopted due to the vast computational power available from GPUs.''')
    st.markdown(r'''''')
    st.markdown(r'''The simplest form of a neural network is the multi-layer perceptron (MLP). The multi-layer part means that if we want to make a very complex function, one obvious way is to string together many simple functions in a recursive way. The idea is that the recursive composition of simple functions can create a very complex function.''')
    st.markdown(r'''''')
    st.markdown(r'''For example, starting with a simple sigmoid function and allowing it to do shifting, stretching, and scaling, you can approximate a target function by summing up different sigmoid functions. Even starting with a simple function, using recursive composition of nonlinear functions can lead to a very complex function. That's the logic behind neural networks.''')
    st.markdown(r'''''')
    st.markdown(r'''Mathematically, for one layer, we allow individual input $x$ to shift and stretch, and then apply some nonlinear function $f$, which can be the sigmoid function. This itself is not very expressive. But if we make something more expressive by taking some $x$, doing some linear stretching and shifting ($w$ here also includes the bias term, so it has both stretching and shifting), then applying a sigmoid function and summing up all the sigmoid functions with arbitrary weights, we can create a new function. We can keep doing this to make it even more expressive.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's break this down mathematically. For one layer of our network, we allow individual inputs to be shifted and stretched, and then we apply some nonlinear function $f$, which could be our sigmoid function. Mathematically, it looks like this:''')
    st.markdown(r'''''')
    st.markdown(r'''$y(x, w) = f(\sum_{j=1}^M w_j * x_j)$''')
    st.markdown(r'''''')
    st.markdown(r'''This by itself isn't very expressive. But here's where it gets interesting. We can make it more expressive by taking our input $x$, doing some linear stretching and shifting (our $w$ here includes a bias term, so it covers both stretching and shifting), then applying our sigmoid function, and summing up all these sigmoid functions with arbitrary weights. And guess what? We can keep doing this to make it even more expressive.''')
    st.markdown(r'''''')
    st.markdown(r'''For a two-layer network, it would look something like this:''')
    st.markdown(r'''''')
    st.markdown(r'''$y_k(x, w) = f(\sum_{j=0}^M w^{(2)}_{kj} * f(\sum_{i=0}^D w^{(1)}_{ji} * x_i))$''')
    st.markdown(r'''''')
    st.markdown(r'''This network is what we classically call a feedforward network or a fully connected network. It means that given any input, we do some scaling and stretching, sum them up, further use that as a function, do some more scaling and stretching, and sum them up again. We can generalize this to a feed-forward network that isn't fully connected, where we string together different summations of different parts of the function.''')
    st.markdown(r'''''')
    st.markdown(r'''## Historical Context and Advanced Concepts''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's put this in historical context. Back in the 1970s and 80s, few people believed this could actually work. They thought the calculations would be too expensive, and they were worried about overfitting – similar to the problems you get when fitting a high-order polynomial. It seemed like you were just throwing a bunch of parameters at the problem and hoping for the best.''')
    st.markdown(r'''''')
    st.markdown(r'''But here's the fascinating part – there turned out to be a lot of "magic" happening that people started to understand over time. Despite the initial skepticism, researchers discovered many tricks, even in this simplest network architecture.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's talk about some of these advanced concepts. First, skip connections. In our basic model, we typically go from the first layer to the second layer to the third layer, and so on. But researchers have found that you can actually create connections that skip layers - going directly from, say, the first layer to the third layer. Why is this important? Well, it helps address a problem called vanishing gradients.''')
    st.markdown(r'''''')
    st.markdown(r'''Most of the optimization in neural networks is done by gradient descent. In a very deep network, we can run into an issue where the gradients become extremely small as we backpropagate through the layers. Imagine each layer's gradient is 0.1. By the time we get to the third layer, we're dealing with $0.1^3$, or 0.001. This exponential diminishing of gradients means that in practice, we might update some layers quickly, but never get around to updating others because their gradients are too small. Skip connections provide a nice way to circumvent this problem.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, there's one key requirement for these feedforward networks: they can't have any circular loops. The information always flows forward from the input to the output layers. This is crucial for performing gradient descent.''')
    st.markdown(r'''''')
    st.markdown(r'''Around 2017 and 2018, there was a lot of research into what we call Network Architecture Search or NAS. The idea was to optimize not just the parameters of the network, but its structure as well. While this approach has become less popular recently, partly due to the emergence of more powerful architectures like transformers, it's still an interesting area to keep in mind.''')
    st.markdown(r'''''')
    st.markdown(r'''## The Universal Approximation Property''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let me share something really fascinating about these networks. Despite their apparent simplicity, you can mathematically prove - though we won't do so here - that even a two-layer MLP can approximate any continuous function to arbitrary accuracy, given enough hidden units. This is known as the Universal Approximation Property.''')
    st.markdown(r'''''')
    st.markdown(r'''This might seem hard to believe at first, but think about it - if we keep summing up more and more sigmoid functions, at some point we should be able to approximate any function, right? The formal proof involves some advanced mathematical concepts, like how we define a continuous function, which we haven't covered in this course. But for those of you who've taken real analysis, you might be familiar with similar theorems, like the Stone-Weierstrass theorem, which states that any continuous function can be approximated arbitrarily accurately with trigonometric functions.''')
    st.markdown(r'''''')
    st.markdown(r'''In physics, you might have encountered a similar idea with Fourier series - any continuous function can be well-approximated by expanding its Fourier series. This universal approximation property is like saying that in principle, a neural network could approximate any continuous function, even something as complex as your consciousness - assuming, of course, that conscious thought is a continuous function!''')
    st.markdown(r'''''')
    st.markdown(r'''But here's the catch - while this property tells us that a solution exists, it doesn't tell us how to find it easily. The solution could be hiding somewhere in a very high-dimensional space, and finding it is the real challenge.''')
    st.markdown(r'''''')
    st.markdown(r'''If you're interested in gaining more intuition about how simple neurons can create complex functions, I highly recommend checking out some of the interactive playgrounds available online. TensorFlow and PyTorch both have excellent ones. These tools let you play around with networks and see firsthand how combining simple functions can lead to very complex behaviors.''')
    st.markdown(r'''''')
    st.markdown(r'''## The Origin of "Perceptrons"''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's talk about why they're called "perceptrons" in the first place. It's actually quite an interesting bit of history. The term "perceptron" originally referred to a specific type of activation function - essentially a step function. If the input is greater than zero, it outputs +1; if it's less than zero, it outputs -1. This idea was first promoted by Frank Rosenblatt back in 1958. Yes, you heard that right - 1958! Neural networks are far from a new concept.''')
    st.markdown(r'''''')
    st.markdown(r'''The perceptron function was inspired by the idea of neurons firing in the brain. If you've seen the movie "The Imitation Game," you might recall the concept of switches being either on or off based on some threshold. That's essentially what the perceptron function does.''')
    st.markdown(r'''''')
    st.markdown(r'''However, modern-day MLPs don't typically use this step function. Why? Well, remember how we talked about gradient descent for optimization? A step function has a sharp kink, which means the gradient isn't smooth. This makes optimization tricky. Instead, we use activation functions that are smooth and differentiable, like the sigmoid function we discussed earlier.''')
    st.markdown(r'''''')
    st.markdown(r'''Despite this change, the name "Multi-Layer Perceptron" has stuck around. You'll often hear people use MLP, feedforward network, and fully connected network interchangeably, though the latter two are technically more accurate for what we use today.''')
    st.markdown(r'''''')
    st.markdown(r'''## Training Neural Networks''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's talk about how we actually train these networks. The objectives are really quite similar to what we've seen before in our previous lectures. For regression tasks, we're still trying to minimize the chi-square, just like before. The main difference is that our function $y$ now has a more flexible form. Instead of just $w \cdot \phi(x)$, it's now a more complex, recursive function parameterized by $w$.''')
    st.markdown(r'''''')
    st.markdown(r'''For binary classification, we want our network to output a $y$ value between 0 and 1, representing a probability. To ensure this, we typically apply a sigmoid function to the final layer's output. The loss function in this case is the cross-entropy loss, which we've seen before in logistic regression.''')
    st.markdown(r'''''')
    st.markdown(r'''Multi-class classification is similar, but instead of a sigmoid, we use a softmax function to ensure our outputs sum to 1 across all classes. The loss function becomes a sum of cross-entropy terms across all classes.''')
    st.markdown(r'''''')
    st.markdown(r'''One thing to keep in mind is that neural networks have several hyperparameters. The number of hidden layers (how deep the network is) and the number of nodes in each layer are all things we can adjust. These choices can significantly impact the network's performance.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, here's something really interesting - and it relates to a common misconception about neural networks. Many people think of neural networks as just very flexible functions, like high-order polynomials. This view actually offends me a bit, because it's far from the truth!''')
    st.markdown(r'''''')
    st.markdown(r'''If neural networks were just flexible functions, they wouldn't work nearly as well as they do. Think about it - if you have five data points and fit a 100th-order polynomial, you'll massively overfit. But somehow, neural networks don't do this. You can have a million data points, fit a billion parameters, and it often doesn't overfit. This is a profound property that we're only now starting to understand well.''')
    st.markdown(r'''''')
    st.markdown(r'''Of course, we can still use regularization techniques if we want to prune the weights, just like in linear regression. We can add a term to our loss function that penalizes large weights. This is often controlled by a "weight decay" parameter in deep learning packages, which gradually prunes away unnecessary weights.''')
    st.markdown(r'''''')
    st.markdown(r'''This idea of preferring simpler solutions is related to the principle of Occam's Razor or the rule of parsimony - we want the simplest solution that explains our data well. It's an example of what we call an "inductive bias" in machine learning.''')
    st.markdown(r'''''')
    st.markdown(r'''I've prepared tutorials on neural networks that will give you hands-on experience with these concepts. It's really important that you play around with these tutorials - that's where the real learning happens. What we're doing here is giving you an overview and hopefully dispelling some common misconceptions.''')
    st.markdown(r'''''')
    st.markdown(r'''## Practical Aspects of Training Neural Networks''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's talk about some practical aspects of training neural networks. One nice thing about neural networks is that they tend not to overfit too much. However, this doesn't mean we can ignore the issue entirely.''')
    st.markdown(r'''''')
    st.markdown(r'''If we had infinite computing time, we'd always find a very nice solution after training long enough. But most of us don't have that luxury. Sometimes, you might still tend to overfit because you're using a lot of parameters to fit a relatively small dataset.''')
    st.markdown(r'''''')
    st.markdown(r'''A practical way to address this is through early stopping. This is similar to what we discussed in linear regression, like when fitting a polynomial. You monitor the training loss as a function of the number of training steps. But here's the key: you also keep an eye on the validation loss. The slides show a great visualization of this:''')
    st.markdown(r'''''')
    st.markdown(r'''[Training loss and Validation loss graph over time]''')
    st.markdown(r'''''')
    st.markdown(r'''As you can see, there's a point where the validation loss starts to increase while the training loss continues to decrease. This is where early stopping comes in. You choose the best weights from the point just before the validation loss starts to worsen.''')
    st.markdown(r'''''')
    st.markdown(r'''Technically, as we discussed in the first lecture, you want to separate your data into training, validation, and test sets. The test set should have nothing to do with any of your decisions. Here, the validation set is being used to decide where to truncate the training, even though the weights themselves never see the validation data. In practice, while it's ideal to have a separate test set, it's not always a big deal for many applications. You can often get by using just the training and validation (which you might call a test set in this context) to determine where to stop training.''')
    st.markdown(r'''''')
    st.markdown(r'''## Optimization in Neural Networks''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's talk about how we optimize these complex functions. We use stochastic gradient descent, just like we learned about in logistic regression. The concept itself isn't hard to grasp. The magical thing is that it actually works - I think that's where the real surprise lies.''')
    st.markdown(r'''''')
    st.markdown(r'''Our goal is to find the best parameters $w$ that minimize the loss function $E(w)$. Mathematically, we're trying to find:''')
    st.markdown(r'''''')
    st.markdown(r'''$w^* = \arg\min E(w)$''')
    st.markdown(r'''''')
    st.markdown(r'''The way we do this is through stochastic gradient descent. Just to remind you, what makes this stochastic? It means we take a sub-batch of the training data for each update. Why is this good? Even if the loss surface isn't very jagged, you still want to inject some randomness into your gradient descent so it doesn't get stuck in local minima.''')
    st.markdown(r'''''')
    st.markdown(r'''As mentioned earlier, recall from our lecture on logistic regression that even without an analytic closed form, we can optimize and find the best $w^*$ that minimizes the loss through stochastic gradient descent. The key is that as long as we can calculate the gradient $\nabla E(w)$, we can perform the optimization.''')
    st.markdown(r'''''')
    st.markdown(r'''For neural networks, we can derive an analytic formula for the gradient through the chain rule of derivatives. This is where things get really interesting, and it leads us to the concept of backpropagation, which we'll discuss in more detail soon.''')
    st.markdown(r'''''')
    st.markdown(r'''The beauty of this approach is that it allows us to optimize incredibly complex functions with millions of parameters. It's not just about having a flexible function - it's about having a flexible function that we can actually optimize effectively.''')
    st.markdown(r'''''')
    st.markdown(r'''## Calculating Gradients in Neural Networks''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's dive into a crucial aspect of neural networks: calculating gradients. We know that to perform gradient descent, we need to calculate the gradient. But here's the million-dollar question: if you have a very complex function, how do you actually calculate this gradient? This is what we'll focus on for the remainder of our time today.''')
    st.markdown(r'''''')
    st.markdown(r'''In principle, finding the gradient isn't hard. You might remember from high school that if you have a complex function, say F composite with G composite with H, and you want to take the derivative, you just apply the chain rule: $dF/dG * dG/dH * dH/dx$. It's always a chain rule. But implementing this efficiently in a computer is where things get interesting.''')
    st.markdown(r'''''')
    st.markdown(r'''### Methods for Calculating Gradients''')
    st.markdown(r'''''')
    st.markdown(r'''Let's look at a few ways we can calculate gradients in a computer:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Finite Differences:''')
    st.markdown(r'''   This is probably the first method that comes to mind. If we want to calculate $df/dx$, we can approximate it using $(\Delta f / \Delta x)$ with some finite $\Delta$. This is a numerical approximation based on the limit definition. However, this method has significant drawbacks:''')
    st.markdown(r'''   - You need to calculate it many times for different axes.''')
    st.markdown(r'''   - There's no arbitrary way to choose the right $\Delta x$. If it's too small, you run into numerical precision issues. If it's too large, it's not a good approximation of the gradient.''')
    st.markdown(r'''   - It can incur large numerical errors and is computationally expensive.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Symbolic Computation:''')
    st.markdown(r'''   This is what you might have learned in high school or seen in math competitions. It's what systems like Wolfram Alpha do. Given a complex function, it can find $\partial f/\partial x$ in symbolic form. This involves running rules on the symbolic representation. But this method also has limitations:''')
    st.markdown(r'''   - We don't really care about the exact symbolic representation of the derivative; we just want the gradient to update our parameters.''')
    st.markdown(r'''   - For complex functions, the symbolic expressions can be incredibly long and hard to express.''')
    st.markdown(r'''   - It's inefficient, equations can be verbose, and many derivatives are not easy to express symbolically.''')
    st.markdown(r'''''')
    st.markdown(r'''3. Automatic Differentiation:''')
    st.markdown(r'''   This is the method that's really important for most cases in machine learning, especially for neural networks. Here's the key idea: any function we work with can be broken down into a composition of primitive operations like addition, multiplication, exponentials, logarithms, sine, cosine, etc. For each of these primitive operations, we know the derivative analytically.''')
    st.markdown(r'''''')
    st.markdown(r'''   We can summarize this approach as follows:''')
    st.markdown(r'''   - We consider any mathematical statements as the composition of primitive operations.''')
    st.markdown(r'''   - We implement the derivative for each elementary operation, and then use the chain rule to compose the gradients.''')
    st.markdown(r'''''')
    st.markdown(r'''   This method has several advantages:''')
    st.markdown(r'''   - It's very accurate. By default, it's the most accurate thing we can do because we're calculating the analytic composite, the analytic gradient for individual points.''')
    st.markdown(r'''   - It can be quite efficient. This is where backpropagation comes in, which we'll discuss in more detail soon.''')
    st.markdown(r'''   - It's perfect for our needs in neural network optimization. It's accurate and efficient.''')
    st.markdown(r'''''')
    st.markdown(r'''The only downside is that it's numerical and doesn't give us a closed analytic form. But for neural network optimization using methods like stochastic gradient descent (SGD), this isn't critical at all.''')
    st.markdown(r'''''')
    st.markdown(r'''Here's why automatic differentiation is so powerful: when we do gradient descent, we want to take $\eta * \nabla E(w)$, where $\eta$ is our learning rate and $\nabla E(w)$ is our gradient. We don't need the symbolic form of this gradient; we just need its numerical value at each point. That's exactly what automatic differentiation gives us.''')
    st.markdown(r'''''')
    st.markdown(r'''This approach allows us to handle incredibly complex functions efficiently. We're not trying to derive some impossibly long symbolic expression. Instead, we're breaking down our complex function into a series of simple operations, each of which we know how to differentiate, and then we're applying the chain rule step by step.''')
    st.markdown(r'''''')
    st.markdown(r'''### Example: One-Layer Multi-Layer Perceptron (MLP)''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's make the concept of automatic differentiation clearer with a specific example. We'll look at a one-layer Multi-Layer Perceptron (MLP). ''')
    st.markdown(r'''''')
    st.markdown(r'''Imagine we have an output $y$ that we want to estimate using a one-layer MLP. Our model would look like this:''')
    st.markdown(r'''''')
    st.markdown(r'''$l = E(W, b) = (y - \sigma(Wx + b))^2$''')
    st.markdown(r'''''')
    st.markdown(r'''Here, $\sigma$ is our activation function (like sigmoid), $W$ is our weight matrix, $x$ is our input, and $b$ is our bias. This whole function, which represents our loss, can be decomposed into a string of operations. Let's break it down step by step:''')
    st.markdown(r'''''')
    st.markdown(r'''1. We start with some input $x$ and weight matrix $W$.''')
    st.markdown(r'''2. We perform matrix multiplication $Wx$.''')
    st.markdown(r'''3. We add the bias $b$ to get $z = Wx + b$.''')
    st.markdown(r'''4. We apply the activation function to get $h = \sigma(z)$.''')
    st.markdown(r'''5. We subtract this from $y$ and square the result to get our loss $l$.''')
    st.markdown(r'''''')
    st.markdown(r'''This can be visualized as a computation graph.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, if we want to update our weights $W$, we need to calculate $\partial l/\partial W$ (the gradient of the loss with respect to $W$). Using the chain rule, this can be written as:''')
    st.markdown(r'''''')
    st.markdown(r'''$\dfrac{\partial l}{\partial W} = \dfrac{\partial l}{\partial h} \cdot \dfrac{\partial h}{\partial z} \cdot \dfrac{\partial z}{\partial y} \cdot \dfrac{\partial y}{\partial W}$''')
    st.markdown(r'''''')
    st.markdown(r'''To calculate this, we need two key ingredients:''')
    st.markdown(r'''''')
    st.markdown(r'''1. We need to know how to do the differentiation for individual steps. This is trivial because we've decomposed our function into primitive operations, each with a known derivative.''')
    st.markdown(r'''''')
    st.markdown(r'''2. We need to calculate the intermediate values. This is crucial because when we calculate each part of the chain rule, we're evaluating it at a specific point. For example, when we calculate $\partial l/\partial h$, we need to evaluate it at the current value of $h$. The same goes for $z$ and $y$.''')
    st.markdown(r'''''')
    st.markdown(r'''### Implementing Automatic Differentiation''')
    st.markdown(r'''''')
    st.markdown(r'''Knowing these two ingredients, we can implement automatic differentiation in two steps:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Forward Pass: We calculate all the intermediate values. Given $W$, $x$, and $b$, we compute $y$, $z$, and $h$. This is essentially dynamic programming - we evaluate all the intermediate variables and store them.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Backward Pass (Backpropagation): We iteratively calculate all the gradients by evaluating the gradient part-by-part, moving backwards through our computation graph. We start by calculating $\partial l/\partial h$, then use this to calculate $\partial l/\partial z$, and so on, moving back towards $\partial l/\partial W$.''')
    st.markdown(r'''''')
    st.markdown(r'''You might wonder, "Why do we have to do this backwards? Couldn't we just do everything in the forward direction?" The beauty of backpropagation lies in its computational efficiency. Let's delve deeper into why backpropagation is so computationally efficient and why it's the preferred method for optimizing neural networks.''')
    st.markdown(r'''''')
    st.markdown(r'''### Efficiency of Backpropagation''')
    st.markdown(r'''''')
    st.markdown(r'''Firstly, backpropagation is efficient because it reuses calculations. For example, when we calculate $\partial L/\partial b$ (the gradient with respect to the bias), we can reuse many of the parts we calculated for $\partial L/\partial W$ (the gradient with respect to the weights). This reuse of computations is a key aspect of its efficiency.''')
    st.markdown(r'''''')
    st.markdown(r'''In practice, we often use libraries like PyTorch rather than implementing backpropagation ourselves. However, implementing a neural network using just NumPy can be very illuminating and help you understand these concepts at a deeper level.''')
    st.markdown(r'''''')
    st.markdown(r'''Secondly, let's talk about why backpropagation is much more efficient than forward propagation for optimization. The key lies in the structure of our computations. Your loss function, by definition, is a scalar. This means that when you look at $\partial L/\partial h$, the first dimension is always one because you're outputting one scalar. If $h$ represents some number of hidden neurons, let's say $K$, then this matrix is always $1 \times K$.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's look at the dimensions more closely:''')
    st.markdown(r'''''')
    st.markdown(r'''$\underbrace{\dfrac{\partial L}{\partial W}}_{(1,MD)} = \underbrace{\dfrac{\partial L}{\partial h}}_{(1,K)} \cdot \underbrace{\dfrac{\partial h}{\partial z}}_{(K,K)} \cdot \underbrace{\dfrac{\partial z}{\partial y}}_{(K,M)} \cdot \underbrace{\dfrac{\partial y}{\partial W}}_{(M,MD)}$''')
    st.markdown(r'''''')
    st.markdown(r'''When we do this multiplication in the backward direction:''')
    st.markdown(r'''1. We start with a $1 \times K$ matrix $(\partial L/\partial h)$''')
    st.markdown(r'''2. Multiply it by a $K \times K$ matrix $(\partial h/\partial z)$, resulting in a $1 \times K$ matrix''')
    st.markdown(r'''3. Multiply that by a $K \times M$ matrix $(\partial z/\partial y)$, resulting in a $1 \times M$ matrix''')
    st.markdown(r'''4. Finally multiply by an $M \times MD$ matrix $(\partial y/\partial W)$, resulting in our $1 \times MD$ gradient''')
    st.markdown(r'''''')
    st.markdown(r'''Notice that at each step, we're always multiplying a $1 \times N$ matrix by an $N \times M$ matrix, which is a computationally cheap operation. We're always dealing with row vectors, which makes the matrix multiplications very efficient.''')
    st.markdown(r'''''')
    st.markdown(r'''On the other hand, if we were to do this in the forward direction, we'd be dealing with much larger matrices. This efficiency is crucial because it allows us to take many optimization steps in a finite amount of computational time. It's why we call it backpropagation - it's not just a mathematical trick, but a dynamic programming technique that makes gradient calculation very effective.''')
    st.markdown(r'''''')
    st.markdown(r'''### Auto-differentiation vs. Auto-integration''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's step back. There's an interesting question: "Why do we have auto-differentiation but not auto-integration?" The answer lies in the simplicity of differentiation compared to integration. Differentiation is essentially just applying the chain rule repeatedly. We do the chain rule, then do it again, and again. It's a simple, repetitive process.''')
    st.markdown(r'''''')
    st.markdown(r'''Integration, on the other hand, doesn't have such simple rules. You might need to use the Cauchy formula, deal with partial differentials, or even call your mathematician friends for help. There's no straightforward, algorithmic approach to integration like there is for differentiation.''')
    st.markdown(r'''''')
    st.markdown(r'''This simplicity of differentiation is key to why neural networks have become so popular. All these chain rules lead to a bunch of matrix multiplications. And guess what's incredibly good at matrix multiplications? GPUs (Graphics Processing Units).''')
    st.markdown(r'''''')
    st.markdown(r'''In fact, you could say that neural networks won what we call the "hardware lottery." There's no inherent reason why humans needed to develop GPUs, except that young people love to play video games. For games, when you output RGB colors on your screen, you need to do a lot of matrix calculations in parallel. So GPUs were developed for this purpose. And it turns out that this is exactly what we need for backpropagation and neural networks.''')
    st.markdown(r'''''')
    st.markdown(r'''This leads us to what's known as the "cheap gradient principle." Backpropagation to get the gradient is not much more expensive than the forward pass. Why? Because it really comes down to the fact that your loss function is a scalar, and therefore you always have these $1 \times K$ matrices. So when you backpropagate, the complexity is about the same as evaluating the function. It's only a few factors of the gradients, and combined with GPU technology, this becomes very efficient because you can optimize very quickly.''')
    st.markdown(r'''''')
    st.markdown(r'''## Advanced Concepts in Neural Networks''')
    st.markdown(r'''''')
    st.markdown(r'''Let us now delve into some advanced concepts regarding neural networks. While not essential for basic implementation, these insights are crucial for a deeper understanding of the efficacy of neural networks.''')
    st.markdown(r'''''')
    st.markdown(r'''Firstly, it's important to note that the ability to perform gradient descent does not guarantee finding an optimal solution. In the 1990s, the majority of researchers, with notable exceptions such as Yann LeCun, were skeptical about consistently finding solutions. This skepticism stemmed from three primary concerns:''')
    st.markdown(r'''''')
    st.markdown(r'''1. The misconception that neural networks were analogous to high-order polynomial fitting, leading to concerns about overfitting.''')
    st.markdown(r'''2. The assumption that the loss landscape, given the high number of parameters, would be too complex for efficient optimization.''')
    st.markdown(r'''3. The belief that finding a good solution amidst numerous local minima would be improbable.''')
    st.markdown(r'''''')
    st.markdown(r'''Recent research, particularly on the neural tangent kernel, has demonstrated that the loss landscape with respect to weights is surprisingly smooth. This smoothness allows for effective optimization from various initial points.''')
    st.markdown(r'''''')
    st.markdown(r'''These findings have sparked significant interest in the mathematical community. It's a source of frustration when neural networks are oversimplified as mere black boxes or flexible lookup tables. Such characterizations fail to capture the complex mathematical phenomena at play. The excitement in the fields of mathematics and statistics stems not just from the practical success of machine learning, but from the discovery of previously unknown phenomena that require rigorous mathematical understanding.''')
    st.markdown(r'''''')
    st.markdown(r'''An apt analogy can be drawn to statistical mechanics. While it might seem impossible to comprehend the behavior of all particles in a system using classical mechanics due to weak individual interactions, collective motion leads to emergent properties. Similarly, neural networks exhibit global properties of solutions, akin to how particles in a system naturally conform to a Boltzmann distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''### Recent Developments in Neural Network Theory''')
    st.markdown(r'''''')
    st.markdown(r'''Recent developments have further expanded our understanding:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Deep Double Descent: This phenomenon, which gained prominence around 2016-2017, challenges classical intuitions about model complexity and generalization. In traditional machine learning, we expect performance to improve with model complexity up to a point, after which overfitting occurs. However, neural networks exhibit a fascinating property: after an initial overfitting phase, performance often improves again with increased complexity. This allows for the effective use of models with billions of parameters to fit millions of data points without overfitting.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Neural Tangent Kernel: Popularized in 2018-2019, this concept provides a theoretical framework for understanding the smoothness of the loss landscape in neural networks.''')
    st.markdown(r'''''')
    st.markdown(r'''3. Grokking: A concept that gained traction in 2022, grokking addresses the question of whether neural networks merely memorize or truly learn when the number of parameters exceeds the number of data points. Research on simple arithmetic tasks, such as modular addition, has shown that neural networks often learn underlying patterns rather than memorizing individual data points. This demonstrates a form of generalization that transcends mere memorization.''')
    st.markdown(r'''''')
    st.markdown(r'''The phenomenon of grokking is particularly intriguing. When training a network on a task like $A + B \mod C$, a true understanding of the concept manifests as periodic patterns in neuron activations, reflecting the inherent periodicity of modular arithmetic. This ability to extract fundamental patterns, even when provided with fewer data points than parameters, challenges our understanding of learning and generalization in artificial systems.''')
    st.markdown(r'''''')
    st.markdown(r'''These developments underscore the excitement surrounding deep learning. While popular attention often focuses on applications like GPT-3 or ChatGPT, the true intellectual excitement lies in uncovering the statistical behaviors of neurons that were previously unknown. These discoveries open new avenues for understanding learning, generalization, and potentially, the nature of intelligence itself.''')
    st.markdown(r'''''')
    st.markdown(r'''The importance of understanding this theory cannot be overstated. It's tempting to view machine learning as a purely empirical process of data manipulation and iterative improvement. However, this perspective misses the profound insights that theoretical understanding can provide. A solid grasp of these concepts has tangible consequences for network design, optimization strategies, and problem-solving in practical applications.''')
    st.markdown(r'''''')
    st.markdown(r'''It's crucial to avoid the misconception that neural network development is simply a matter of arbitrarily stacking layers and allowing the model to magically adapt to the data. A theoretical understanding informs decision-making in architecture design, guides the choice of optimization techniques, and aids in diagnosing and resolving issues that arise during training and deployment.''')
    st.markdown(r'''''')
    st.markdown(r'''## Activation Functions''')
    st.markdown(r'''''')
    st.markdown(r'''Let us conclude by discussing the importance of activation function choice in neural networks and summarizing the key concepts we've covered.''')
    st.markdown(r'''''')
    st.markdown(r'''The selection of an appropriate activation function is crucial for effective network training. The sigmoid function, despite its historical significance, is no longer favored as an activation function. While sigmoid functions are simple and were once popular, they possess inherent limitations.''')
    st.markdown(r'''''')
    st.markdown(r'''Consider the derivative of the sigmoid function: $\sigma(x)(1 - \sigma(x))$. This results in a product of an S-shaped curve and its inverse. The resulting shape is problematic for gradient descent optimization. It creates regions where the gradient is either extremely small or lacks clear directionality. This characteristic can significantly impede training progress, as the network struggles to make decisive parameter updates.''')
    st.markdown(r'''''')
    st.markdown(r'''This indecisiveness in the gradient is analogous to excessive politeness in communication, where the message becomes too subtle to be effective. In optimization, we often require more decisive signals. The sigmoid's tendency to produce gradients that are neither clearly positive nor negative, but rather ambiguously in between, leads to prolonged training times and potential convergence issues.''')
    st.markdown(r'''''')
    st.markdown(r'''In contrast, activation functions like ReLU (Rectified Linear Unit) provide more decisive gradient information, facilitating more efficient training. This illustrates why the choice of activation function matters significantly in neural network design and optimization.''')
    st.markdown(r'''''')
    st.markdown(r'''## Summary''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's summarize the key concepts we've covered in this lecture:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Neural Networks as Adaptive Basis Functions: We've explored how neural networks overcome the limitations of linear regression by acting as adaptive basis functions. This flexibility allows them to model complex, non-linear relationships in data.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Regularization and Weight Decay: We discussed how concepts from linear regression, such as regularization, are applied to neural networks in the form of weight decay. This helps in preventing overfitting and improving generalization.''')
    st.markdown(r'''''')
    st.markdown(r'''3. Network Training - Parameter Optimization and Gradient Descent: We examined how neural networks are trained using gradient descent techniques, similar to other machine learning models. However, we also touched upon the unique aspects that make this process effective in neural networks, despite their complexity.''')
    st.markdown(r'''''')
    st.markdown(r'''4. Autodifferentiation and Backpropagation: We delved into the crucial concepts of autodifferentiation and backpropagation. These techniques enable efficient computation of gradients in complex neural architectures, forming the backbone of modern deep learning frameworks.''')
    st.markdown(r'''''')
    st.markdown(r'''Before we conclude, I'd like to address a question that was raised:''')
    st.markdown(r'''''')
    st.markdown(r'''The question pertains to the compatibility of predefined, non-trivial activation functions with autodifferentiation systems. It's a common misconception that custom or complex functions might not be amenable to autodifferentiation. However, it's important to understand that any function we can express in code is, by definition, autodifferentiable. This is because any computable function can be decomposed into a series of primitive operations.''')
    st.markdown(r'''''')
    st.markdown(r'''In the context of computational graphs, your custom function, let's call it $g(x)$, is represented as a composition of more fundamental operations. The autodifferentiation system can navigate this composition, applying the chain rule at each step. This principle holds true regardless of the function's apparent complexity at the high level.''')
    st.markdown(r'''''')
    st.markdown(r'''It's somewhat tautological: if we can define an operation in code, it must be composed of primitive operations that the system can differentiate. This is analogous to how complex biological structures are ultimately composed of fundamental cellular units.''')
    st.markdown(r'''''')
    st.markdown(r'''This capability is a key feature of modern deep learning frameworks like PyTorch, TensorFlow, and JAX, which are built on robust autodifferentiation engines. These systems allow for great flexibility in network design while maintaining the ability to compute gradients efficiently.''')
    st.markdown(r'''''')
    st.markdown(r'''''')

#if __name__ == '__main__':
show_page()