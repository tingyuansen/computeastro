import streamlit as st
from streamlit_app import navigation_menu

def show_page():

    # Page Title
    st.title('Sampling - Markov Chain Monte Carlo')
    navigation_menu()

    # Embed the external HTML page
    # st.info('''
    # Course Slides
    # ''')
    # slideshare_embed_code = """
    #     <iframe src="https://www.slideshare.net/slideshow/embed_code/key/vEktMhy19dmo9q" 
    #             width="700" height="394" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" 
    #             style="border:0px solid #000; border-width:0px; margin-bottom:0px; max-width: 100%;" allowfullscreen> 
    #     </iframe> 
    # """
    # components.html(slideshare_embed_code, height=412)

    # st.markdown('---')
    
    st.markdown(r'''''')
    st.markdown(r'''Welcome back, everyone. Let's move on to today's lecture topic: Markov chains. This concept is widely applied in astronomy, particularly in the form of Markov Chain Monte Carlo (MCMC) methods. Often, we use these techniques without fully understanding the underlying principles, treating them as black boxes. However, many popular astronomy packages implementing these methods are simpler than you might think - often less than 50 lines of code.''')
    st.markdown(r'''''')
    st.markdown(r'''The goal of today's lecture is to demystify MCMC. We'll discuss its theoretical foundations, though not as deeply as we might in a computer science course. We'll cover why MCMC works and the logic behind it. We'll also explore practical aspects, including three key algorithms: the Metropolis algorithm, the Metropolis-Hastings algorithm, and Gibbs sampling. Interestingly, the Metropolis algorithm alone can solve about 95% of astronomical problems requiring MCMC.''')
    st.markdown(r'''''')
    st.markdown(r'''## Recap and Context''')
    st.markdown(r'''''')
    st.markdown(r'''Let's start by recapping what we learned previously and how sampling, including MCMC, fits into the broader landscape of this course. As we discussed, much of machine learning boils down to evaluating an integral: $\mathbb{E}(f) = \int f(z) p(z) dz$. Here, $p$ is the posterior distribution of our models, and the expectation represents the predictive distribution. This formulation is crucial in scientific inquiry. We start with physical models, quantify their uncertainties (given finite data), and then use these uncertain models to make predictions. This process allows us to test and potentially negate our theories - a fundamental aspect of scientific progress.''')
    st.markdown(r'''''')
    st.markdown(r'''For example, in cosmology, $p$ might represent the parameters of the Lambda-CDM model, and the expectation could be the predicted number of dwarf galaxies in the universe. By integrating over the uncertainty in our models, we can forecast what we expect to observe and compare it to actual observations. This approach has led to important discoveries in astronomy, such as the "too big to fail" and "missing satellite" problems.''')
    st.markdown(r'''''')
    st.markdown(r'''However, evaluating this integral is often not straightforward. In many cases, especially when dealing with non-Gaussian distributions or complex likelihoods (as in Gaussian Process Classification), the integral becomes intractable. This is where sampling methods, particularly MCMC, become invaluable.''')
    st.markdown(r'''''')
    st.markdown(r'''## The Power of MCMC''')
    st.markdown(r'''''')
    st.markdown(r'''MCMC allows us to sample from the posterior distribution of our models. This is crucial not just for creating visually appealing corner plots for publications, but for making meaningful predictions and testing our theories. By sampling from the posterior, we can evaluate expected outcomes and compare them to observations, even when we can't solve the integral analytically.''')
    st.markdown(r'''''')
    st.markdown(r'''Before we dive into MCMC, let's briefly recap the sampling methods we've discussed previously: inverse CDF (Cumulative Distribution Function) and rejection sampling. These techniques are straightforward but come with significant limitations. The inverse CDF method requires us to be able to integrate the distribution, which is often not feasible for complex posteriors. For instance, if I give you a posterior distribution and ask you to integrate over all the parameters, it's unlikely to be a tractable problem.''')
    st.markdown(r'''''')
    st.markdown(r'''Rejection sampling, on the other hand, requires us to find a proposal distribution that envelopes the target distribution entirely. This can be challenging, especially in high-dimensional spaces or with complex, multimodal distributions that are common in astronomical problems. While these methods are valuable in certain scenarios, they often fall short when dealing with the complex, high-dimensional distributions we encounter in modern astrophysics.''')
    st.markdown(r'''''')
    st.markdown(r'''This is where MCMC comes into play. MCMC allows us to sample from the posterior distribution of our models, even when we can't directly sample from it or easily integrate it. This is crucial not just for creating visually appealing corner plots for publications, but for making meaningful predictions and testing our theories. By sampling from the posterior, we can evaluate expected outcomes and compare them to observations, even when we can't solve the integral analytically.''')
    st.markdown(r'''''')
    st.markdown(r'''## The Core Idea of MCMC''')
    st.markdown(r'''''')
    st.markdown(r'''The key insight of MCMC is that we can design an algorithm that samples from a distribution even when we can't directly sample from it, as long as we can evaluate the distribution at any given point. This is typically possible in Bayesian inference, where we can calculate the posterior probability for any set of parameters. So, while we might not be able to sample directly or integrate the posterior, we can usually evaluate it for any given set of parameter values.''')
    st.markdown(r'''''')
    st.markdown(r'''MCMC exploits this ability to evaluate the posterior by constructing a Markov chain whose stationary distribution is the posterior we're interested in. It's like having a bunch of walkers that randomly explore the parameter space, but their movements are guided by the landscape of the posterior distribution. Over time, the positions of these walkers will be distributed according to the posterior, giving us our desired samples.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's delve into the core idea of MCMC. The goal is to sample from a distribution $p(z)$ when we can't directly sample from it, but we can evaluate it at any point. This is precisely what we need for Bayesian inference, where we can calculate the posterior probability for any set of parameters. The MCMC approach is like having a smart traveler exploring a city. Let's use Canberra as our example. Imagine you want to sample the population distribution of Canberra. You start at a random location, $Z'$. Each day, you decide where to go next based on a travel guide (our proposal distribution) and some clever rules (our acceptance criteria).''')
    st.markdown(r'''''')
    st.markdown(r'''## The Metropolis Algorithm''')
    st.markdown(r'''''')
    st.markdown(r'''The Metropolis algorithm, a specific MCMC method, works as follows: You start at a random location $Z'$, then propose a new location $Z$ based on a symmetric proposal distribution $q(z|z') = q(z'|z)$. This is like your travel guide suggesting the next place to visit. The key to the algorithm is the calculation of an acceptance probability: $A(z', z) = \min(1, \dfrac{p(z)}{p(z')})$. This probability determines whether you move to the new location or stay put.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's break this down with our Canberra example. Say you're in Weston Creek and your guide suggests going to Weston. The acceptance probability depends on the relative "excitement" (population density) of these locations. If Weston is more populous, you'll always go there when it's proposed. However, if you're in Weston and the guide suggests Weston Creek, you might go, but with a probability proportional to the population ratio. For instance, if Weston Creek has half the population of Weston, you have a 50% chance of moving there from Weston. This ingenious mechanism injects information about the distribution into your random walk.''')
    st.markdown(r'''''')
    st.markdown(r'''Crucially, this method only requires us to evaluate the ratio of probabilities, not the absolute probabilities. In Bayesian terms, we're working with $\dfrac{p(z)}{p(z')} = \frac{\mathrm{likelihood}(z) * \mathrm{prior}(z)}{\mathrm{likelihood}(z') * \mathrm{prior}(z')}$. The normalization constants cancel out, which is why MCMC is so powerful for Bayesian inference where we often can't calculate the evidence term.''')
    st.markdown(r'''''')
    st.markdown(r'''Implementing this in a computer is straightforward. If the acceptance probability is 50%, you draw a random number between 0 and 1. If it's less than 0.5, you move; if not, you stay put. Importantly, you record your location every day, even if you didn't move. This is different from rejection sampling where rejected samples are discarded. The beauty of this method is that, over time, your recorded locations will follow the true population distribution of Canberra. It's easier to move from less populated areas to more populated ones, and harder to do the reverse, naturally sampling the underlying distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''In practice, the Metropolis algorithm proceeds as follows: You generate a new sample $z$ from $q(z|z')$, where $z'$ is your current position. Then you calculate $u$, a random number from a uniform distribution between 0 and 1. You accept the new sample if $\dfrac{p(z)}{p(z')} > u$. If accepted, you move to the new position $z$; if rejected, you stay at your current position $z'$. This process, repeated many times, will eventually sample from the target distribution $p(z)$, regardless of the starting point or the specific form of the symmetric proposal distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''## Theoretical Foundations: Detailed Balance and Ergodicity''')
    st.markdown(r'''''')
    st.markdown(r'''While our intuitive explanation of the Metropolis algorithm helps us understand the process, we can actually prove that this method converges to the desired distribution. The proof involves concepts from Markov chain theory, which we'll now explore in more detail. What's remarkable about the Metropolis algorithm is its simplicity and effectiveness. It forms the basis for many more advanced MCMC techniques and has revolutionized our ability to perform Bayesian inference in complex, high-dimensional problems.''')
    st.markdown(r'''''')
    st.markdown(r'''To begin our more formal treatment, we need to introduce the concept of transition probability. This is defined as $T(z', z) \equiv p(z_t | z'_{t-1})$, which represents the probability of moving from state $z'$ to state $z$ in adjacent steps. In the case of the Metropolis algorithm, this transition probability is the product of two factors: the proposal probability and the acceptance probability. Mathematically, we can express this as:''')
    st.markdown(r'''''')
    st.markdown(r'''$T(z', z) = q(z | z') A(z', z)$''')
    st.markdown(r'''''')
    st.markdown(r'''where $q(z | z')$ is the proposal distribution and $A(z', z)$ is the acceptance probability we discussed earlier.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's consider what happens when we apply this transition probability repeatedly. We're interested in the long-term behavior of the system. In our Canberra example, you can think of $T$ as your travel guide. The guide tells you how to become a Canberran, and you follow it step by step. After practicing your yoga or whatever else the guide suggests, you hope to reach some equilibrium state. This equilibrium state is what we call the stationary distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''Formally, a distribution $p(z)$ is called an invariant or stationary distribution if it doesn't change after applying the transition probability. Mathematically, this is expressed as:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(z) = \sum_{z'} T(z', z) p(z')$''')
    st.markdown(r'''''')
    st.markdown(r'''This equation is saying that if we start with the distribution $p(z')$ and apply our transition probability $T(z', z)$, we end up with the same distribution $p(z)$. In our Canberra analogy, if all Canberrans have the same travel guide, and we let the entire population move one step according to the guide, the overall distribution of people in the city shouldn't change if we're at equilibrium.''')
    st.markdown(r'''''')
    st.markdown(r'''For almost all cases in your research, we can assume that we're dealing with a homogeneous Markov chain. This means that your travel guide (transition probability) never changes – it's the same at each step, regardless of which step you're on or how long you've been following it. While there are cases where you might want to change the transition probability (like in reinforcement learning), for the MCMC methods we're discussing today, you can assume it's fixed.''')
    st.markdown(r'''''')
    st.markdown(r'''### Stationary Distribution''')
    st.markdown(r'''''')
    st.markdown(r'''Given these definitions, one crucial question arises: if we have a target distribution $p(z)$ that we want to sample from, and we have a transition probability $T(z', z)$, how do we ensure that our target distribution is indeed a stationary distribution of our Markov chain? In other words, how do we make sure that our travel guide will actually lead us to become true Canberrans?''')
    st.markdown(r'''''')
    st.markdown(r'''This is where the concept of detailed balance comes in. The concept of detailed balance provides a sufficient (though not necessary) condition for ensuring that our Markov chain has the desired stationary distribution. Mathematically, detailed balance is expressed as:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(z') T(z', z) = p(z) T(z, z')$''')
    st.markdown(r'''''')
    st.markdown(r'''This equation states that the probability of being in state $z'$ and transitioning to state $z$ is equal to the probability of being in state $z$ and transitioning to state $z'$. If this condition holds for all pairs of states $z$ and $z'$, then $p(z)$ is guaranteed to be a stationary distribution of the Markov chain.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's prove that detailed balance indeed ensures that $p(z)$ is a stationary distribution. We start with the detailed balance equation and sum over all possible $z'$:''')
    st.markdown(r'''''')
    st.markdown(r'''$\sum_{z'} p(z') T(z', z) = \sum_{z'} p(z) T(z, z')$''')
    st.markdown(r'''''')
    st.markdown(r'''The right-hand side simplifies to $p(z)$, because $\sum_{z'} T(z, z') = 1$ (the sum of probabilities of transitioning from $z$ to all possible states, including staying at $z$, must be 1). Therefore, we have:''')
    st.markdown(r'''''')
    st.markdown(r'''$\sum_{z'} p(z') T(z', z) = p(z)$''')
    st.markdown(r'''''')
    st.markdown(r'''This is precisely the definition of a stationary distribution that we introduced earlier. Thus, if detailed balance holds, $p(z)$ is indeed a stationary distribution of the Markov chain.''')
    st.markdown(r'''''')
    st.markdown(r'''Now that we've introduced the Metropolis algorithm, let's discuss a crucial condition that ensures its effectiveness: the detailed balance condition. This condition is what guarantees that our "travel guide" (the Metropolis algorithm) indeed has the potential to sample the distribution of Canberra (or any target distribution we're interested in). The detailed balance condition is expressed mathematically as $p(z') T(z', z) = p(z) T(z, z')$. This equation states that you can swap the order of $z'$ and $z$, and the equality still holds. It's relatively straightforward to prove that if this condition is satisfied, then the target distribution $p$ is indeed one of the stationary distributions of $T$.''')
    st.markdown(r'''''')
    st.markdown(r'''To prove this, we start with the definition of a stationary distribution: $p(z) = \sum_{z'} T(z', z) p(z')$. By applying the detailed balance condition, we can swap $z$ and $z'$, giving us $p(z) = \sum_{z'} T(z, z') p(z)$. Notice that $p(z)$ is now independent of $z'$ and can be pulled out of the summation: $p(z) = p(z) \sum_{z'} T(z, z')$. The sum $\sum_{z'} T(z, z')$ equals 1, as it represents the sum of probabilities of transitioning from $z$ to all possible states (including staying at $z$). Thus, we end up with $p(z) = p(z)$, proving that if detailed balance holds, then the target distribution $p$ is indeed a stationary distribution of $T$.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's verify that the Metropolis algorithm satisfies the detailed balance condition. We can do this by expanding the transition probability $T(z', z)$ and using the properties of the algorithm. Starting with $T(z', z) p(z') = q(z | z') A(z', z) p(z')$, we can substitute the acceptance probability $A(z', z) = \min(1, \dfrac{p(z)}{p(z')})$. After some algebraic manipulation and using the symmetry of the proposal distribution $q(z | z') = q(z' | z)$, we arrive at $T(z', z) p(z') = T(z, z') p(z)$. This derivation shows that the Metropolis algorithm indeed satisfies the detailed balance condition with respect to the target distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''To put this in terms of our Canberra analogy: if you follow the travel guide called the Metropolis algorithm, becoming a true Canberran (sampling from the target distribution) is within reach. You will have the opportunity to walk and breathe like a Canberran. However, this proof alone is not sufficient for our purposes in astronomy and other scientific fields. When you run an MCMC in your research, you want to be confident not just that you might get the correct posterior, but that you will get the correct posterior.''')
    st.markdown(r'''''')
    st.markdown(r'''So far, we've shown that if you satisfy the detailed balance condition (which is guaranteed by the Metropolis algorithm), you could get the desired distribution. But this doesn't necessarily mean that the chain will converge, or if it converges, that it will converge to the correct distribution. To ensure that when we run an MCMC, we indeed get the correct posterior, we need to consider two additional concepts: convergence and uniqueness.''')
    st.markdown(r'''''')
    st.markdown(r'''## Convergence and Uniqueness in MCMC''')
    st.markdown(r'''''')
    st.markdown(r'''Convergence refers to whether the Markov chain will eventually settle into a stationary distribution, regardless of where it starts. In our Canberra analogy, this would be like asking whether, no matter where in Australia you begin your journey, you'll eventually end up walking around Canberra like a local. Uniqueness asks whether there's only one stationary distribution that the chain can converge to, ensuring that we're sampling from the distribution we actually want. In our analogy, this is equivalent to making sure that our travel guide doesn't accidentally turn us into Sydneysiders instead of Canberrans.''')
    st.markdown(r'''''')
    st.markdown(r'''In the following sections, we'll explore the conditions under which we can guarantee both convergence and uniqueness. This will give us confidence that our MCMC methods are truly sampling from the correct posterior distribution in our astronomical and scientific applications. Understanding these concepts is crucial for anyone using MCMC methods in their research, as it allows us to trust the results of our simulations and make valid inferences from our data.''')
    st.markdown(r'''''')
    st.markdown(r'''The convergence just means that if I draw a bunch of walkers from some random distribution, so let's say I initialize all my walkers in a uniform distribution, meaning that I randomly sample from some people in Canberra. No matter where you come from, I hope that at some point, all these walkers will converge into the posterior distribution. So then we can use their footsteps as the sample that we want to do.''')
    st.markdown(r'''''')
    st.markdown(r'''So this is the definition of the convergence, meaning that for any starting distribution, it does not matter. But when you run long enough, then all this population of people that you have signed up in, I don't know, like some open days of a city tour, if you give them the travel guide, it will slowly become a local.''')
    st.markdown(r'''''')
    st.markdown(r'''But also, you want to make sure that the outcome is unique. When this converges into some distribution, it converges into one distribution. And the uniqueness is good enough here because we already showed that if you fulfill the detailed balance, becoming a local is one of the outcomes. If the outcome is unique, then you are done. Then you know that you would converge to the distribution. ''')
    st.markdown(r'''''')
    st.markdown(r'''This is actually not a guarantee in MCMC. This is something that people somewhat overlook in astronomy. We just assume that it works. But this is not always a guarantee. And to show that this does not always hold true, you can cook up some very pathological examples. ''')
    st.markdown(r'''''')
    st.markdown(r'''It turns out that the simplest one, if your travel guide just says, be yourself, stay where you are at. Then you, by definition, all starting distributions are stationary distributions because no people move. They stay at where they are. By definition, the distribution is stable, but this means that your stationary distribution is not unique. You could have a travel guide that gives you, you will reach that equilibrium, but it's not the equilibrium that you want. ''')
    st.markdown(r'''''')
    st.markdown(r'''And this is a very interesting, simple, trivial pathological case where your MCMC chain converges, but not into a distribution that you want. You can also have the cases where the distributions simply do not converge. And it's also very easy to code up an example as such.''')
    st.markdown(r'''''')
    st.markdown(r'''For example, I only have two places in Canberra, Western and Western Creek. And your travel guide says, if you wake up at Western, you go to Western Creek. If you wake up at Western Creek, you go to Western. So you just swap up the order. In this case, you can also see that most of the distributions would not ever converge. Why? ''')
    st.markdown(r'''''')
    st.markdown(r'''So let's say I have, first of all, it has a stationary distribution, which is if it just happened that Western and Western Creek have 50-50 population, then it reached an equilibrium. Because today I have 50-50, the next day I swap the whole population, but it's still 50-50. And then the next day they swap back, it's still 50-50. ''')
    st.markdown(r'''''')
    st.markdown(r'''But if I start with a distribution that is not 50-50, then this chain will never converge. Because let's say I start with Western Creek having 30% of the population and Western having 70% of the population. The next day, now Western Creek has 70% and Western has 30%, and then go back and forth. So if you have a travel guide that just asks you to swap the order in a periodic way, then you can also see that your chain will never converge. ''')
    st.markdown(r'''''')
    st.markdown(r'''So this is to say that not all Markov chains will converge. So I think that's a misconception when you run your MCMC. You always just assume that it works. In order for that to work, you need to have extra criteria. Turns out that these extra criteria are quite mundane. So usually it does apply in real life cases, which is why we never check. And the condition is called ergodic. ''')
    st.markdown(r'''''')
    st.markdown(r'''## Ergodicity in MCMC''')
    st.markdown(r'''''')
    st.markdown(r'''Now that we've established the importance of detailed balance, let's explore two crucial properties of Markov chains that are essential for MCMC methods: convergence and uniqueness. These properties are often taken for granted in astronomical applications, but they're not guaranteed for all Markov chains.''')
    st.markdown(r'''''')
    st.markdown(r'''Convergence, in the context of MCMC, means that no matter where we start our walkers, they will eventually sample from the target distribution. Mathematically, we express this as:''')
    st.markdown(r'''''')
    st.markdown(r'''$\lim_{n\to\infty} T^n \tilde{p}(z) = p(z)$''')
    st.markdown(r'''''')
    st.markdown(r'''Where $\tilde{p}(z)$ is any starting distribution, $T$ is our transition probability, and $p(z)$ is our target distribution. Imagine we're running an open day for Canberra tourism, and we've given our travel guide to a diverse group of people from all over Australia. Convergence means that, given enough time, all these people will start behaving like true Canberrans, regardless of where they originally came from.''')
    st.markdown(r'''''')
    st.markdown(r'''Uniqueness, on the other hand, ensures that there's only one stationary distribution that our Markov chain can converge to. This is crucial because we've already shown that our target distribution is a stationary distribution (thanks to detailed balance), but we need to make sure it's the only one. If uniqueness holds, we can be confident that our MCMC method will converge to the distribution we actually want to sample from.''')
    st.markdown(r'''''')
    st.markdown(r'''### Counterexamples to Convergence and Uniqueness''')
    st.markdown(r'''''')
    st.markdown(r'''However, it's important to note that neither convergence nor uniqueness is guaranteed for all Markov chains. Let's look at a couple of counterexamples to illustrate this point.''')
    st.markdown(r'''''')
    st.markdown(r'''First, consider a very simple "travel guide" that just tells everyone to stay where they are. Mathematically, this is equivalent to a transition matrix $T = Id$ (the identity matrix). In this case, every distribution is a stationary distribution because no one ever moves. While this Markov chain technically "converges" instantly, it doesn't converge to a unique distribution. If we started with a uniform distribution of people across Canberra, they'd stay uniform. If we started with everyone in Weston, they'd all stay in Weston. This violates our uniqueness requirement.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's look at a case where convergence itself fails. Imagine a simplified Canberra with only two suburbs: Weston and Weston Creek. Our travel guide tells people in Weston to go to Weston Creek, and vice versa, every single day. Mathematically, this is represented by the transition matrix:''')
    st.markdown(r'''''')
    st.markdown(r'''$T = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$''')
    st.markdown(r'''''')
    st.markdown(r'''This chain does have a stationary distribution $p^*(z) = [0.5, 0.5]$ - if we start with exactly half the population in each suburb, this will remain true as people swap back and forth. We can verify this by checking that $Tp^* = p^*$. However, for any other starting distribution $\tilde{p}(z) \neq p^*(z)$, the chain will never converge. Instead, we get:''')
    st.markdown(r'''''')
    st.markdown(r'''$p^{(t)}(z) = \begin{cases} \tilde{p}(z), & \mathrm{if } t \mathrm{ is even} \\ 1 - \tilde{p}(z), & \mathrm{if } t \mathrm{ is odd} \end{cases}$''')
    st.markdown(r'''''')
    st.markdown(r'''If we start with 70% of people in Weston and 30% in Weston Creek, we'll eternally oscillate between this state and its reverse, never settling into a stable distribution. Mathematically, this means:''')
    st.markdown(r'''''')
    st.markdown(r'''$\lim_{n\to\infty} T^n \tilde{p}(z) \neq p^*(z)$''')
    st.markdown(r'''''')
    st.markdown(r'''These examples highlight why we can't simply assume that any MCMC method will work correctly. We need additional criteria to ensure both convergence and uniqueness. Fortunately, there's a property called ergodicity that, when satisfied, guarantees both of these desirable features. Most practical MCMC methods, including the Metropolis algorithm we've discussed, are designed to be ergodic.''')
    st.markdown(r'''''')
    st.markdown(r'''### Understanding Ergodicity''')
    st.markdown(r'''''')
    st.markdown(r'''This term might sound familiar to those who've studied statistical mechanics, but don't worry if you haven't - it's a concept that's often used to sound more technical than necessary. I often joke that "I explore Canberra ergodically."''')
    st.markdown(r'''''')
    st.markdown(r'''In essence, ergodicity means that a system can explore all its possible states over time and reach an equilibrium. In the context of Markov chains and MCMC, it's the key to guaranteeing that our sampling methods will work correctly. An ergodic Markov chain will converge to a unique stationary distribution, regardless of its starting point.''')
    st.markdown(r'''''')
    st.markdown(r'''Mathematically, a Markov chain is considered ergodic if it satisfies three properties:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Irreducibility: The chain can go from any state to any other state in a finite number of steps. In our Canberra analogy, this means you can travel from any suburb to any other suburb given enough time. It's like having the freedom to go anywhere in Canberra whenever you want.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Aperiodicity: The chain doesn't visit states in a predictable cyclic pattern. This prevents the oscillating behavior we saw in our Weston/Weston Creek example, where people were swapping back and forth endlessly.''')
    st.markdown(r'''''')
    st.markdown(r'''3. Positive recurrence: The expected time to return to any state is finite. This prevents the chain from wandering off to infinity. Imagine dropping a dye from Mount Stromlo and letting it diffuse - without positive recurrence, it could keep spreading out forever without reaching an equilibrium.''')
    st.markdown(r'''''')
    st.markdown(r'''These conditions help us avoid the pathological cases we discussed earlier. For instance, the identity transition matrix (where everyone stays put) violates irreducibility. The periodic swapping between Weston and Weston Creek violates aperiodicity.''')
    st.markdown(r'''''')
    st.markdown(r'''When a Markov chain satisfies these three conditions, we can prove two important results:''')
    st.markdown(r'''''')
    st.markdown(r'''1. A unique stationary distribution exists.''')
    st.markdown(r'''2. The chain will converge to this stationary distribution from any starting point.''')
    st.markdown(r'''''')
    st.markdown(r'''This is why gas particles in a room reach a Boltzmann distribution - they ergodically explore the entire phase space. But if you were to put a vacuum screen in the middle of the room, preventing particles from one half from reaching the other, you'd break this ergodicity and the system wouldn't reach equilibrium.''')
    st.markdown(r'''''')
    st.markdown(r'''In practice, most MCMC methods used in astronomy, including the Metropolis algorithm we discussed earlier, are designed to be ergodic. However, it's worth being aware of potential pitfalls:''')
    st.markdown(r'''''')
    st.markdown(r'''- Irreducibility can be violated if your posterior has completely separated modes with zero probability between them. It's like having two rooms with a perfect vacuum between them - particles starting in one room can never reach the other.''')
    st.markdown(r'''''')
    st.markdown(r'''- Aperiodicity is usually satisfied unless you've explicitly designed a periodic transition scheme.''')
    st.markdown(r'''''')
    st.markdown(r'''Positive recurrence is typically ensured by setting proper priors that bound your parameter space. This is like ensuring your particles are in a finite room rather than an infinite universe.''')
    st.markdown(r'''''')
    st.markdown(r'''While we often don't explicitly check for ergodicity in astronomical applications, understanding these conditions can help diagnose issues when MCMC methods aren't behaving as expected. It's really hard to accidentally code something that's not ergodic unless you're trying to be pathological.''')
    st.markdown(r'''''')
    st.markdown(r'''## The Power and Practicality of MCMC''')
    st.markdown(r'''''')
    st.markdown(r'''Let's take a moment to appreciate the remarkable power of Markov Chain Monte Carlo (MCMC). This method allows us to sample from potentially infinite-dimensional spaces, which is truly astounding. As long as we can run the Metropolis algorithm and ensure the chain is ergodic, we can, in principle, sample the posterior of anything. Of course, there's an asterisk here – the time to convergence could be a Hubble time! But this theoretical capability is still fascinating and puts MCMC at the heart of modern Bayesian computation.''')
    st.markdown(r'''''')
    st.markdown(r'''The Metropolis algorithm, a cornerstone of MCMC, works because it satisfies some key criteria. For a given target distribution $p(z)$, it uses an acceptance probability $A(z', z) = \min(1, \dfrac{p(z)}{p(z')})$, which ensures detailed balance. This, in turn, guarantees that $p(z)$ is a stationary distribution of the Markov chain. The transition probability in the algorithm is given by $T(z', z) = q(z | z') A(z', z)$, where $q(z | z')$ is our proposal distribution. If this transition matrix is ergodic, we're assured convergence from any initial distribution to the unique stationary distribution $p(z)$.''')
    st.markdown(r'''''')
    st.markdown(r'''In practice, implementing the Metropolis algorithm is surprisingly straightforward. We start at some point $z$, make proposals based on a symmetric distribution $q(z | z') = q(z' | z)$, and decide whether to accept each proposal based on the ratio of posterior probabilities. We draw $u \sim U(0,1)$ and accept if $\dfrac{p(z)}{p(z')} > u$. If accepted, we move to the new point; if rejected, we stay put and count the current point again. This simple process, repeated many times, allows us to sample from complex posteriors that we couldn't directly sample from otherwise.''')
    st.markdown(r'''''')
    st.markdown(r'''The beauty of this method lies in its dependence only on the ratio $\dfrac{p(z)}{p(z')}$, making it independent of the often hard-to-compute normalization constant. This universal sampling ability of MCMC is analogous to how neural networks are universal function approximators. Just as neural networks can theoretically create consciousness (a slightly unsettling thought), MCMC demonstrates that there's nothing fundamentally hard to understand from a machine learning standpoint. We can always sample the posterior; it's just a question of how long it might take.''')
    st.markdown(r'''''')
    st.markdown(r'''### Practical Considerations in MCMC Implementation''')
    st.markdown(r'''''')
    st.markdown(r'''However, when you're facing a deadline for research or a project, you can't rely solely on the theoretical guarantee of eventual convergence. This is where the engineering aspects of MCMC come into play – how do we make it run efficiently? A crucial factor is the choice of the proposal distribution $q$. While theoretically any $q$ could work, some choices lead to faster convergence than others. In practice, we often choose a Gaussian distribution centered on the current state because it's easy to sample from. But the width of this Gaussian is critical and requires careful tuning.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's delve deeper into a practical example that beautifully illustrates the power and nuances of Markov Chain Monte Carlo (MCMC): estimating the unknown mean of a Gaussian distribution. This scenario is ubiquitous in various fields, from estimating the average wealth in a city to predicting election vote percentages. In our case, we're dealing with a random variable $Y$ that follows a normal distribution with an unknown mean $\theta$ and a unit variance: $Y \sim N(\theta, 1)$.''')
    st.markdown(r'''''')
    st.markdown(r'''To approach this problem using Bayesian inference, we need to specify a prior for $\theta$. A Cauchy distribution is often an excellent choice due to its long tails. Mathematically, we express this prior as $p(\theta) \propto \dfrac{1}{1 + \theta^2}$. The beauty of this prior lies in its reluctance to taper off quickly, effectively leaving the door slightly ajar for unexpected outcomes. It's like maintaining a healthy scientific skepticism – we're not ruling out the possibility of extreme values, however unlikely they might be!''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's construct our posterior distribution. It's the product of the likelihood (based on our observed data) and the prior:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(\theta | \{y_i\}) \propto \prod_i p(y_i | \theta) p(\theta) \propto \prod_i \exp(-(y_i - \theta)^2/2) / (1 + \theta^2)$''')
    st.markdown(r'''''')
    st.markdown(r'''This posterior is a fascinating beast – neither purely Gaussian nor purely Cauchy, but a complex product of the two. It's precisely this complexity that makes MCMC shine. We can't directly sample from this distribution using simpler methods, but MCMC allows us to dance around this probabilistic landscape with surprising grace.''')
    st.markdown(r'''''')
    st.markdown(r'''To implement MCMC for this problem using the Metropolis algorithm, we begin with an initial guess $\theta_0$. This could be an educated guess based on our data or even a random starting point – the beauty of MCMC is that, given enough time, it will converge to the correct distribution regardless of where we start. We then propose new values using a symmetric proposal distribution. A common and convenient choice is a Gaussian centered on our current value: $\theta^{(t)} \sim N(\theta^{(t-1)}, v^2)$, where $v$ is the width of our proposal distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''For each proposal, we calculate the acceptance probability:''')
    st.markdown(r'''''')
    st.markdown(r'''$A(\theta^{(t-1)}, \theta^{(t)}) = \min(1, \frac{p(\theta^{(t)} | \{y_i\})}{p(\theta^{(t-1)} | \{y_i\})})$''')
    st.markdown(r'''''')
    st.markdown(r'''This is where the magic of MCMC truly happens. We're essentially taking a drunken walk through our parameter space, randomly proposing new values and deciding whether to accept them based on how likely they are compared to our current position. Yet, remarkably, theory guarantees that this process will eventually sample from our target distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''The efficiency of our sampling, however, hinges crucially on our choice of the proposal distribution's width, $v$. This is where the art of MCMC implementation comes into play, and it's a aspect that's often underappreciated by newcomers to the field.''')
    st.markdown(r'''''')
    st.markdown(r'''If $v$ is too small (say, $v = 0.01$), we'll have a very high acceptance ratio. At first glance, this might seem ideal – we're accepting most of our proposals, so surely we're exploring the space well? But this is a trap. It leads to a painfully long "burn-in" period and highly correlated samples. Imagine you're trying to explore a city, but you can only take baby steps. You started far away, and at this rate, it'll take eons to reach your destination, let alone explore it properly! Your samples will be highly correlated because each new position is very close to the previous one.''')
    st.markdown(r'''''')
    st.markdown(r'''On the flip side, if $v$ is too large (like $v = 0.5$), we'll have a very low acceptance rate. Now you can teleport from one side of the city to another in one giant leap, which sounds great for quickly covering ground. But there's a catch – most of your proposals will be rejected because they're too far from the high-probability regions of your distribution. You'll spend most of your time standing still, rejecting proposals to jump to unlikely locations. This also leads to high correlation between samples, as you often remain in the same spot for many iterations.''')
    st.markdown(r'''''')
    st.markdown(r'''The sweet spot is usually around $v = 0.1$. This is where you get a good balance of exploration and acceptance. You're taking steps that are large enough to explore the space efficiently, but not so large that most of your proposals are rejected. This typically results in an acceptance rate of about 30-40%, which is a good rule of thumb for many problems.''')
    st.markdown(r'''''')
    st.markdown(r'''### Autocorrelation and Thinning''')
    st.markdown(r'''''')
    st.markdown(r'''It's crucial to understand that even with a suboptimal proposal distribution, your samples are still drawn from the correct posterior – they're just not as efficient as they could be. You might think you've drawn 1000 samples, but due to high correlation, you might effectively only have 10 independent samples. This is why we often employ techniques like "thinning" – only keeping every nth sample to reduce correlation between samples.''')
    st.markdown(r'''''')
    st.markdown(r'''To determine an appropriate thinning interval, we can use the concept of autocorrelation length. This involves analyzing how similar our chain is to itself when shifted by different amounts. It's analogous to the two-point correlation function used in astronomy to analyze galaxy distributions. If you shift a highly correlated chain by one step, positive values will likely still be positive, and negative values will still be negative. But for a well-mixed chain, a small shift should result in no correlation.''')
    st.markdown(r'''''')
    st.markdown(r'''In practice, you can visualize this by plotting your chain and looking for "clumping" of values. A well-mixed chain should look like noise, with no discernible patterns. You can also calculate the autocorrelation function numerically and look for the lag at which it drops below a certain threshold.''')
    st.markdown(r'''''')
    st.markdown(r'''Remember, the Metropolis algorithm we've discussed here is incredibly powerful – it solves a vast majority of astronomy questions involving MCMC. But its power comes from the interplay of theory and practice. The theory (detailed balance, ergodicity) guarantees that we'll eventually sample from the correct distribution. The practice (choosing proposal distributions, handling burn-in, applying thinning) determines how quickly and efficiently we get there.''')
    st.markdown(r'''''')
    st.markdown(r'''In your research, whether you're estimating cosmological parameters, characterizing exoplanets, or analyzing large-scale structure, these principles will help you harness the full power of MCMC methods. You're not just running an algorithm; you're conducting a sophisticated exploration of a probabilistic space, guided by the elegant mathematics of Markov chains and the practical wisdom of effective sampling strategies.''')
    st.markdown(r'''''')
    st.markdown(r'''## The Metropolis-Hastings Algorithm: A Powerful Generalization''')
    st.markdown(r'''''')
    st.markdown(r'''To really close up our discussion on MCMC methods, let's delve into the Metropolis-Hastings algorithm, a powerful extension of the Metropolis algorithm we've been focusing on. If you're clear about the logic behind the Metropolis algorithm and why it works, you're in a great position to understand this generalization.''')
    st.markdown(r'''''')
    st.markdown(r'''Recall that in the Metropolis algorithm, we used a symmetric proposal distribution, meaning $q(z'|z) = q(z|z')$. The Metropolis-Hastings algorithm lifts this restriction, allowing for non-symmetric proposal distributions where $q(z'|z) \neq q(z|z')$. This generalization can be powerful in situations where we have some prior knowledge about the direction or scale of likely moves in our parameter space.''')
    st.markdown(r'''''')
    st.markdown(r'''To compensate for this asymmetry in our proposal distribution, we need to adjust our acceptance probability to maintain the detailed balance condition. The new acceptance probability for the Metropolis-Hastings algorithm is:''')
    st.markdown(r'''''')
    st.markdown(r'''$A(z', z) = \min(1, \dfrac{p(z) q(z'|z)}{p(z') q(z|z')})$''')
    st.markdown(r'''''')
    st.markdown(r'''This might look more complex than the Metropolis acceptance probability, but it's doing something quite intuitive. It's not just comparing the probability of the proposed state to the current state, but also taking into account the asymmetry in how we proposed the move.''')
    st.markdown(r'''''')
    st.markdown(r'''### Proving Detailed Balance''')
    st.markdown(r'''''')
    st.markdown(r'''As always, the crucial property we need to verify is that this new algorithm satisfies detailed balance. This is vital because detailed balance ensures that our target distribution $p(z)$ is a stationary distribution of our Markov chain. Let's walk through the proof step by step:''')
    st.markdown(r'''''')
    st.markdown(r'''1. We start with the left side of the detailed balance equation:''')
    st.markdown(r'''   $T(z', z) p(z') = q(z|z') A(z', z) p(z')$''')
    st.markdown(r'''''')
    st.markdown(r'''2. Now, let's substitute our new acceptance probability:''')
    st.markdown(r'''   $= q(z|z') \min(1, \dfrac{p(z) q(z'|z)}{p(z') q(z|z')}) p(z')$''')
    st.markdown(r'''''')
    st.markdown(r'''3. We can use the fact that all these terms are positive to push them inside the min operation:''')
    st.markdown(r'''   $= \min(q(z|z') p(z'), q(z'|z) p(z))$''')
    st.markdown(r'''''')
    st.markdown(r'''4. Now, we can use the property that min(a,b) = min(b,a) to swap the order:''')
    st.markdown(r'''   $= \min(q(z'|z) p(z), q(z|z') p(z'))$''')
    st.markdown(r'''''')
    st.markdown(r'''5. We can factor out $q(z'|z)$ and $p(z)$, which are positive:''')
    st.markdown(r'''   $= q(z'|z) \min(1, \dfrac{p(z') q(z|z')}{p(z) q(z'|z)}) p(z)$''')
    st.markdown(r'''''')
    st.markdown(r'''6. And finally, we recognize this as:''')
    st.markdown(r'''   $= T(z, z') p(z)$''')
    st.markdown(r'''''')
    st.markdown(r'''This completes our proof that the Metropolis-Hastings algorithm satisfies detailed balance. Each step in this derivation is quite mechanical, but it's important to understand the logic behind it. We're essentially showing that the probability of transitioning from $z'$ to $z$, weighted by the probability of being at $z'$, is the same as the reverse process. This symmetry is what ensures our Markov chain will converge to our desired distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''The next thing to check is that this chain is ergodic. Most of the time, we don't explicitly check this; we just assume it's true because it's very hard to accidentally make a chain non-ergodic. If ergodicity holds, we're guaranteed that by following this "travel guidebook," we'll be able to sample from our target distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''It's worth noting that the Metropolis-Hastings algorithm, like its predecessor, is independent of the normalization constant of our target distribution. This is crucial in Bayesian inference, where we often work with unnormalized posterior distributions.''')
    st.markdown(r'''''')
    st.markdown(r'''The development of MCMC methods is an active area of research. Even at recent machine learning conferences, researchers are proposing new algorithms to make sampling faster and more efficient, especially for challenging problems like sampling the posterior of neural network parameters. These new methods might seem quite exotic, but they all share the same fundamental principle: they satisfy detailed balance, ensuring that they will, in principle, sample from the correct posterior distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''## Gibbs Sampling: A Bridge Between Analytical and MCMC Methods''')
    st.markdown(r'''''')
    st.markdown(r'''Let's conclude our exploration of MCMC methods with Gibbs sampling, a powerful technique that bridges the gap between analytical sampling methods and full MCMC approaches. This method is particularly interesting because it seems to combine two different scenarios we've discussed: cases where we can sample exactly (like with inverse CDF) and cases where we need MCMC.''')
    st.markdown(r'''''')
    st.markdown(r'''Gibbs sampling addresses a common scenario in complex problems: some dimensions of your multivariate distribution are easy to sample from analytically, while others require MCMC techniques. Instead of applying MCMC to all dimensions, Gibbs sampling allows us to "divide and conquer," potentially saving significant computational resources.''')
    st.markdown(r'''''')
    st.markdown(r'''### The Gibbs Sampling Algorithm''')
    st.markdown(r'''''')
    st.markdown(r'''The key idea of Gibbs sampling is to sample one variable at a time while conditioning on (or "fixing") all other variables. For a multivariate random variable $X = (x_1, x_2, ..., x_k)$, one iteration of the Gibbs sampler proceeds as follows:''')
    st.markdown(r'''''')
    st.markdown(r'''$x_1^{(t)} \sim p(x_1 | x_2^{(t-1)}, ..., x_k^{(t-1)})$''')
    st.markdown(r'''''')
    st.markdown(r'''$x_2^{(t)} \sim p(x_2 | x_1^{(t)}, x_3^{(t-1)}, ..., x_k^{(t-1)})$''')
    st.markdown(r'''''')
    st.markdown(r'''...''')
    st.markdown(r'''''')
    st.markdown(r'''$x_m^{(t)} \sim p(x_m | x_1^{(t)}, x_2^{(t)}, ..., x_{m-1}^{(t)}, x_{m+1}^{(t-1)}, ..., x_k^{(t-1)})$''')
    st.markdown(r'''''')
    st.markdown(r'''...''')
    st.markdown(r'''''')
    st.markdown(r'''$x_k^{(t)} \sim p(x_k | x_1^{(t)}, ..., x_{k-1}^{(t)})$''')
    st.markdown(r'''''')
    st.markdown(r'''This approach is particularly useful when you encounter high-dimensional problems. There are many cases where researchers run MCMC for days without convergence, often because they're trying to sample many parameters when most of them are easy to sample analytically, and only a few require MCMC. By using Gibbs sampling, you can dramatically reduce computational time and resource usage.''')
    st.markdown(r'''''')
    st.markdown(r'''It's important to be thoughtful about your approach. Sometimes, just writing a few more lines of code can save your time, computing resources, and energy.''')
    st.markdown(r'''''')
    st.markdown(r'''### Why Gibbs Sampling Works''')
    st.markdown(r'''''')
    st.markdown(r'''But why does Gibbs sampling work? Intuitively, it might seem clear - if you're sampling a 2D Gaussian, alternating between sampling the X and Y dimensions should eventually give you the joint distribution. However, we need to prove that sampling from conditional distributions will indeed yield the desired joint distribution, especially for high-dimensional, potentially multimodal distributions.''')
    st.markdown(r'''''')
    st.markdown(r'''Surprisingly, the proof is quite simple. Gibbs sampling can be viewed as a special case of the Metropolis-Hastings algorithm. When updating the k-th component, the Gibbs sampler is equivalent to using a proposal distribution:''')
    st.markdown(r'''''')
    st.markdown(r'''$q_k(z | z') = p(z_k | z'_{\backslash k})$''')
    st.markdown(r'''''')
    st.markdown(r'''Here, $z'_{\backslash k}$ denotes all components except for the k-th component. ''')
    st.markdown(r'''''')
    st.markdown(r'''Recall the acceptance probability for Metropolis-Hastings:''')
    st.markdown(r'''''')
    st.markdown(r'''$A(z', z) = \min(1, \dfrac{p(z) q(z'|z)}{p(z') q(z|z')})$''')
    st.markdown(r'''''')
    st.markdown(r'''For Gibbs sampling, this becomes:''')
    st.markdown(r'''''')
    st.markdown(r'''$A(z', z) = \frac{p(z_k | z_{\backslash k}) p(z_{\backslash k}) p(z'_k | z_{\backslash k})}{p(z'_k | z'_{\backslash k}) p(z'_{\backslash k}) p(z_k | z'_{\backslash k})}$''')
    st.markdown(r'''''')
    st.markdown(r'''Because $z'_{\backslash k} = z_{\backslash k}$ (these components don't change in this iteration), this simplifies to:''')
    st.markdown(r'''''')
    st.markdown(r'''$A(z', z) = \frac{p(z_k | z_{\backslash k}) p(z_{\backslash k}) p(z'_k | z_{\backslash k})}{p(z'_k | z_{\backslash k}) p(z_{\backslash k}) p(z_k | z_{\backslash k})} = 1$''')
    st.markdown(r'''''')
    st.markdown(r'''Thus, Gibbs sampling is a special case of the Metropolis-Hastings algorithm with an acceptance probability of 1. Since we've already proven that Metropolis-Hastings satisfies detailed balance and converges to the target distribution, we know Gibbs sampling will too.''')
    st.markdown(r'''''')
    st.markdown(r'''### Practical Applications and Considerations''')
    st.markdown(r'''''')
    st.markdown(r'''Let's explore some interesting corollaries and practical applications of Gibbs sampling. Consider a simple example: sampling from a two-dimensional Gaussian distribution. In NumPy, you can sample a 2D Gaussian with one line of code. But what if you need to sample from a 100-dimensional Gaussian? That's not as straightforward in NumPy. This is where Gibbs sampling shines.''')
    st.markdown(r'''''')
    st.markdown(r'''For any dimensional Gaussian, we know that the conditional distribution is also a Gaussian. This means if we can sample from a 1D conditional Gaussian, we can sample from a Gaussian of any dimension. In fact, there's an important corollary: even with just the ability to sample from a 1D Gaussian, you can theoretically sample from an infinite-dimensional Gaussian!''')
    st.markdown(r'''''')
    st.markdown(r'''Let's look at a specific case. Assume we have a 2D Gaussian with zero mean and a covariance matrix:''')
    st.markdown(r'''''')
    st.markdown(r'''$\Sigma = \begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix}$''')
    st.markdown(r'''''')
    st.markdown(r'''where $\rho$ is the correlation coefficient. The conditional distribution for this case is:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(\theta_1^t | \theta_2^t) = N(\rho\theta_2^t, 1 - \rho^2)$''')
    st.markdown(r'''''')
    st.markdown(r'''This example illustrates an important point about Gibbs sampling. We said earlier that Gibbs sampling is equivalent to a Metropolis-Hastings algorithm with an acceptance probability of one. You might think this is great because you're always accepting steps. However, an acceptance probability of one doesn't mean your samples are independent.''')
    st.markdown(r'''''')
    st.markdown(r'''Imagine a highly correlated Gaussian ($\rho$ close to 1). If you start at a point and keep sampling from the conditional distributions, you'll zigzag through the space. It will take many steps before your samples become effectively independent. Even though each new sample doesn't "know" about the old sample directly, they're still highly correlated due to the structure of the distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''Looking at the chain for $\rho = 0.9$, you'll see it takes a long time to reach the equilibrium state. Even after reaching equilibrium, the samples remain highly correlated. This is because you're zigzagging through a very "tilted" distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''Conversely, if $\rho$ is close to zero, your samples will quickly decorrelate. Even though you're still sampling from conditional distributions, each step takes you to a relatively independent part of the distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''This serves as a warning: even though Gibbs sampling has an acceptance probability of one, it's not necessarily more advantageous than other methods. Its main advantage comes from being able to use exact sampling methods for some dimensions.''')
    st.markdown(r'''''')
    st.markdown(r'''It's worth noting that you don't always have to sample one dimension at a time. Often, you can sample blocks of variables together. For instance, if you have 200 parameters and conditioning on 100 of them gives you a Gaussian distribution, you can sample those 100 at once. Then you might do MCMC on the other 100 together. The proof of why this works remains the same as for sampling one variable at a time.''')
    st.markdown(r'''''')
    st.markdown(r'''The key takeaway is that Gibbs sampling allows you to break down complex problems into simpler parts. You can alternate between different methods like rejection sampling, inverse CDF, and MCMC, choosing the most efficient method for each part of your problem. In my experience, I rarely see students do this in practice, but it's a powerful approach that you should always consider.''')
    st.markdown(r'''''')
    st.markdown(r'''## Summary''')
    st.markdown(r'''''')
    st.markdown(r'''This brings us to the conclusion of not just this lecture, but our entire course series. We began by discussing the importance and philosophy of Bayesian methods in science. We're concluding with the powerful realization that, in principle, nothing is too hard. We can sample from any posterior distribution - the remaining challenges are engineering tasks to make these methods work efficiently in finite time.''')
    st.markdown(r'''''')
    st.markdown(r'''I hope this course has given you a comprehensive overview of statistics and machine learning in astronomy. I know it hasn't been an easy journey. I've heard some of you say, "It's a lot of math." But I believe I haven't taught any unnecessary math. Even though the assignments might not require all this mathematical detail, it's crucial to have this bird's-eye view with the minimum rigor needed. This understanding will serve you well in your research, allowing you to apply these techniques more effectively and creatively.''')
    st.markdown(r'''''')
    st.markdown(r'''To recap, in this final section, we've covered:''')
    st.markdown(r'''1. Sampling through Markov Chains''')
    st.markdown(r'''2. Theoretical Support for Markov Chain Monte Carlo''')
    st.markdown(r'''3. The Metropolis Algorithm''')
    st.markdown(r'''4. The Metropolis-Hastings Algorithm''')
    st.markdown(r'''5. Gibbs Sampling''')
    st.markdown(r'''''')
    st.markdown(r'''These tools, when understood deeply, will empower you to tackle complex probabilistic problems in astronomy and beyond. Remember, the journey from a simple idea – taking a random walk guided by probability ratios – to a powerful tool for probing the mysteries of the universe is a testament to the beauty and power of computational statistics.''')
    st.markdown(r'''''')

#if __name__ == '__main__':
show_page()