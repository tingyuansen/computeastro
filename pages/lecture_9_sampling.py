import streamlit as st
from streamlit_app import navigation_menu

def show_page():
    st.set_page_config(page_title="Comp Astro",
                       page_icon="./image/tutor_favicon.png", layout="wide")

    # Page Title
    st.title('Sampling - Basic Techniques')
    navigation_menu()

    # Embed the external HTML page
    # st.info('''
    # Course Slides
    # ''')
    # slideshare_embed_code = """
    #     <iframe src="https://www.slideshare.net/slideshow/embed_code/key/2FRoyH4o8Op9Mm" 
    #             width="700" height="394" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" 
    #             style="border:0px solid #000; border-width:0px; margin-bottom:0px; max-width: 100%;" allowfullscreen> 
    #     </iframe> 
    # """
    # components.html(slideshare_embed_code, height=412)

    # st.markdown('---')
    
    st.markdown(r'''''')
    st.markdown(r'''Welcome to today's lecture on sampling in machine learning. We'll be exploring the crucial role of sampling techniques in the broader context of machine learning and their applications in astronomical research.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's begin with a brief recap. In our previous discussions, we've covered a wide range of topics in machine learning, from linear regression to logistic regression, and from Gaussian processes to neural networks. This overview represents the core of what we typically consider machine learning. While neural networks dominate much of today's machine learning discourse, our broad understanding of the classical landscape remains crucial for appreciating the need for these more modern tools.''')
    st.markdown(r'''''')
    st.markdown(r'''With our previous lecture, we concluded this overview. However, we have more to explore. Today and in our next lecture, we'll focus on a critical topic in machine learning: sampling. Rather than delving into the specifics of sampling techniques, we'll concentrate on understanding how sampling fits into the broader machine learning framework.''')
    st.markdown(r'''''')
    st.markdown(r'''Many of you have likely used sampling in your research, perhaps through functions like `numpy.random`. The key question we'll address is: Why is sampling so important for machine learning? We'll tackle this puzzle today, starting with how sampling fits into the machine learning landscape.''')
    st.markdown(r'''''')
    st.markdown(r'''In our next lecture, we'll continue our discussion on sampling, shifting our focus to the more practical aspects of Markov Chain Monte Carlo (MCMC). Today, however, we'll cover three basic sampling techniques: the inverse Cumulative Distribution Function (CDF) method, rejection sampling, and importance sampling. We'll explore what these methods are and, importantly, their limitations.''')
    st.markdown(r'''''')
    st.markdown(r'''It's crucial to note that these methods often have limitations, particularly in higher-dimensional spaces, which are common in substantial machine learning tasks. Even in a seemingly simple scenario like linear regression with 10 features, we're dealing with a 10-dimensional weight space. This 10-dimensional case can be quite challenging for these basic sampling techniques. Nevertheless, understanding these methods is important as they can still be useful in certain scenarios.''')
    st.markdown(r'''''')
    st.markdown(r'''A crucial concept in any sampling method is the effective sample size. This refers to how efficiently each sample contributes to our desired outcome per unit of computation. We'll discuss this concept today, but it becomes even more critical when we talk about MCMC in the next lecture. If you go through the tutorials for this week and next, you'll see a strong focus on calculating the effective sample size for various sampling techniques.''')
    st.markdown(r'''''')
    st.markdown(r'''## Historical Importance of Sampling''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's step back and discuss why sampling has been historically important. Sampling has always played a significant role in mathematics. Even before the advent of computers, people used sampling techniques for statistics and what we now call machine learning. A classic example is the estimation of pi, which occurred independently in cultures around the world.''')
    st.markdown(r'''''')
    st.markdown(r'''The Buffon's needle experiment illustrates this concept well. The process involves dropping needles of length $L$ onto parallel lines separated by distance $D$, then counting how many needles cross a line. The probability of a needle crossing a line is related to pi. If a needle falls perpendicular to the lines, the probability of crossing is simply $L/D$. For a needle falling at an angle $A$ to the vertical, the probability becomes $(L * sin(A)) / D$. With randomly thrown needles, the angle $A$ is uniformly distributed between 0 and $\pi$. The overall probability of crossing is:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(\mathrm{crossing}) = \dfrac{L}{\pi D} \int_{0}^{\pi} \sin(A) dA = \dfrac{2L}{\pi D}$''')
    st.markdown(r'''''')
    st.markdown(r'''If we throw N needles and n of them cross a line, we can estimate:''')
    st.markdown(r'''''')
    st.markdown(r'''$\dfrac{n}{N} \approx \dfrac{2L}{\pi D}$''')
    st.markdown(r'''''')
    st.markdown(r'''Rearranging this, we get:''')
    st.markdown(r'''''')
    st.markdown(r'''$\pi \approx \dfrac{2L * N}{D * n}$''')
    st.markdown(r'''''')
    st.markdown(r'''This simple experiment allows us to estimate pi just by throwing needles and counting!''')
    st.markdown(r'''''')
    st.markdown(r'''While this example might seem far removed from our course content, it illustrates a fundamental utility of sampling: calculating the expectation of a function with respect to a distribution. This concept is crucial in machine learning, especially from a Bayesian perspective.''')
    st.markdown(r'''''')
    st.markdown(r'''## Sampling in Modern Machine Learning''')
    st.markdown(r'''''')
    st.markdown(r'''In modern computations, we often deal with incredibly complex distributions such as posterior distributions, mixture distributions, and graphical models. Many machine learning tasks boil down to estimating the expectation of a function under a distribution:''')
    st.markdown(r'''''')
    st.markdown(r'''$\mathbb{E}(f) = \int f(z) p(z) dz$''')
    st.markdown(r'''''')
    st.markdown(r'''This is where sampling techniques become invaluable, allowing us to approximate these complex integrals efficiently.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's delve deeper into why sampling is crucial in modern machine learning. If you think about most of the concepts we've been covering in the second half of this course, they really boil down to some form of integral. We're often dealing with the posterior of our models, trying to integrate over the likelihood.''')
    st.markdown(r'''''')
    st.markdown(r'''Consider the example of logistic regression we discussed recently. This illustrates a fundamental concept applicable to all machine learning tasks. On the left side of our equation, we have the predictive distribution. In a supervised learning task, we start with known training data ($X$ and $T$) and build a model of the world based on this data. Our goal is to predict $Y$ when we encounter a new data point $X^*$ that wasn't part of our original dataset. Crucially, we're not just after the best-fit line; we want to predict the entire distribution of $Y$, including the uncertainty, given $X$.''')
    st.markdown(r'''''')
    st.markdown(r'''This process is the essence of machine learning from a supervised learning perspective. It's how we summarize knowledge and make predictions based on past experiences. This is not just machine learning; it's the fundamental basis of knowledge acquisition itself.''')
    st.markdown(r'''''')
    st.markdown(r'''From a Bayesian perspective, creating such a predictive distribution requires building a world model. This model is captured by parameters, which we typically denote as $W$. This term can represent various models - linear regression, Gaussian processes, or neural networks. Fundamentally, we're asking: given some training data, can we constrain our world models?''')
    st.markdown(r'''''')
    st.markdown(r'''The key point is that our data is always uncertain, which means our parameters are also uncertain. By definition, these parameters are random variables that we need to marginalize over. Because the model itself is uncertain, our predictions are also uncertain. This single equation, in various forms, captures everything we aim to learn in machine learning.''')
    st.markdown(r'''''')
    st.markdown(r'''In many astronomical applications, we might not care about the entire predictive distribution. Often, we're only interested in the mean - we have some data, we have a world model (let's say, Lambda CDM), and our goal is to determine the uncertainty of the Lambda CDM parameters. But even in this case, the predictive distribution remains crucial because it's how we verify or nullify our models.''')
    st.markdown(r'''''')
    st.markdown(r'''We initially focused on basic models because only in those cases do we know how to solve this integral exactly. In linear regression, for instance, both the likelihood and posterior are Gaussian. When you have two Gaussians, the integral becomes doable, and the predictive distribution comes for free. But this is a very limited case. We pay a high cost by making the model super rigid. It's important to understand that we don't make the model rigid because we want to - the rigidity comes from our desire to solve the integral. The integral is only possible when the model is simple.''')
    st.markdown(r'''''')
    st.markdown(r'''As we move to more flexible models, like logistic regression, complications arise. The posterior becomes non-Gaussian because the likelihood is non-Gaussian, and the posterior is proportional to the likelihood times the prior. The dysfunction, which is the likelihood, is also non-Gaussian. This makes things highly complex.''')
    st.markdown(r'''''')
    st.markdown(r'''We discussed one strategy for Gaussian process classification - assuming that even though it's implicitly non-Gaussian, the central limit theorem kicks in at some point. Think about the "banana plots" you've seen in cosmology. When data is limited, you have some degeneracy, and it's not entirely Gaussian, hence the banana shape. But as data improves, the posterior becomes tighter. As you zoom in on the posterior, to first order, it becomes Gaussian. This is because any distribution, even if non-Gaussian, can be approximated by a Gaussian as you zoom in on the modes.''')
    st.markdown(r'''''')
    st.markdown(r'''This is also the principle behind the Laplace approximation. At the maximum point, the first-order terms drop out, leaving you with just the second order, which is exactly the Hessian matrix - the curvature. When this is Gaussian and we apply some tricks, we might be able to integrate this case.''')
    st.markdown(r'''''')
    st.markdown(r'''However, in many cases - either when your model is fixed, like in cosmological models, or when you want to use something even more flexible like a neural network - there's no chance of Gaussianizing the distribution. This is where sampling comes in.''')
    st.markdown(r'''''')
    st.markdown(r'''## Introduction to Monte Carlo Methods''')
    st.markdown(r'''''')
    st.markdown(r'''Sampling, or what we call Monte Carlo, essentially says that we can forcefully find a way to integrate this thing. Remember, this is your posterior, which might be some likelihood, and you're trying to calculate the expectation of that. It's similar to calculating the expectation of the crossing probability in our earlier Buffon's needle example.''')
    st.markdown(r'''''')
    st.markdown(r'''You can forcefully find a way to do the integral analytically, but you can also say, "Perhaps what I can do is regard this as the weighted sum of f(z), where z is sampled from p(z)." Here, p(z) serves as the weighting term. As long as I can draw samples from p(z), I can turn this integral into a finite average sum.''')
    st.markdown(r'''''')
    st.markdown(r'''But there's one catch - you can only turn the integral into a sum if you can draw from p(z), which is the posterior. This is exactly why we learn sampling techniques. In the following sections, you might wonder why we're talking about sampling, but this is the key connection between sampling and machine learning. We want to turn this predictive distribution integration into a numerical sum, and that's only possible when we can draw samples from p(z).''')
    st.markdown(r'''''')
    st.markdown(r'''So today, we're going to discuss some very simple sampling techniques. These methods will allow us to approximate complex integrals that we encounter in machine learning, especially when dealing with non-Gaussian distributions and flexible models.''')
    st.markdown(r'''''')
    st.markdown(r'''Remember, the core idea is this:''')
    st.markdown(r'''''')
    st.markdown(r'''$\mathbb{E}(f) = \int f(z) p(z) dz$''')
    st.markdown(r'''''')
    st.markdown(r'''We're approximating this expectation by the sample mean of a function of simulated random variables. In mathematical terms:''')
    st.markdown(r'''''')
    st.markdown(r'''$\mathbb{E}(f) \approx \dfrac{1}{L} \sum_{l=1}^L f(z^{(l)})$''')
    st.markdown(r'''''')
    st.markdown(r'''Where $z^{(l)}$ are independent and identically distributed samples drawn from $p(z)$.''')
    st.markdown(r'''''')
    st.markdown(r'''This Monte Carlo approach is powerful because it allows us to work with arbitrarily complex distributions, including posterior distributions, mixture distributions, and graphical models. It's a fundamental tool in our machine learning toolkit, enabling us to tackle problems that would be intractable with analytical methods alone.''')
    st.markdown(r'''''')
    st.markdown(r'''In this lecture, we'll cover four main approaches: the grid approach, inverse transform sampling (also known as the inverse cumulative distribution function method), rejection sampling, and importance sampling. These methods form the foundation for more advanced techniques we'll encounter later in the course.''')
    st.markdown(r'''''')
    st.markdown(r'''## Foundations of Sampling: Pseudorandom Number Generation''')
    st.markdown(r'''''')
    st.markdown(r'''Before we dive in, let's establish our starting point. Throughout this lecture, we'll assume that we have access to a good pseudorandom number generator for a uniform distribution. In practice, this is often provided by functions like numpy.random.uniform. You might be wondering how computers generate these uniform random numbers. Well, one common method is the Linear Congruential Generator, or LCG. This method uses a simple formula:''')
    st.markdown(r'''''')
    st.markdown(r'''$X_{n+1} = (aX_n + c) \mod m$''')
    st.markdown(r'''''')
    st.markdown(r'''where $m$ is typically a large prime number. This generates a sequence of pseudorandom numbers.''')
    st.markdown(r'''''')
    st.markdown(r'''It's worth noting that these numbers aren't truly random - if you know the parameters ($m$, $a$, and $c$), you can predict the entire sequence. This predictability can be a concern in some applications, particularly in the age of quantum computing. For those who require true randomness, alternatives exist. For instance, some systems use atmospheric noise captured by radio receivers to generate truly random numbers. But for our purposes, we'll stick with the pseudorandom generators commonly available in programming languages.''')
    st.markdown(r'''''')
    st.markdown(r'''## The Grid Approach: A Simple Starting Point''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's focus on our first sampling technique: the grid approach. This method is particularly useful for understanding one-dimensional or two-dimensional distributions, and it serves as an excellent stepping stone to more advanced techniques. The grid approach is quite intuitive, so let's walk through it step by step.''')
    st.markdown(r'''''')
    st.markdown(r'''Imagine we have a random variable with a known probability density function $p(x)$. We start by choosing an equally spaced grid of points, let's say $x_1$ through $x_n$, and we set $x_0$ to 0. At each of these grid points, we calculate the value of our probability density function, giving us $p_i = p(x_i)$ for each point. Next, we normalize these values by dividing each $p_i$ by the sum of all $p_j$. This gives us a set of values that sum to 1, representing the relative probability of each grid point.''')
    st.markdown(r'''''')
    st.markdown(r'''The next step is to calculate the cumulative distribution function (CDF) at each point. We do this by summing our normalized probabilities up to each point. This gives us a function that starts at 0 and increases to 1 as we move through our grid points.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, here's where the sampling comes in. To generate a sample, we first draw a uniform random number between 0 and 1. Then, we find where this number falls in our cumulative distribution function. The grid point corresponding to this location becomes our sampled value.''')
    st.markdown(r'''''')
    st.markdown(r'''Visually, you can imagine this as dividing a line segment from 0 to 1 into bins, where the width of each bin corresponds to the probability of that value. When we generate our uniform random number, it "falls" into one of these bins, and we return the corresponding $x$ value.''')
    st.markdown(r'''''')
    st.markdown(r'''This grid approach has several advantages. First, it's straightforward to implement and understand. You can easily visualize what's happening at each step. Second, it works for any density distribution - you don't need to assume any particular shape for your probability function. Moreover, you can extend this method to multivariate densities. For a two-dimensional distribution, for example, you'd use a 2D grid of boxes instead of a 1D line of bins.''')
    st.markdown(r'''''')
    st.markdown(r'''However, the grid approach also has some limitations. The most obvious is that our samples are discrete - we're only generating values from our grid points, which only approximates the true continuous distribution. Another significant issue arises as we move to higher dimensions. The number of grid points we need grows exponentially with the number of dimensions. In one dimension, we might get by with 100 grid points. In two dimensions, we'd need 10,000. In three dimensions, a million. You can see how this quickly becomes impractical for high-dimensional problems.''')
    st.markdown(r'''''')
    st.markdown(r'''## Inverse CDF Method: A Continuous Approach''')
    st.markdown(r'''''')
    st.markdown(r'''Let's delve deeper into our exploration of sampling techniques, focusing now on the inverse Cumulative Distribution Function (CDF) method. This approach can be seen as a natural extension of the grid method we discussed earlier, taking it to its logical conclusion by considering infinitesimally small bins.''')
    st.markdown(r'''''')
    st.markdown(r'''If you've studied calculus, you're familiar with the concept of making bins infinitely small. This is precisely the idea behind the inverse CDF method. While the grid approach required us to discretize our distribution, the inverse CDF method allows us to work with continuous distributions directly.''')
    st.markdown(r'''''')
    st.markdown(r'''### The Mathematics of Inverse CDF Sampling''')
    st.markdown(r'''''')
    st.markdown(r'''The core idea is simple yet powerful: we start with a uniform distribution that we know how to sample from, and we transform these samples to follow our desired distribution. Mathematically, if we have $Z \sim U(0,1)$, we want to find a transformation $f: Z \rightarrow X$ such that $X$ follows our target distribution $p(x)$.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's break down the math behind this. In calculus, when we change variables in an integral, we introduce a Jacobian term. This principle applies here:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(x) = p(z) * |\dfrac{dz}{dx}|$''')
    st.markdown(r'''''')
    st.markdown(r'''Since $Z$ is uniformly distributed between 0 and 1, $p(z) = 1$. Thus, we have:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(x) = |\dfrac{dz}{dx}|$''')
    st.markdown(r'''''')
    st.markdown(r'''Integrating both sides, we get:''')
    st.markdown(r'''''')
    st.markdown(r'''$z = \int p(x') dx' = P(x)$''')
    st.markdown(r'''''')
    st.markdown(r'''Where $P(x)$ is the cumulative distribution function of our target distribution. To sample from $p(x)$, we simply need to generate a uniform random number $z$ and then compute $x = P^{-1}(z)$.''')
    st.markdown(r'''''')
    st.markdown(r'''### Geometric Interpretation and Examples''')
    st.markdown(r'''''')
    st.markdown(r'''This method has a beautiful geometric interpretation. Imagine plotting the CDF of your target distribution. If you drop points uniformly along the y-axis (which represents the uniform distribution $Z$) and then project these points onto the x-axis using the CDF curve, you'll end up with points distributed according to $p(x)$. Areas of steep increase in the CDF correspond to regions of high probability in $p(x)$, while flatter regions correspond to lower probability.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's look at a couple of examples to solidify our understanding. First, consider the exponential distribution, given by $p(x) = \lambda e^{-\lambda x}$. The CDF of this distribution is $P(x) = 1 - e^{-\lambda x}$. Inverting this, we get $x = -\ln(1-z) / \lambda$. So, to generate samples from an exponential distribution, we simply generate uniform random numbers and apply this transformation.''')
    st.markdown(r'''''')
    st.markdown(r'''A more interesting example is the Cauchy distribution, given by:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(x) = \dfrac{1}{\pi\sigma(1 + ((x-\mu)/\sigma)^2)}$''')
    st.markdown(r'''''')
    st.markdown(r'''The Cauchy distribution is particularly useful in many scenarios. It resembles a normal distribution near its peak but has much heavier tails, decaying as $1/x^2$ rather than exponentially. This makes it an excellent choice for a prior distribution when we want to allow for the possibility of extreme values.''')
    st.markdown(r'''''')
    st.markdown(r'''The CDF of the Cauchy distribution is $P(x) = \dfrac{1}{2} + \dfrac{1}{\pi}\arctan((x-\mu)/\sigma)$. Inverting this gives us $x = \mu + \sigma\tan(\pi(z-1/2))$. By generating uniform random numbers and applying this transformation, we can sample from a Cauchy distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''### Advantages and Limitations of Inverse CDF Method''')
    st.markdown(r'''''')
    st.markdown(r'''The inverse CDF method has several advantages. Primarily, it provides exact samples from the target distribution, avoiding the discretization issues we saw with the grid approach. It's also generally efficient, requiring only the generation of a uniform random number and the application of a single function.''')
    st.markdown(r'''''')
    st.markdown(r'''However, it's not without limitations. The main drawback is that it requires an analytic form of the CDF and its inverse, which isn't always available or easily computed. This becomes particularly challenging when dealing with complex distributions like posteriors in Bayesian inference, which often involve products of many terms.''')
    st.markdown(r'''''')
    st.markdown(r'''### Distribution Transformations''')
    st.markdown(r'''''')
    st.markdown(r'''It's worth noting that the inverse CDF method is just one way to transform distributions. There are several other useful transformations that you might encounter in your work. For instance, if $X$ follows a standard normal distribution, then $\exp(X)$ follows a log-normal distribution. If you square a standard normal variable, you get a chi-squared distribution with one degree of freedom. And if you have two gamma-distributed variables, you can combine them to form a beta distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''These transformations aren't just mathematical curiosities. They're often used in practice to generate samples from various distributions. In fact, many of the built-in random number generators in scientific computing libraries use these methods under the hood.''')
    st.markdown(r'''''')
    st.markdown(r'''The inverse CDF method and these related transformations highlight an important principle in probability theory and statistics: distributions are like clay that we can mold and shape. If we understand the relationship between different distributions, we can often find clever ways to generate samples from complex distributions by transforming samples from simpler ones.''')
    st.markdown(r'''''')
    st.markdown(r'''In practice, when you're working on a problem involving Bayesian inference or other statistical techniques, it's always worth asking whether your posterior distribution might have an analytic CDF. If it does, the inverse CDF method can be a powerful and efficient way to generate samples. However, in many real-world problems, especially those involving complex likelihoods or priors, this won't be the case.''')
    st.markdown(r'''''')
    st.markdown(r'''That's why we'll now explore more advanced sampling techniques that can handle even more complex distributions. We'll look at rejection sampling and importance sampling, which can work in cases where the inverse CDF method isn't applicable. These methods will set the stage for our discussion of Markov Chain Monte Carlo techniques later.''')
    st.markdown(r'''''')
    st.markdown(r'''## Rejection Sampling: Sculpting Distributions''')
    st.markdown(r'''''')
    st.markdown(r'''Rejection sampling. This method takes a more brute-force approach to generating samples from complex distributions. To understand rejection sampling, imagine you're a sculptor working with a block of tofu, carving it into the shape of a Buddha. This analogy, while perhaps unusual, perfectly captures the essence of rejection sampling. It's similar to what you might see in a Chinese restaurant, where a carrot is carved into a flower. We start with a simple distribution that we can easily sample from - our block of tofu - and then carefully carve away the parts we don't need, leaving us with samples from our target distribution - our Buddha.''')
    st.markdown(r'''''')
    st.markdown(r'''### The Mechanics of Rejection Sampling''')
    st.markdown(r'''''')
    st.markdown(r'''The core idea of rejection sampling is straightforward. We have a proposal distribution $Q(z)$ that we know how to sample from, and a target distribution $P(z)$ that we want to sample from. The key is that we need to be able to evaluate both $Q(z)$ and $P(z)$, even if we can't directly sample from $P(z)$. We also need to ensure that $Q(z)$ can be scaled up to completely envelop $P(z)$ - think of this as making sure our block of tofu is big enough to contain the entire Buddha we want to carve.''')
    st.markdown(r'''''')
    st.markdown(r'''Here's how it works in practice. First, we draw a sample $z$ from our proposal distribution $Q(z)$. Then, we evaluate the ratio $P(z) / (k * Q(z))$, where $k$ is our scaling factor. This ratio tells us how much of our "tofu" we should keep at this point. If the ratio is 0.5, for example, it means we should keep this sample half the time. To decide whether to keep it, we generate a random number between 0 and 1. If this number is less than our ratio, we keep the sample; otherwise, we discard it.''')
    st.markdown(r'''''')
    st.markdown(r'''It's important to understand the distinction between evaluation and sampling here. When we write down a posterior that's proportional to the likelihood and the prior, we know how to evaluate it. Given any $z$, we can calculate $P(z)$. However, we might not know how to directly sample from $P(z)$. Rejection sampling leverages our ability to evaluate $P(z)$ to generate samples from it indirectly.''')
    st.markdown(r'''''')
    st.markdown(r'''### An Example: Sampling from a Gamma Distribution''')
    st.markdown(r'''''')
    st.markdown(r'''Let's look at a concrete example. Suppose we want to sample from a Gamma distribution. The Gamma distribution can actually be sampled using the inverse CDF method, but let's use it as our target for this example. We'll use a Cauchy distribution as our proposal distribution. The Cauchy distribution is particularly useful here because it has very long tails, making it likely to envelop our target Gamma distribution when properly scaled.''')
    st.markdown(r'''''')
    st.markdown(r'''We can visualize this process. Imagine the Gamma distribution as a purple line and our scaled Cauchy proposal as a blue line that completely envelops it. For each point we sample from the Cauchy distribution, we evaluate the ratio of the Gamma density to our scaled Cauchy density at that point. This ratio determines the probability of keeping the sample.''')
    st.markdown(r'''''')
    st.markdown(r'''### Advantages and Limitations of Rejection Sampling''')
    st.markdown(r'''''')
    st.markdown(r'''Rejection sampling is quite flexible. We can extend it to discrete distributions, sampling from things like Poisson distributions. In principle, we can also apply it to higher-dimensional problems - imagine carving a 2D Buddha from a 2D block of tofu, or a 3D Buddha from a 3D block.''')
    st.markdown(r'''''')
    st.markdown(r'''However, rejection sampling isn't without its limitations. As the dimensionality of our problem increases, the proportion of accepted samples tends to decrease dramatically. This is akin to carving a high-dimensional Buddha - most of our tofu ends up discarded! This "curse of dimensionality" makes rejection sampling impractical for many real-world machine learning problems, which often involve high-dimensional spaces. Even for something as seemingly simple as linear regression, rejection sampling can quickly become insufficient.''')
    st.markdown(r'''''')
    st.markdown(r'''Another challenge arises when our target distribution has very heavy tails. Imagine trying to carve a Buddha with extremely long, thin limbs. If our proposal distribution (our block of tofu) doesn't extend far enough, we'll never capture these tails. For example, if we try to use a Gaussian proposal to sample from a Cauchy target, we'll run into trouble. The Gaussian decays exponentially in the tails, while the Cauchy decays only quadratically (as $1/z^2$). No matter how we scale our Gaussian, it will always fall below the Cauchy in the far tails.''')
    st.markdown(r'''''')
    st.markdown(r'''When choosing our proposal distribution, we need to be careful about the trade-off between coverage and efficiency. We can always scale up our proposal distribution by increasing our factor $k$, but if $k$ is too large, we're using too big a block of tofu for our Buddha. This means we'll end up discarding a lot of samples, which is inefficient. Remember, generating samples from $Q(z)$ takes time, so we want to keep as many samples as possible.''')
    st.markdown(r'''''')
    st.markdown(r'''To summarize, rejection sampling has several advantages. It's very flexible and can work with a wide range of distributions, at least in low-dimensional spaces. However, it also has significant drawbacks. It struggles with very heavy-tailed distributions, where finding a suitable proposal distribution can be challenging. It also becomes increasingly inefficient in higher dimensions. Think about trying to carve a high-dimensional sphere from a hypercube - the volume of wasted space grows exponentially with the number of dimensions.''')
    st.markdown(r'''''')
    st.markdown(r'''## Importance Sampling: Weighted Averages''')
    st.markdown(r'''''')
    st.markdown(r'''Now that we've covered rejection sampling, let's move on to our last simple sampling technique: importance sampling. This method is quite similar to rejection sampling, but with some crucial differences that make it particularly useful in certain scenarios.''')
    st.markdown(r'''''')
    st.markdown(r'''### The Fundamental Problem and Importance Sampling Solution''')
    st.markdown(r'''''')
    st.markdown(r'''Let's start by recalling our fundamental problem. We often want to compute expectations of the form:''')
    st.markdown(r'''''')
    st.markdown(r'''$\mathbb{E}[f] = \int f(z) p(z) dz$''')
    st.markdown(r'''''')
    st.markdown(r'''If we could sample directly from $p(z)$, this would be straightforward. We could approximate the integral as:''')
    st.markdown(r'''''')
    st.markdown(r'''$\mathbb{E}[f] \approx \dfrac{1}{L} \sum f(z^{(l)})$, where $z^{(l)} \sim p(z)$''')
    st.markdown(r'''''')
    st.markdown(r'''However, in many cases, especially when dealing with complex posteriors in machine learning, we can't directly sample from $p(z)$. This is where importance sampling comes in.''')
    st.markdown(r'''''')
    st.markdown(r'''The key idea behind importance sampling is to introduce another distribution, $q(z)$, that we can easily sample from. We then use $q(z)$ to indirectly sample from $p(z)$. Here's how it works mathematically:''')
    st.markdown(r'''''')
    st.markdown(r'''$\mathbb{E}[f] = \int f(z) p(z) dz = \int f(z) \dfrac{p(z)}{q(z)} q(z) dz \approx \dfrac{1}{L} \sum \frac{p(z^{(l)})}{q(z^{(l)})} f(z^{(l)})$, where $z^{(l)} \sim q(z)$''')
    st.markdown(r'''''')
    st.markdown(r'''The term $p(z) / q(z)$ is called the importance weight, often denoted as $\tilde{r}$. It corrects for the fact that we're sampling from $q$ instead of $p$.''')
    st.markdown(r'''''')
    st.markdown(r'''### Handling Unnormalized Distributions''')
    st.markdown(r'''''')
    st.markdown(r'''Now, you might be wondering: what if $p(z)$ isn't normalized? This is often the case in machine learning, where we work with posteriors that are proportional to the likelihood times the prior, omitting the normalization constant (often called the evidence). Fortunately, importance sampling still works in this case.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's say $p(z) = \tilde{p}(z) / Z_p$ and $q(z) = \tilde{q}(z) / Z_q$, where $\tilde{p}$ and $\tilde{q}$ are unnormalized densities. We can write:''')
    st.markdown(r'''''')
    st.markdown(r'''$\mathbb{E}[f] = \dfrac{Z_q}{Z_p} \int f(z) \frac{\tilde{p}(z)}{\tilde{q}(z)} q(z) dz$''')
    st.markdown(r'''''')
    st.markdown(r'''The ratio $Z_q / Z_p$ can be estimated as:''')
    st.markdown(r'''''')
    st.markdown(r'''$\dfrac{Z_p}{Z_q} \approx \dfrac{1}{L} \sum \frac{\tilde{p}(z^{(l)})}{\tilde{q}(z^{(l)})}$, where $z^{(l)} \sim q(z)$''')
    st.markdown(r'''''')
    st.markdown(r'''Putting this all together, we get our final importance sampling estimator:''')
    st.markdown(r'''''')
    st.markdown(r'''$\mathbb{E}[f] \approx \frac{\sum \tilde{r}_l f(z^{(l)})}{\sum \tilde{r}_l}$, where $\tilde{r}_l = \frac{\tilde{p}(z^{(l)})}{\tilde{q}(z^{(l)})}$''')
    st.markdown(r'''''')
    st.markdown(r'''This formulation is particularly nice because we only need to know $\tilde{p}$ and $\tilde{q}$ up to a constant of proportionality. We don't need to know their exact normalizing constants.''')
    st.markdown(r'''''')
    st.markdown(r'''### Intuition and Considerations''')
    st.markdown(r'''''')
    st.markdown(r'''Intuitively, you can think of this as a weighted average. We're drawing samples from $q$, but we're weighting them to reflect how likely they would have been under $p$. If $q$ undersamples a region that $p$ considers important, we correct for this by giving more weight to samples from that region.''')
    st.markdown(r'''''')
    st.markdown(r'''The beauty of this method is that we've completely eliminated the need to sample directly from $p$. We only need to be able to evaluate $p$ (or $\tilde{p}$), which is typically much easier. For instance, if $p$ is a posterior distribution, we can usually evaluate it up to a constant by multiplying the likelihood and the prior.''')
    st.markdown(r'''''')
    st.markdown(r'''However, it's important to note that this isn't magic. The choice of $q$ is crucial. If $q$ is very different from $p$, most of our samples will have very low importance weights, and our estimate will be dominated by a few samples with large weights. This can lead to high variance in our estimates.''')
    st.markdown(r'''''')
    st.markdown(r'''### A Concrete Example: Sampling from the Tail of a Gaussian''')
    st.markdown(r'''''')
    st.markdown(r'''At first glance, importance sampling might seem too good to be true - it appears to solve our sampling problems without much difficulty. However, there are crucial considerations we need to keep in mind.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's explore these considerations through a concrete example. Suppose we have a distribution $p(z)$ that's a standard normal Gaussian, and we're interested in sampling from its tail, specifically from 5 to infinity. Our goal is to calculate the mean of this truncated distribution. This scenario is quite relevant to astronomy. Imagine, for instance, that the brightness of stars follows a Gaussian distribution, but due to the limitations of our survey, we only observe the brightest stars. We might then ask: what's the average brightness of the stars we can see?''')
    st.markdown(r'''''')
    st.markdown(r'''Mathematically, we can express this as:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(z) = \dfrac{N(z|0,1) * 1(z \in [5,\infty))}{\int_5^\infty N(z|0,1) dz}$''')
    st.markdown(r'''''')
    st.markdown(r'''Where $N(z|0,1)$ is the standard normal distribution and $1(z \in [5,\infty))$ is an indicator function that's 1 when $z \geq 5$ and 0 otherwise. Note that this $p(z)$ is not normalized, but that's okay - remember, importance sampling doesn't require normalized distributions.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, what should we choose for our proposal distribution $q(z)$? A naive choice might be to use the full Gaussian, $q(z) = N(0,1)$. We know how to sample from this, and we can easily calculate the importance weights:''')
    st.markdown(r'''''')
    st.markdown(r'''$\tilde{r} = \frac{\tilde{p}(z)}{q(z)} = 1(z \in [5,\infty))$''')
    st.markdown(r'''''')
    st.markdown(r'''This means our weights are either 1 (when $z \geq 5$) or 0 (when $z < 5$). Using the importance sampling formula, we get:''')
    st.markdown(r'''''')
    st.markdown(r'''$E[f] \approx \dfrac{\sum z_l * 1(z_l \in [5,\infty))}{\sum 1(z_l \in [5,\infty))}$''')
    st.markdown(r'''''')
    st.markdown(r'''Here's where we encounter our first caveat. Unlike rejection sampling, importance sampling uses all samples, but some weights might be zero. In this case, any sample less than 5 gets a weight of zero. The problem is that the probability of drawing a sample greater than 5 from a standard normal is tiny - about $3 \times 10^{-7}$. This means that out of every 30 million samples, only one, on average, will contribute to our sum!''')
    st.markdown(r'''''')
    st.markdown(r'''This leads to a highly unstable estimator. We're effectively estimating our average with a very small number of samples, even if we draw millions of samples from $q(z)$.''')
    st.markdown(r'''''')
    st.markdown(r'''A better approach is to use a shifted exponential distribution as our proposal:''')
    st.markdown(r'''''')
    st.markdown(r'''$q(z) = \mathrm{Exp}(1) + 5 = \exp(5 - z), z \in [5,\infty)$''')
    st.markdown(r'''''')
    st.markdown(r'''Now our importance weights become:''')
    st.markdown(r'''''')
    st.markdown(r'''$\tilde{r} = \frac{\tilde{p}(z)}{q(z)} = \dfrac{N(z_l|0,1)}{\exp(5 - z_l)}$''')
    st.markdown(r'''''')
    st.markdown(r'''These weights are always non-zero for $z \geq 5$, meaning all our samples contribute to the estimate. This results in a much more stable estimator.''')
    st.markdown(r'''''')
    st.markdown(r'''We can visualize the impact of different proposal distributions. Imagine we're trying to estimate the mean of our truncated Gaussian. If we use a uniform distribution as our proposal, we'll get high variance in our estimates across different runs. This is because most of our samples are "wasted" - they're drawn from regions where $p(z)$ is very small or zero. In contrast, if we use a proposal that better matches $p(z)$, like our shifted exponential, we get much more stable estimates.''')
    st.markdown(r'''''')
    st.markdown(r'''### Key Principles for Choosing a Proposal Distribution''')
    st.markdown(r'''''')
    st.markdown(r'''This example illustrates some key principles for choosing a good proposal distribution:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Whenever $f(z)p(z) \neq 0$, we want $q(z) > 0$. This ensures we don't miss any important regions of the target distribution.''')
    st.markdown(r'''2. We want $q(z)$ to be as close as possible to $f(z)p(z)$. This minimizes the variance of our importance weights.''')
    st.markdown(r'''3. $q(z)$ should envelop $p(z)$, having heavier tails. This ensures we don't underestimate the tails of our target distribution.''')
    st.markdown(r'''4. $q(z)$ should be easy to sample from and evaluate. After all, if $q(z)$ is as hard to sample from as $p(z)$, we haven't gained anything!''')
    st.markdown(r'''''')
    st.markdown(r'''These rules might seem straightforward, but applying them effectively can be challenging in practice. The choice of proposal distribution can dramatically affect the efficiency and accuracy of our estimates.''')
    st.markdown(r'''''')
    st.markdown(r'''It's worth noting that importance sampling has applications beyond just estimating expectations. It's used in various areas of research, including deconvolution problems. For instance, if you're trying to understand the intrinsic light distribution of celestial objects given noisy observations, you might encounter integrals that are hard to compute exactly but can be approximated using importance sampling.''')
    st.markdown(r'''''')
    st.markdown(r'''### Reflections on Sampling Techniques in Astronomy and Machine Learning''')
    st.markdown(r'''''')
    st.markdown(r'''As we conclude this section on sampling techniques, let's reflect on what we've learned and why it's crucial for understanding machine learning and scientific research in astronomy.''')
    st.markdown(r'''''')
    st.markdown(r'''First and foremost, we've explored why sampling is such an integral part of machine learning. Many of you may have encountered these sampling techniques before, but the key insight is understanding the deep connection between sampling and the core objectives of machine learning. Fundamentally, machine learning aims to do two things: estimate posteriors and generate predictive distributions.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, you might think that in astronomy, we're only interested in fitting models and estimating parameters, not in making predictions. But this is a common misconception. Astronomy is inherently predictive. Even if your primary goal is to estimate cosmological parameters like $\Omega_m$ or $w$, you still need to predict what observations you should see given these parameters. This is how we test theories like $\Lambda$CDM. And these predictions invariably involve complex integrals that require sophisticated sampling techniques.''')
    st.markdown(r'''''')
    st.markdown(r'''It's important to understand that the simple cases we've covered are not representative of real-world research problems. You might think these simple cases are too hard for practical use, but it's actually the opposite. They're simple enough that we can solve the integrals analytically. However, these analytically solvable models often lack the expressiveness needed for real-world phenomena. That's why in research, we turn to methods like Markov Chain Monte Carlo (MCMC) for sampling. The key is to connect the principles we've learned from these simpler cases to the more complex methods used in actual research.''')
    st.markdown(r'''''')
    st.markdown(r'''In this lecture, we covered several fundamental sampling techniques. We learned about the inverse Cumulative Distribution Function (CDF) method, which allows us to sample exactly from a distribution if we can integrate it. For low-dimensional cases where exact integration isn't possible, we explored rejection sampling, which lets us "carve out" the desired distribution from a simpler one. Building on this idea, we then discussed importance sampling, which provides a way to estimate integrals using samples from a different, more convenient distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''We also introduced the crucial concept of effective sample size. While sampling is powerful, it's not just about running your computer for a long time. It's about generating samples efficiently, much like trying to estimate $\pi$ by throwing as few needles as possible while still getting an accurate estimate. This idea of effective sample size will become even more critical when we delve into MCMC methods in future discussions.''')
    st.markdown(r'''''')
    st.markdown(r'''The techniques we've covered – inverse CDF, rejection sampling, and importance sampling – might seem basic, but they form the foundation for more advanced methods. Understanding these fundamentals is crucial for grasping the more complex sampling techniques used in cutting-edge machine learning and astronomical research.''')
    st.markdown(r'''''')
    st.markdown(r'''### Tutorial: Sampling from the Kroupa IMF''')
    st.markdown(r'''''')
    st.markdown(r'''Now that we've covered the theoretical aspects of sampling, let's consider how these techniques can be applied in practical astronomical scenarios. These sampling methods are particularly useful in various astrophysical applications, especially when studying galaxies or working with Integral Field Unit (IFU) data.''')
    st.markdown(r'''''')
    st.markdown(r'''The tutorial is divided into several parts, each focusing on a different aspect of sampling and its applications. Let's walk through what you can expect:''')
    st.markdown(r'''''')
    st.markdown(r'''First, we'll start with the basics of inverse Cumulative Distribution Function (CDF) sampling. You'll learn how to sample from an exponential distribution by performing the analytic integral and then inverting it. This fundamental technique is crucial for many more complex sampling tasks.''')
    st.markdown(r'''''')
    st.markdown(r'''The second part of the tutorial introduces a more realistic scenario: sampling from the Kroupa Initial Mass Function (IMF). This is particularly important for various astrophysical applications, especially when studying galaxies or working with Integral Field Unit (IFU) data. Most astronomical observations don't capture individual stars, especially when observing distant galaxies. Instead, we typically observe the integrated light from all the stars in a galaxy. To build models that accurately represent this integrated light spectrum, we need to understand how to sample the constituents of the galaxy.''')
    st.markdown(r'''''')
    st.markdown(r'''The properties that largely govern any integrated observation of a galaxy are the distributions of stellar age, mass, and metallicity. In this tutorial, we'll focus on the mass distribution, which is described by the Kroupa IMF. This function is more complex than a simple exponential, as it's defined piecewise with different power-law slopes for different mass ranges.''')
    st.markdown(r'''''')
    st.markdown(r'''The Kroupa IMF presents some interesting challenges. While each piece of the function is integrable, determining the constant values for each segment requires solving a system of equations. These constants must satisfy continuity constraints at the segment boundaries (0.08 and 0.5 solar masses) and ensure that the entire distribution integrates to one.''')
    st.markdown(r'''''')
    st.markdown(r'''You'll learn how to handle piecewise functions in the context of inverse CDF sampling. This involves calculating the integral for each piece to determine which part of the [0, 1] interval corresponds to each segment. Once you've identified the appropriate segment, you'll apply the inverse CDF for that specific power law to draw the mass value.''')
    st.markdown(r'''''')
    st.markdown(r'''If you prefer to avoid the complex integrals, the tutorial will also demonstrate how to use rejection sampling to draw from the Kroupa IMF. You'll learn how to choose an appropriate envelope distribution and implement the accept-reject algorithm.''')
    st.markdown(r'''''')
    st.markdown(r'''Additionally, we'll explore how importance sampling can be used to estimate properties of the IMF without directly sampling from it. For example, you'll learn how to calculate the average mass of a star using importance sampling, introducing a proposal distribution to simplify the computation.''')
    st.markdown(r'''''')
    st.markdown(r'''Throughout the tutorial, you'll have the opportunity to compare these different sampling methods, observing their relative efficiencies and accuracies. You'll see how the choice of proposal distribution in importance sampling affects the convergence rate, and how rejection sampling compares to the exact inverse CDF method in terms of computational efficiency.''')
    st.markdown(r'''''')

#if __name__ == '__main__':
show_page()