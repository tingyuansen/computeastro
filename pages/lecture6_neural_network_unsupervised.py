import streamlit as st
from streamlit_app import navigation_menu

def show_page():

    # Page Title
    st.title('Neural Networks - Unsupervised Learning')
    navigation_menu()

    # Embed the external HTML page
    # st.info('''
    # Course Slides
    # ''')
    # slideshare_embed_code = """
    #     <iframe src="https://www.slideshare.net/slideshow/embed_code/key/cxXgDjfbQ7GNHM" 
    #             width="700" height="394" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" 
    #             style="border:0px solid #000; border-width:0px; margin-bottom:0px; max-width: 100%;" allowfullscreen> 
    #     </iframe> 
    # """
    # components.html(slideshare_embed_code, height=412)

    # st.markdown('---')
    
    st.markdown(r'''''')
    st.markdown(r'''Welcome back, everyone. Let's begin by recapping what we've covered in our previous discussions on machine learning in astronomy.''')
    st.markdown(r'''''')
    st.markdown(r'''We've explored both supervised and unsupervised learning in their basic forms. For individual tasks, we introduced some of the most widely used and classical methods: regression and classification for supervised learning, and density estimation and dimension reduction for unsupervised learning.''')
    st.markdown(r'''''')
    st.markdown(r'''Today, we'll continue our exploration of more advanced methods, while still anchoring ourselves in the realm of supervised and unsupervised tasks. We'll focus on how neural networks can be applied to unsupervised tasks. There are two main areas we'll concentrate on:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Dimension reduction: We previously discussed Principal Component Analysis (PCA) as a method for reducing dimensions. We'll revisit this concept and examine the connection between autoencoders and PCA.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Density estimation: We previously talked about using Gaussian Mixture Models (GMMs), which evolved from K-means clustering. In the realm of neural networks, we have an advanced version called Mixed Density Networks, which we'll introduce today.''')
    st.markdown(r'''''')
    st.markdown(r'''These topics will conclude our discussion on the basic applications of neural networks for dimension reduction and density estimation. As with our previous discussions on neural networks, it's important to note that deep learning and neural networks are vast fields, and it's impossible to cover everything in depth. However, we'll introduce you to some concepts you may have heard of, such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and simulation-based inference, and how they relate to autoencoders and mixed density networks.''')
    st.markdown(r'''''')
    st.markdown(r'''## Autoencoders for Dimension Reduction''')
    st.markdown(r'''''')
    st.markdown(r'''We'll start with autoencoders and their use in dimension reduction. But before we dive in, it's crucial to review PCA. Just as understanding the limitations of linear regression led us to neural networks, recognizing the constraints of PCA will help us appreciate the advantages of autoencoders.''')
    st.markdown(r'''''')
    st.markdown(r'''PCA is elegant in its simplicity, requiring only the covariance matrix of the data. By diagonalizing this matrix, we can determine the transformation direction based solely on the data's properties, without needing labels. However, PCA has limitations, which we'll explore in the next few slides.''')
    st.markdown(r'''''')
    st.markdown(r'''The fundamental assumption of PCA is that the transformation from X to Z space is linear, using the eigenvectors of the covariance matrix. We denote this transformation matrix as $B$. One limitation is that PCA assumes the latent representation is uncorrelated, as the eigenvectors of a symmetric matrix are orthogonal.''')
    st.markdown(r'''''')
    st.markdown(r'''To gain a deeper understanding of PCA's limitations, let's revisit its derivation. In our earlier discussions, we explored how many concepts could be derived from maximum likelihood estimation. PCA initially seemed unrelated to this principle, as we focused on maximizing variance. However, it turns out that PCA can indeed be derived from maximum likelihood estimation.''')
    st.markdown(r'''''')
    st.markdown(r'''I didn't cover this derivation earlier because it's more complex and might not have provided immediate insights. But now, let's explore this connection, as it will lead to key insights that inform the design of autoencoders. We'll provide a sketch of the proof to illustrate this relationship.''')
    st.markdown(r'''''')
    st.markdown(r'''### PCA from a Probabilistic Perspective''')
    st.markdown(r'''''')
    st.markdown(r'''In our exploration of Principal Component Analysis (PCA), we implicitly build models based on specific assumptions. Our primary goal is to find a low-dimensional representation $z$ of our high-dimensional data $x$. We assume that $z$ follows a Gaussian distribution, which we can express without loss of generality as:''')
    st.markdown(r'''''')
    st.markdown(r'''$z \sim \mathcal{N}(0, I)$''')
    st.markdown(r'''''')
    st.markdown(r'''where $z \in \mathbb{R}^M$.''')
    st.markdown(r'''''')
    st.markdown(r'''The fundamental objective of PCA is to identify the optimal linear transformation from z back to x that maximizes the likelihood. Specifically, PCA assumes that for any given z drawn from this low-dimensional Gaussian distribution, we can project it back to the high-dimensional space using a matrix W and a vector μ. However, since z is a low-dimensional representation, we acknowledge that it may not capture all the variance in the data. Therefore, we introduce an unknown variance term not captured by the PCA. This can be expressed mathematically as:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(x|z) \sim \mathcal{N}(x|Wz + \mu, \sigma^2I)$''')
    st.markdown(r'''''')
    st.markdown(r'''where $x \in \mathbb{R}^D$.''')
    st.markdown(r'''''')
    st.markdown(r'''These formalisms lead us to the familiar solution we've encountered in PCA. By establishing these two assumptions, we can proceed to maximize the likelihood. The key question is: which likelihood are we maximizing?''')
    st.markdown(r'''''')
    st.markdown(r'''Our objective is to maximize the likelihood of observing the actual data, which exists in the x space. However, our model only specifies the conditional likelihood (how to transform z back to x) and the prior of z. Fortunately, we've assumed both are Gaussian distributions. As we learned in the linear regression lecture, when we integrate over one variable in a product of two Gaussians, the resulting distribution is also Gaussian with an analytically calculable mean and covariance.''')
    st.markdown(r'''''')
    st.markdown(r'''Applying our assumptions and integrating, we find that p(x) can be described by a Gaussian distribution determined by the parameters μ, W, and σ. These are the model parameters we aim to optimize. The marginal distribution is given by:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(x) = \int p(x|z)p(z) dz = \mathcal{N}(x|\mu, C)$''')
    st.markdown(r'''''')
    st.markdown(r'''where $C = WW^T + \sigma^2I$.''')
    st.markdown(r'''''')
    st.markdown(r'''Following the approach used in linear regression, we first formulate the model and then optimize its parameters. Given a set of observed data points x_n, our goal is to optimize the linear transformation and the uncaptured variance by maximizing the likelihood of observing our dataset.''')
    st.markdown(r'''''')
    st.markdown(r'''Assuming independence between data points, the likelihood is the product of individual likelihoods. Taking the logarithm of this product yields the sum of log-likelihoods, a form we're familiar with. The log-likelihood can be expressed as:''')
    st.markdown(r'''''')
    st.markdown(r'''$\ln p(\{x_n\}|\mu, W, \sigma^2) = \sum_{n=1}^N \ln p(x_n|W, \mu, \sigma^2)$''')
    st.markdown(r'''''')
    st.markdown(r'''$= -\dfrac{ND}{2}\ln(2\pi) - \dfrac{N}{2}\ln|C| - \dfrac{1}{2}\sum_{n=1}^N (x_n - \mu)^T C^{-1} (x_n - \mu)$''')
    st.markdown(r'''''')
    st.markdown(r'''This expression results from evaluating the data points based on the functional form of the Gaussian distribution. It includes the normalizing term and the quadratic terms derived from the exponential's logarithm. This formulation demonstrates how maximizing the likelihood leads to the familiar PCA solution.''')
    st.markdown(r'''''')
    st.markdown(r'''Having established the log-likelihood function, the next step is to optimize the parameters $\mu$, $W$, and $\sigma^2$ based on our observed data $x_n$. This optimization process involves some intricate matrix calculations and concepts from linear algebra, such as matrix traces, which are beyond the scope of our current discussion. However, a closed-form solution does exist, as detailed in Tipping and Bishop's 1999 paper. It's important to understand the general approach and results.''')
    st.markdown(r'''''')
    st.markdown(r'''The optimization process involves differentiating the loss function with respect to $\mu$, $W$, and $\sigma^2$ to find the optimal points. While the detailed derivation can be found in the original papers, the optimal solutions yield several interesting insights:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Optimal mean ($\mu$):''')
    st.markdown(r'''   The optimal shift is to move z to the centroid of x, which can be expressed as:''')
    st.markdown(r'''   ''')
    st.markdown(r'''   $\mu = \dfrac{1}{N}\sum_{n=1}^N x_n$''')
    st.markdown(r'''''')
    st.markdown(r'''   This result is intuitively clear: to find a transformation from a Gaussian blob that best captures our data, we first need to shift the blob to the data's centroid.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Optimal transformation (W):''')
    st.markdown(r'''   The optimal transformation matrix W is composed of two parts:''')
    st.markdown(r'''   ''')
    st.markdown(r'''   $W = B\Lambda$''')
    st.markdown(r'''''')
    st.markdown(r'''Here, B consists of the eigenvectors of the data covariance matrix, and Λ is a diagonal matrix of eigenvalues. Specifically:''')
    st.markdown(r'''   ''')
    st.markdown(r'''   $\Lambda = \mathrm{diag}(\lambda_1, ..., \lambda_{m-1}, 0, ..., 0)$''')
    st.markdown(r'''''')
    st.markdown(r'''   This decomposition is significant because our model assumes an isotropic Gaussian for $z$, while PCA typically results in an elongated ellipsoid. The transformation effectively stretches the isotropic Gaussian based on the eigenvalues, shifts it to the centroid, and then rotates it according to the eigenvectors.''')
    st.markdown(r'''''')
    st.markdown(r'''3. Optimal variance ($\sigma^2$):''')
    st.markdown(r'''   The optimal value for $\sigma^2$, representing the variance not captured by the PCA transformation, is given by:''')
    st.markdown(r'''   ''')
    st.markdown(r'''   $\sigma^2 = \dfrac{1}{D-M}\sum_{i=M+1}^D \lambda_i$''')
    st.markdown(r'''''')
    st.markdown(r'''   This result aligns with our understanding of PCA: the uncaptured variance is the sum of the eigenvalues not included in the first $M$ principal components.''')
    st.markdown(r'''''')
    st.markdown(r'''These results are consistent with what we derived earlier using the maximum variance principle in PCA. However, this probabilistic framework provides a more principled approach to understanding why we transform the data based on eigenvectors.''')
    st.markdown(r'''''')
    st.markdown(r'''The key insight is this: if we assume the latent space is an isotropic Gaussian and we want to find the best linear transformation to match our observed data, the optimal solution involves shifting $z$ to the data's centroid, stretching it according to the eigenvalues, and then rotating it based on the eigenvectors.''')
    st.markdown(r'''''')
    st.markdown(r'''This probabilistic perspective demonstrates that PCA is effectively maximizing the likelihood of our observations under specific model assumptions:''')
    st.markdown(r'''''')
    st.markdown(r'''1. The latent space $Z$ is a single isotropic Gaussian.''')
    st.markdown(r'''2. We can transform $Z$ back to $X$ using a linear operation.''')
    st.markdown(r'''''')
    st.markdown(r'''When we maximize the likelihood under these assumptions, we arrive at the familiar PCA solution.''')
    st.markdown(r'''''')
    st.markdown(r'''Understanding PCA from this probabilistic, maximum likelihood viewpoint is crucial because it illuminates the underlying assumptions of PCA. This understanding will serve as a foundation for exploring why we need more advanced techniques like autoencoders. While we will continue to draw inspiration from PCA, recognizing its limitations will help us appreciate the advantages of more flexible models.''')
    st.markdown(r'''''')
    st.markdown(r'''## Autoencoders: Addressing PCA's Limitations''')
    st.markdown(r'''''')
    st.markdown(r'''Building upon our understanding of Principal Component Analysis (PCA), we can now explore its limitations and how autoencoders address these constraints. This progression illustrates the balance between modeling and learning in machine learning techniques. PCA, while powerful, has two main limitations: it assumes a linear transformation between the high-dimensional data space and the low-dimensional latent space, and it assumes the latent representation is a unit Gaussian. These constraints push PCA towards the modeling end of the spectrum, potentially sacrificing learning flexibility. As we discussed in earlier lectures, there's a delicate balance between modeling and learning in machine learning approaches.''')
    st.markdown(r'''''')
    st.markdown(r'''Autoencoders specifically target these limitations of PCA by introducing non-linear transformations. The key components of an autoencoder are an encoder function $c(x)$ that maps the input $x$ to a low-dimensional representation $z$, and a decoder function $f(z)$ that reconstructs the input from this low-dimensional representation. Mathematically, we express this as $z = c(x)$ for the encoder and $\tilde{x} = f(z)$ for the decoder. The goal is to train the autoencoder to encode the input into a low-dimensional representation $z$, such that the input can be reconstructed from $z$ with minimal loss of information.''')
    st.markdown(r'''''')
    st.markdown(r'''Following the maximum likelihood formalism we used in PCA, and assuming an uninformative prior, the loss function for an autoencoder becomes equivalent to the chi-square or Mean Squared Error (MSE) loss. We can express this as:''')
    st.markdown(r'''''')
    st.markdown(r'''$L = \sum_{i=1}^N ||x_i - f(c(x_i))||^2$''')
    st.markdown(r'''''')
    st.markdown(r'''$\mathcal{L} = -\sum_{n=1}^N \ln P(x | c(x)) = \sum_{n=1}^N \|x_n - f(c(x_n))\|^2$''')
    st.markdown(r'''''')
    st.markdown(r'''This loss function measures the reconstruction error between the input and its reconstruction through the autoencoder.''')
    st.markdown(r'''''')
    st.markdown(r'''The key difference between PCA and autoencoders lies in the nature of the encoder and decoder functions. In autoencoders, both $c(x)$ and $f(z)$ are parametrized by highly non-linear neural networks. For instance, they can be implemented as Multi-Layer Perceptrons (MLPs). These networks are trained through backpropagation and Stochastic Gradient Descent (SGD), allowing for more flexible and expressive mappings between the input and latent spaces.''')
    st.markdown(r'''''')
    st.markdown(r'''The effectiveness of an autoencoder relies on the "bottleneck" created by the low-dimensional latent space. This bottleneck forces the network to learn an abstract representation of the data. Without this constraint, the network could simply learn an identity mapping, which would not provide any meaningful compression or representation learning. The dimension of the latent space (the bottleneck) is a hyperparameter that needs to be chosen carefully. A very small bottleneck might lead to significant information loss, while a large bottleneck might not force the network to learn a meaningful compressed representation.''')
    st.markdown(r'''''')
    st.markdown(r'''Unlike PCA, where we have a clear metric (explained variance) to guide the choice of dimensions to retain, the choice of bottleneck size in autoencoders is more heuristic. It depends on the specific task and the trade-off between compression and reconstruction fidelity that is acceptable for the given application. You can plot the loss as a function of the bottleneck size, but this is really a case-by-case basis of how much you are hoping to get a meaningful low-dimensional representation and how much you are willing to sacrifice fidelity.''')
    st.markdown(r'''''')
    st.markdown(r'''## Non-linear Transformations and Representation Learning''')
    st.markdown(r'''''')
    st.markdown(r'''By allowing for non-linear transformations, autoencoders can potentially capture more complex patterns in the data. They can learn to map data that lies on complex manifolds in the high-dimensional space to simpler representations in the low-dimensional latent space. For example, if we have data points that live in a three-dimensional space but really form a two-dimensional manifold, an autoencoder can find a way to encode these three-dimensional data points back to the two-dimensional plane. Of course, we have no way to construct the loss function directly in the latent space, so we decode it back to the three-dimensional space and see how well we are matching the three-dimensional manifold.''')
    st.markdown(r'''''')
    st.markdown(r'''Unlike PCA, which involves a single operation to map $X$ to $Z$ and another to map $Z$ back to $X$, autoencoders employ a recursive function implemented as a neural network. This structure introduces an interesting complexity: there isn't a single layer that can be definitively identified as "the representation." In principle, the output of any layer could be considered a representation of the data. However, as we progress through the network to deeper layers with successively lower dimensionality, we generally expect these layers to learn higher levels of abstraction about the data.''')
    st.markdown(r'''''')
    st.markdown(r'''This is because the most constrained (or "bottlenecked") layer is forced to use a limited degree of freedom to capture global information about the data. In the classical formulation of autoencoders, no assumptions are made about the shape or distribution of the latent space $Z$. This flexibility can be both advantageous and problematic. On one hand, it allows for potentially more expressive representations. On the other hand, if we map $X$ to a $Z$ space that we can't easily characterize, we lose the ability to sample from it, which can be a significant drawback for certain applications.''')
    st.markdown(r'''''')
    st.markdown(r'''This limitation led to the development of the variational autoencoder (VAE), an advanced concept that we won't delve into deeply in this course. The key idea behind VAEs is to encourage the latent space $Z$ to approximate a Gaussian distribution, similar to what we see in PCA. However, unlike PCA, which strictly enforces a Gaussian latent space, VAEs use an additional loss term that encourages the representation in $Z$ to be as close to Gaussian as possible, while still allowing for non-linear mappings between $X$ and $Z$. This approach is beneficial because we know how to sample from a Gaussian distribution, enabling us to generate new samples by sampling from the latent space and then using the decoder to project back into the data space.''')
    st.markdown(r'''''')
    st.markdown(r'''It's important to note that in autoencoders, there's no strict definition of which layer represents the "true" latent representation. Each successive layer in the encoder can be viewed as a representation of the data at different levels of abstraction. Generally, earlier layers capture more local, fine-grained features, while deeper layers with more restricted dimensionality tend to capture global, high-level features of the data. This progression from local to global features as we move through the network layers is similar to what we observed in the Pokemon tutorial, where deeper layers learned higher-level abstractions of the data.''')
    st.markdown(r'''''')
    st.markdown(r'''## Motivations and Applications of Dimensionality Reduction''')
    st.markdown(r'''''')
    st.markdown(r'''The motivation for dimensionality reduction stems from an implicit assumption about the nature of real-world data. We assume that our data doesn't span the entire dimension of the space it inhabits. If it did, it would essentially be white noise. For example, human faces look like faces because adjacent pixels are not independent – a face pixel is likely to be next to another face pixel. This correlation in the data implies that any structure we observe must inherently be lower-dimensional. By forcing our model to learn a low-dimensional representation, we're encouraging it to capture meaningful abstractions of the underlying knowledge.''')
    st.markdown(r'''''')
    st.markdown(r'''This concept of learning compact, meaningful representations is at the heart of many modern machine learning techniques, including language models. For instance, the BERT model, which has been highly influential in natural language processing, employs an encoder-decoder framework similar to autoencoders. The goal is to encode complex, high-dimensional input (like sentences) into a lower-dimensional space that captures high-level abstractions of the knowledge contained in the input. This process of knowledge extraction and representation is fundamental to how these models understand and generate language.''')
    st.markdown(r'''''')
    st.markdown(r'''Beyond knowledge extraction, autoencoders have numerous practical applications, particularly in astronomy. One significant application is denoising. Consider an input $X$ that contains noise, such as white noise in an image. To encode all the white noise, we would need as many dimensions as the image itself, because noise should be uncorrelated. However, autoencoders are designed to force a low-dimensional representation, making it unlikely for the model to waste its limited dimensions capturing noise. Instead, the model will try to ignore the noise as it's not predictive of the underlying data structure.''')
    st.markdown(r'''''')
    st.markdown(r'''Of course, in practice, we don't know the true dimension of the latent space. If we set the latent dimension higher than necessary, say 10 nodes when only 5 are needed, the model might still capture some noise due to the extra degrees of freedom. This is where regularization comes into play. By adding a regularization term to the loss function, typically in the form of $\lambda/2 * \omega^T * \omega$ (where $\omega$ represents the weights), we encourage the model to use neurons sparingly. This regularization, often called weight decay in PyTorch, promotes a sparse representation.''')
    st.markdown(r'''''')
    st.markdown(r'''This concept of pruning unnecessary complexity is related to the idea of "grokking" that we discussed earlier. Even if we don't know the optimal dimension beforehand, we can start with a larger dimension and then gradually prune it through weight decay. This approach allows the model to learn all it can with more freedom initially, and then refine its knowledge representation.''')
    st.markdown(r'''''')
    st.markdown(r'''In some fortunate scenarios, we might have access to ground truth images without noise. In these cases, we can input the noisy images to the encoder but calculate the Mean Squared Error (MSE) loss by comparing the output to the noise-free version. This approach encourages the model to learn how to denoise effectively, as it's penalized for reproducing noise in its output.''')
    st.markdown(r'''''')
    st.markdown(r'''Denoising autoencoders aren't limited to handling white noise. They can be applied to various types of image restoration tasks. For instance, we can add text to an image and then train the autoencoder to remove it. This principle is similar to what's used in creating the "bokeh" effect in video conferencing software, where the background is blurred.''')
    st.markdown(r'''''')
    st.markdown(r'''In astronomy, these techniques have found significant applications. Many astronomical images suffer from similar issues – cosmic rays, saturated light, and other artifacts. A 2020 paper demonstrated the use of an autoencoder-type formalism to reject cosmic rays in images, effectively cleaning up astronomical data. This approach can be extended to cleaning up spectra and other types of astronomical data.''')
    st.markdown(r'''''')
    st.markdown(r'''The utility of autoencoders in this context is immense. Even if we're not abstracting data to learn high-level representations, the ability to denoise, remove artifacts, or clean data is extremely powerful in fields like astronomy where data quality is crucial.''')
    st.markdown(r'''''')
    st.markdown(r'''It's important to note that these denoising techniques don't actually increase the information content of the data. The data itself remains what it is. Rather, these methods help us learn models that are insensitive to noise, which is particularly useful when our training data is noisy. The key insight is that if the noise is truly high-dimensional and random, the model shouldn't be able to learn a function to reproduce it, allowing it to focus on the underlying structure of the data.''')
    st.markdown(r'''''')
    st.markdown(r'''## Adversarial Loss and Generative Adversarial Networks''')
    st.markdown(r'''''')
    st.markdown(r'''As we delve deeper into the world of autoencoders, it's crucial to understand their limitations. One significant constraint lies in the implicit assumptions we make when using the Mean Squared Error (MSE) loss function. The MSE loss, which stems from the principle of maximum likelihood estimation, essentially assigns equal importance to all pixels in the reconstruction process. However, this assumption may not hold true in many real-world scenarios.''')
    st.markdown(r'''''')
    st.markdown(r'''Consider, for instance, astronomical images of galaxies. The pixels containing the galaxy itself are inherently more informative than the background pixels. Similarly, in facial reconstruction tasks, certain facial features are more critical for recognition than others. The MSE loss, in its simplicity, fails to capture these nuances. It treats all pixels with equal weight, regardless of their relative importance to the overall structure or meaning of the image.''')
    st.markdown(r'''''')
    st.markdown(r'''This limitation becomes particularly apparent when we consider the coherence of reconstructed images. The MSE loss evaluates each pixel independently, without considering the relationships between adjacent pixels. As a result, reconstructed images might lack the spatial coherence that's crucial for producing realistic outputs. This issue can be evident in various image generation tasks, where the generated images may lack the natural flow and consistency of real images.''')
    st.markdown(r'''''')
    st.markdown(r'''Recognizing these shortcomings, researchers in the field of machine learning began to explore alternative approaches. One such approach that gained significant traction around 2016 was the concept of "adversarial loss." This idea fundamentally changed how we think about evaluating the quality of reconstructed or generated images.''')
    st.markdown(r'''''')
    st.markdown(r'''The core principle of adversarial loss is to move away from predefined metrics like MSE and instead use another neural network to determine whether a reconstruction is successful. This led to the development of Generative Adversarial Networks (GANs), a framework that has since revolutionized the field of image generation and reconstruction.''')
    st.markdown(r'''''')
    st.markdown(r'''In a GAN setup, we have two competing networks: a generator (which could be our autoencoder) that tries to produce realistic images, and a discriminator that attempts to distinguish between real and generated images. These networks engage in a minimax game, where the generator aims to fool the discriminator, and the discriminator strives to correctly identify real and fake images. As this competition progresses, both networks ideally improve their performance, leading to increasingly realistic generated images.''')
    st.markdown(r'''''')
    st.markdown(r'''The key advantage of this approach is that it doesn't rely on the assumption that all pixels are equally important. The discriminator network can learn to focus on the most critical aspects of the image for determining authenticity. This allows for more nuanced and realistic reconstructions, addressing many of the limitations inherent in the MSE-based approach.''')
    st.markdown(r'''''')
    st.markdown(r'''It's worth noting that while GANs represented a significant advancement when introduced, the field has continued to evolve rapidly. As of our current discussion, there are even more sophisticated approaches that have superseded the original GAN framework. However, understanding GANs is crucial as they represent a pivotal shift in thinking about image generation and reconstruction.''')
    st.markdown(r'''''')
    st.markdown(r'''The development of adversarial loss and GANs illustrates a broader principle in machine learning: the importance of carefully considering our evaluation metrics and loss functions. By moving beyond simple pixel-wise comparisons, we can create models that capture more meaningful and perceptually relevant aspects of the data.''')
    st.markdown(r'''''')
    st.markdown(r'''## Domain Transfer and Multi-modal Learning''')
    st.markdown(r'''''')
    st.markdown(r'''The power of autoencoders extends beyond simple dimensionality reduction, venturing into the realm of domain transfer. The core idea here is that the knowledge captured by an autoencoder resides in the embedding space. By encoding information into a low-dimensional bottleneck, we aim to capture the true essence or knowledge behind the data. This principle forms the foundation for what's known as multi-modal training in machine learning.''')
    st.markdown(r'''''')
    st.markdown(r'''Multi-modal learning refers to the ability to process and relate information from multiple types of input or output. For instance, we might input an image and output a caption, or vice versa. While this could be treated as a straightforward supervised learning task (mapping $X$ to $Y$), such an approach often falls short. The more effective method involves using autoencoders for each modality, abstracting the vast dimensions of knowledge into a low-dimensional understanding within the embedding space.''')
    st.markdown(r'''''')
    st.markdown(r'''This process of compressing information into a compact representation is, in many ways, analogous to our concept of understanding. In the context of machine learning, when we say a machine "knows" something, we often mean it has learned to abstract data into a small-dimensional space effectively. This ability to create and manipulate these abstract representations opens up fascinating possibilities.''')
    st.markdown(r'''''')
    st.markdown(r'''One such possibility is the ability to transfer knowledge between domains. By training two separate autoencoders (each with an encoder and decoder) and then combining the encoder from one domain with the decoder from another, we can create a bridge between these domains. Of course, ensuring that the embeddings align properly is crucial and involves various techniques and tricks. This approach underpins much of the current work in multi-modal machine learning.''')
    st.markdown(r'''''')
    st.markdown(r'''## Applications in Astronomy''')
    st.markdown(r'''''')
    st.markdown(r'''In astronomy, this concept of domain transfer has found significant applications. One example from recent work involves morphing between two closely related but distinct domains: simulated spectra and observed spectra. While these domains are similar, they're not identical due to the inherent limitations of our simulation models. This approach was explored in a study conducted about two years ago, addressing the gap between computer-generated models and real observations.''')
    st.markdown(r'''''')
    st.markdown(r'''The key insight here is that even when working within the same general domain (spectra), there can be meaningful differences between simulated and observed data. When we fit observations (represented by a white line in the study) with classical models, we often miss some real features because our models are imperfect. By using an autoencoder approach to encode high-level abstractions and then decode them back, we can autocorrect for some of these model limitations. This allows us to bridge the gap between simulation and observation more effectively, potentially revealing features that might be missed by traditional modeling approaches.''')
    st.markdown(r'''''')
    st.markdown(r'''Another intriguing application in astronomy involves morphing between galaxy images and spectra. In this case, researchers have attempted to use galaxy-like images as input and decode them into predicted spectra. While it might seem counterintuitive to predict spectra from images, there are practical reasons for this approach. Images are often cheaper and easier to obtain than spectra. By forecasting potential spectral characteristics from images, astronomers can prioritize their observational strategies more efficiently. ''')
    st.markdown(r'''''')
    st.markdown(r'''This particular study employed a Variational Autoencoder (VAE) to create the abstraction from galaxy images and then decode it back into the spectral domain. The motivation behind this approach is largely practical: if we can make reasonable predictions about a galaxy's spectrum based on its image, we can make more informed decisions about which objects to prioritize for detailed spectroscopic observation. This can lead to more efficient use of limited and expensive telescope time.''')
    st.markdown(r'''''')
    st.markdown(r'''The concept of domain transfer with autoencoders extends beyond astronomy. In the field of computer vision, similar techniques are used for tasks like "deepfake" generation. The process involves encoding high-level abstractions of one domain (e.g., one person's face) and decoding them into another domain (e.g., another person's face), all while maintaining a coherent structure.''')
    st.markdown(r'''''')
    st.markdown(r'''These applications highlight how autoencoders can be used to overcome model imperfections and to make predictions across different types of astronomical data. By learning to create meaningful, compact representations of data, autoencoders provide a flexible framework for transferring knowledge between domains, correcting for model imperfections, and generating new insights from existing data.''')
    st.markdown(r'''''')
    st.markdown(r'''## Density Estimation with Neural Networks''')
    st.markdown(r'''''')
    st.markdown(r'''Having concluded our discussion on dimension reduction with neural networks, we now turn our attention to the second major application of unsupervised learning: density estimation. Specifically, we'll explore mixed density networks, which allow us to estimate complex probability densities using neural networks.''')
    st.markdown(r'''''')
    st.markdown(r'''### Limitations of Traditional Neural Networks''')
    st.markdown(r'''''')
    st.markdown(r'''To understand the importance of this approach, let's first consider the limitations of vanilla neural networks, particularly in the context of supervised learning. In traditional multilayer perceptrons (MLPs), we typically assume a one-to-one deterministic function mapping from input $x$ to output $y$. However, this assumption often breaks down in real-world scenarios, especially in scientific applications where we frequently encounter one-to-many mappings.''')
    st.markdown(r'''''')
    st.markdown(r'''A classic example that illustrates this problem is the "Swiss roll" model. In this scenario, we have data points ($x$, $y$) that form a complex, folded structure in space. It becomes immediately apparent that there's no straightforward relation mapping $x$ to $y$. If we attempt to use a standard MLP to find the best $y$ for a given $x$, we'll encounter significant issues because for any given $x$, there could be multiple possible $y$ values. In the Swiss roll case, a single $x$ value might correspond to three distinct $y$ values.''')
    st.markdown(r'''''')
    st.markdown(r'''### One-to-Many Relationships in Astronomy''')
    st.markdown(r'''''')
    st.markdown(r'''This one-to-many relationship is ubiquitous in astronomy and other scientific fields. It's rare to deal with true one-to-one mappings in these domains. Why? The answer lies in the inherent randomness and complexity of the systems we study. Consider, for instance, running a cosmological simulation. Even if we know all the physics - dark matter, dark energy, neutrino mass, etc. - and keep these parameters constant, the resulting universes will be structurally similar but not identical at the pixel level. This variability arises from different random seeds in the simulation.''')
    st.markdown(r'''''')
    st.markdown(r'''Similarly, when characterizing galaxies, we might find that multiple types of galaxies satisfy the same stellar mass criterion. This occurs both due to initial condition variability (like in the universe example) and because our characterization (in this case, using only stellar mass) is incomplete.''')
    st.markdown(r'''''')
    st.markdown(r'''Given the prevalence of these one-to-many mappings in science, it's puzzling when people describe neural networks as merely "curve fitting." In most cases, we're not even attempting to fit a curve in the traditional sense. If we try to use an MLP to find a relation in a Swiss roll-type scenario, minimizing the Mean Squared Error (MSE) loss will result in a solution (represented by a blue line in our example) that poorly describes the true data structure.''')
    st.markdown(r'''''')
    st.markdown(r'''### The Need for Conditional Distributions''')
    st.markdown(r'''''')
    st.markdown(r'''What we truly need in these scenarios is a one-to-many mapping. A more effective way to describe such data is to learn the conditional distribution of y given x, denoted as $p(y|x)$. This approach captures all possible y values for a given x, effectively representing a one-to-many mapping through a probability distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''This concept of conditional distributions aligns well with the Bayesian framework, which is central to this course. In Bayesian analysis, we often work with likelihoods, which are themselves conditional distributions. For example, we might fix certain cosmological parameters but still observe different cosmic web structures given the same underlying physics.''')
    st.markdown(r'''''')
    st.markdown(r'''Conditional distributions also offer computational advantages over joint distributions. Describing the joint distribution $p(x,y)$ for a complex structure like the Swiss roll would be extremely challenging. However, if we focus on $p(y|x)$, the problem becomes more tractable. By fixing $x$, we might find that $y$ follows a trimodal distribution, which could potentially be well-approximated by a Gaussian mixture model.''')
    st.markdown(r'''''')
    st.markdown(r'''## Mixed Density Networks: A Solution for Complex Conditional Distributions''')
    st.markdown(r'''''')
    st.markdown(r'''Building upon our understanding of Gaussian Mixture Models (GMMs) and their limitations, we now explore a more sophisticated approach: Mixed Density Networks. This method addresses the challenge of modeling complex conditional distributions, which is crucial for many scientific applications, including those in astronomy.''')
    st.markdown(r'''''')
    st.markdown(r'''### Limitations of Standard GMMs''')
    st.markdown(r'''''')
    st.markdown(r'''Recall that GMMs are effective at describing multimodal distributions, even when they're not as complex as the Swiss roll example. However, a standard GMM describes a joint distribution $p(y)$, expressing the probability of $y$ across all dimensions of the data using a weighted sum of Gaussians. While this is useful, it doesn't quite meet our needs when we want to model a conditional distribution $p(y|x)$.''')
    st.markdown(r'''''')
    st.markdown(r'''The limitation of using a joint distribution becomes apparent when we consider scenarios where the relationship between $x$ and $y$ is complex and varies across the input space. A single, static GMM would struggle to capture these nuanced, input-dependent relationships. We need a more flexible approach that can adapt the distribution parameters based on the input.''')
    st.markdown(r'''''')
    st.markdown(r'''### The Mixed Density Network Approach''')
    st.markdown(r'''''')
    st.markdown(r'''This is where the concept of Mixed Density Networks comes into play. The key idea is to create a model that can describe $p(y|x)$ as a GMM, but with the crucial difference that the parameters of this GMM can vary with $x$. Mathematically, we express this as:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(y|x) = \Sigma w_k(x) * N(y | \mu_k(x), \Sigma_k(x))$''')
    st.markdown(r'''''')
    st.markdown(r'''In this formulation, for any given $x$, we still have a GMM, but the means ($\mu_k$), covariances ($\Sigma_k$), and weights ($w_k$) of the Gaussian components are all functions of $x$. This approach is based on a reasonable assumption: for $x$ values that are close to each other, the conditional distributions of $y$ will be similar, but not identical. The means, weights, and covariances might shift slightly as $x$ changes.''')
    st.markdown(r'''''')
    st.markdown(r'''The challenge, of course, is how to model these complex relationships between $x$ and the GMM parameters. This is where neural networks come into play. Instead of trying to write down explicit models for $\mu(x)$, $\sigma(x)$, or $w(x)$, we use neural networks to learn these functions. ''')
    st.markdown(r'''''')
    st.markdown(r'''The use of neural networks to model these parameter functions is crucial because it allows for a smooth, continuous variation of the GMM parameters as a function of $x$. This is a key advantage over a static GMM or a more rigid parameterization. As $x$ changes, the neural network ensures that the GMM parameters change in a smooth, continuous manner, reflecting the underlying structure of the data.''')
    st.markdown(r'''''')
    st.markdown(r'''Conceptually, the process works as follows: For each input $x$, we pass it through a neural network. The output of this network is a set of values that we interpret as the parameters of a GMM (means, covariances, and weights). This GMM then describes the distribution of $y$ given that particular $x$. Because we're using a neural network, we ensure that these parameters change smoothly as a function of $x$, providing a flexible yet continuous mapping from inputs to conditional distributions.''')
    st.markdown(r'''''')
    st.markdown(r'''This smooth variation is particularly important in scientific applications. In astronomy, for instance, we might expect that slight changes in input parameters (like stellar mass or galaxy age) would lead to gradual, continuous changes in the distribution of observable properties. The neural network component of the Mixed Density Network captures this behavior naturally.''')
    st.markdown(r'''''')
    st.markdown(r'''### Optimizing Mixed Density Networks''')
    st.markdown(r'''''')
    st.markdown(r'''The next question is how to optimize such a model. Fortunately, the approach is straightforward and aligns well with principles we've discussed before: maximum likelihood estimation. Our model defines a likelihood function, and our goal is to maximize this likelihood given our observed data.''')
    st.markdown(r'''''')
    st.markdown(r'''Mathematically, if $\theta$ represents the parameters of our neural network, and we have a set of observations $\{(x,y)_n\}$, our optimization objective is:''')
    st.markdown(r'''''')
    st.markdown(r'''$\mathrm{argmax}_\theta \sum \log p(y_n | x_n) = \mathrm{argmax}_\theta \sum \log [ \sum w_k(x_n) * N(y_n | \mu_k(x_n), \sigma^2_k(x_n)) ]$''')
    st.markdown(r'''''')
    st.markdown(r'''This formulation might look complex, but it's actually quite manageable in practice, especially with modern deep learning frameworks. Each term in this expression can be easily implemented in a framework like PyTorch. The neural network part produces the GMM parameters, which are then used to evaluate the Gaussian probabilities. Thanks to automatic differentiation, we can perform backpropagation through this entire process to update the network parameters and learn the optimal mapping from $x$ to the parameters of $p(y|x)$.''')
    st.markdown(r'''''')
    st.markdown(r'''By combining the flexibility of neural networks with the probabilistic interpretation of GMMs, Mixed Density Networks provide a powerful tool for modeling complex conditional distributions. This approach is particularly valuable in scientific domains like astronomy, where we often need to capture intricate, one-to-many relationships in our data while maintaining a probabilistic framework that aligns with Bayesian methodologies. The ability to model smooth, input-dependent variations in the distribution parameters makes Mixed Density Networks especially suited for capturing the nuanced relationships often found in astronomical data.''')
    st.markdown(r'''''')
    st.markdown(r'''## Practical Considerations in Implementing Mixed Density Networks''')
    st.markdown(r'''''')
    st.markdown(r'''While the concept of Mixed Density Networks (MDNs) is theoretically sound, the practical implementation and optimization of these models require careful consideration. Despite the fact that the objective is, in principle, always maximizing the likelihood, the challenge lies in effectively finding the minimum of the loss function.''')
    st.markdown(r'''''')
    st.markdown(r'''For MDNs, there are several key tricks that ensure stability and robustness in the optimization process:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Constraining the mixture weights: Part of the neural network's output represents the weights of the Gaussian mixture components. However, these weights must satisfy specific constraints: they need to sum to one and be non-negative. To ensure this, we apply a softmax function to this part of the network's output. The softmax function, which you may recall from logistic regression, takes the exponential of individual nodes and then divides by the sum of these exponentials. This operation guarantees that the outputs are positive and sum to exactly one, satisfying our constraints while remaining differentiable.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Ensuring positive variances: We also need to ensure that the variances ($\sigma_k$) output by the network are positive. This can be achieved by applying an exponential function or a softplus function to the relevant outputs.''')
    st.markdown(r'''''')
    st.markdown(r'''3. Numerical stability: The likelihood function involves the product of many probabilities, each containing a summation. This can lead to numerical instability, especially when dealing with very small or very large numbers. To mitigate this, we work with the logarithm of the likelihood. This transformation not only ensures numerical stability but also simplifies the computations by converting products to sums.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's consider a practical example to illustrate the power of MDNs. Imagine a dataset with input $x$ and output $y$, where there's a complex, one-to-many relationship between them. If we were to fit a standard Multilayer Perceptron (MLP) to this data, minimizing the mean squared error would result in a simple average trend - perhaps a straight line at $y=0$. This is because for any given $x$, there are multiple possible $y$ values, and the MLP can only predict a single value.''')
    st.markdown(r'''''')
    st.markdown(r'''In contrast, an MDN can capture the full distribution of $y$ for each $x$. In the example shown, the MDN learns two different Gaussian components. The means and standard deviations of these Gaussians (represented by solid and dashed lines respectively) vary smoothly as functions of $x$. This allows the MDN to accurately model the complex, multi-modal distribution of $y$ conditional on $x$.''')
    st.markdown(r'''''')
    st.markdown(r'''### Application in Stellar Parameter Estimation''')
    st.markdown(r'''''')
    st.markdown(r'''The application of MDNs extends to various fields, including astronomy. For instance, in a recent study, researchers applied this technique to learn the distribution of stellar parameters given photometric inputs. This is a classic example of a one-to-many mapping in astronomy: the spectral energy distribution (SED) does not fully constrain the stellar parameters, as factors like extinction introduce additional variability.''')
    st.markdown(r'''''')
    st.markdown(r'''By using an MDN, researchers were able to efficiently learn the posterior distribution of stellar parameters based on photometric inputs. This probabilistic mapping acknowledges the inherent uncertainty in the problem - the same photometric measurements could correspond to different sets of stellar parameters due to degeneracies and unmodeled effects.''')
    st.markdown(r'''''')
    st.markdown(r'''This application demonstrates the power of MDNs in astronomical research. They allow us to move beyond point estimates and capture the full probability distribution of parameters of interest, accounting for the complex, often multi-modal relationships present in astronomical data. As we continue to face challenges involving one-to-many mappings and inherent uncertainties, techniques like MDNs will play an increasingly important role in extracting meaningful information from our observations.''')
    st.markdown(r'''''')
    st.markdown(r'''## Neural Density Estimators in Modern Astronomy''')
    st.markdown(r'''''')
    st.markdown(r'''As we conclude our discussion on Mixed Density Networks (MDNs), it's important to place them in the broader context of neural density estimators and their applications in modern astronomy. MDNs represent one of the most basic variants in a larger class of techniques known as neural density estimators. This term encompasses a wide range of methods that use neural networks to estimate probability density distribution functions, whether they're joint, marginal, or conditional distributions.''')
    st.markdown(r'''''')
    st.markdown(r'''### Simulation-Based Inference in Astrostatistics''')
    st.markdown(r'''''')
    st.markdown(r'''The significance of neural density estimators extends far beyond simple curve fitting. In fact, in many scientific applications, we're not fitting curves at all. Instead, these techniques allow us to capture the full probabilistic relationship between variables, providing crucial information about the uncertainty of our inferences. This capability for uncertainty quantification (UQ) is critical in the intersection of artificial intelligence and scientific research.''')
    st.markdown(r'''''')
    st.markdown(r'''In recent years, neural density estimators have become increasingly standard in modern astrostatistics, often employed in what is known as simulation-based inference (SBI) or, somewhat misleadingly, likelihood-free inference (LFI). The term "likelihood-free" is a misnomer because these methods are still fundamentally based on Bayesian statistics and do involve likelihoods. What distinguishes them is that they don't require an explicit, analytic form of the likelihood function. Instead, they use neural networks to learn and represent complex likelihoods directly from data.''')
    st.markdown(r'''''')
    st.markdown(r'''The importance of these methods in astronomy becomes clear when we consider complex, non-linear physical systems. In many cases, as physics becomes more intricate, it becomes increasingly difficult, if not impossible, to write down an analytic description of the phenomenon, let alone its likelihood function. However, we can often still simulate these systems. For instance, given a set of cosmological parameters, we can run simulations to generate realizations of the cosmic web. By running many such simulations with different parameters, we collect a dataset of input-output pairs that can be used to train our neural density estimators.''')
    st.markdown(r'''''')
    st.markdown(r'''What makes neural density estimators particularly powerful is their ability to handle highly non-trivial posterior distributions, likelihoods, or joint distributions. They provide a way to build neural network surrogates that can describe complex likelihoods $p(y|x)$ or directly model posteriors $p(x|y)$. This capability opens up new frontiers in astrostatistics, allowing us to connect sophisticated simulations to observations within a proper statistical framework.''')
    st.markdown(r'''''')
    st.markdown(r'''Consider a concrete example in cosmology. We might have a set of cosmological parameters as our input $x$, and complex observations (like weak lensing maps or galaxy distributions) as our output $y$. Traditional methods might struggle to define an analytic likelihood relating these parameters to observations. However, with neural density estimators, we can run numerous simulations, each generating a point in this high-dimensional $x$-$y$ space. By training a neural network on this simulated data, we can learn to represent the distribution $p(y|x)$ or $p(x|y)$.''')
    st.markdown(r'''''')
    st.markdown(r'''Then, when we make a real observation $y^*$, we can use our trained model to infer the posterior distribution of cosmological parameters $x$ that could have given rise to this observation. This approach provides not just a point estimate, but a full probability distribution, capturing the uncertainties and potential degeneracies in our inference.''')
    st.markdown(r'''''')
    st.markdown(r'''The applications of these techniques extend beyond cosmology. Any field where we can simulate complex systems but struggle to write down explicit likelihoods can benefit from this approach. This includes areas like galaxy formation, stellar evolution, and planetary science.''')
    st.markdown(r'''''')
    st.markdown(r'''## Summary ''')
    st.markdown(r'''''')
    st.markdown(r'''To summarize what we've learned:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Neural networks for dimension reduction (Autoencoders): These allow us to extract low-dimensional embeddings of complex data, based on the principle that most knowledge lives in a low-dimensional manifold. This has applications in denoising and data compression, among others.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Neural networks for density estimation (Mixed Density Networks and beyond): These enable us to model complex probabilistic relationships between variables, providing a framework for uncertainty quantification and simulation-based inference.''')
    st.markdown(r'''''')
    st.markdown(r'''3. Connections to classical methods: We've seen how autoencoders relate to Principal Component Analysis, and how Mixed Density Networks extend Gaussian Mixture Models.''')
    st.markdown(r'''''')
    st.markdown(r'''4. Advanced concepts: We've touched on ideas like Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and the broader field of Simulation-Based Inference (SBI).''')
    st.markdown(r'''''')
    st.markdown(r'''### Tutorial: Practical Implementation of Mixed Density Networks''')
    st.markdown(r'''''')
    st.markdown(r'''The practical implementation and applications of Mixed Density Networks (MDNs), particularly in astronomical contexts, provide valuable insights. Despite the complex theory underlying MDNs, their implementation can be surprisingly straightforward. The core of the model can be written in just a few lines of code, leveraging the power of modern deep learning frameworks. This simplicity belies the model's flexibility in capturing complex, multi-modal distributions, as demonstrated in examples where the model learns to represent bimodal distributions that vary with the input.''')
    st.markdown(r'''''')
    st.markdown(r'''One of the key strengths of MDNs is their adaptive complexity. Even if you start with more Gaussian components than necessary, the model can adapt by reducing the weights of unnecessary components to near-zero. This feature allows the model to automatically adjust its complexity to match the data, avoiding overfitting while still capturing intricate patterns. Furthermore, once trained, an MDN allows for easy sampling from the learned conditional distribution, providing a powerful tool for generating plausible outcomes given specific inputs.''')
    st.markdown(r'''''')
    st.markdown(r'''### Tutorial: Modeling Lithium Abundances in Stars''')
    st.markdown(r'''''')
    st.markdown(r'''To illustrate the practical utility of MDNs in astronomy, let's consider the case study of modeling lithium abundances in stars. This problem involves a complex relationship between multiple variables: lithium abundance (the quantity we're trying to predict or understand), stellar age, and stellar temperature. The relationship between these variables is intricate due to the underlying physics. Lithium, being a fragile element, can be destroyed in stellar interiors. The rate of this destruction depends on the depth of the star's convective zone (which is related to temperature) and the age of the star. Cooler stars tend to destroy lithium more quickly because they have deeper convective zones, while older stars have had more time to destroy their lithium.''')
    st.markdown(r'''''')
    st.markdown(r'''MDNs are particularly well-suited for this problem for several reasons. Firstly, the relationship between age, temperature, and lithium abundance is not deterministic. For any given age and temperature, there's a distribution of possible lithium abundances. MDNs can capture this probabilistic nature effectively. Secondly, the distribution of lithium abundances might be multi-modal under certain conditions, which MDNs can represent accurately. Thirdly, MDNs provide not just a point estimate, but a full distribution, allowing astronomers to quantify the uncertainty in their predictions - a crucial aspect of scientific interpretation.''')
    st.markdown(r'''''')
    st.markdown(r'''Moreover, the learned model can be used for inverse problems. For instance, given an observed lithium abundance and temperature, one could infer the probability distribution of the star's age. This capability is particularly valuable in astronomy, where we often need to infer underlying physical parameters from observable quantities. MDNs can also capture complex, non-linear trends in the data. In this case, how the lithium abundance changes with temperature differs for stars of different ages, and MDNs can represent these intricate relationships.''')
    st.markdown(r'''''')
    st.markdown(r'''The lithium abundance problem demonstrates how MDNs can be applied to real astronomical data to model complex physical processes. By learning the full conditional distribution $p(\mathrm{lithium} | \mathrm{age}, \mathrm{temperature})$, the model provides a rich representation of the underlying physics, allowing for both forward prediction and inverse inference. This approach can be extended to other astronomical problems where we need to model complex, probabilistic relationships between observable quantities and underlying physical parameters.''')
    st.markdown(r'''''')
    st.markdown(r'''This lithium abundance case is essentially a one-dimensional version of many statistical problems in astronomy. The principles demonstrated here can be scaled up to higher-dimensional problems in cosmology and other areas of astrophysics. For instance, in cosmology, we might use similar techniques to model the relationship between cosmological parameters and observable features in the cosmic microwave background or large-scale structure of the universe.''')
    st.markdown(r'''''')

#if __name__ == '__main__':
show_page()