import streamlit as st
from menu import navigation_menu

def show_page():
    st.set_page_config(page_title="Comp Astro",
                       page_icon="./image/tutor_favicon.png", layout="wide")

    # Page Title
    st.title('Gaussian Process - Classification')
    navigation_menu()

    # Embed the external HTML page
    # st.info('''
    # Course Slides
    # ''')
    # slideshare_embed_code = """
    #     <iframe src="https://www.slideshare.net/slideshow/embed_code/key/4pR4NsnsjoGJbj" 
    #             width="700" height="394" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" 
    #             style="border:0px solid #000; border-width:0px; margin-bottom:0px; max-width: 100%;" allowfullscreen> 
    #     </iframe> 
    # """
    # components.html(slideshare_embed_code, height=412)

    # st.markdown('---')

    st.markdown(r'''''')
    st.markdown(r'''Welcome back. Let's recap what we covered last time. We discussed the need for Gaussian processes as an extension of linear regression. Linear regression requires defining features, which can be challenging and quite rigid. This is why we consider using neural networks. However, neural networks have the shortcoming of being difficult to make Bayesian, meaning we can find the best neural network, but it's hard to quantify the uncertainty of the networks.''')
    st.markdown(r'''''')
    st.markdown(r'''So we returned to discussing Gaussian processes, which are, in principle, one of the nicest things in machine learning because you get all the analytic solutions, including the Bayesian version. The key idea of Gaussian processes hinges on using the kernel trick. Instead of defining features, we define what we call a kernel. If you have a kernel function that satisfies being symmetric and semi-positive definite, you can guarantee that there exists a feature somewhere corresponding to the kernel solution.''')
    st.markdown(r'''''')
    st.markdown(r'''We also discussed how some kernels correspond to infinite-dimensional features. For example, the RBF kernel corresponds to infinite-dimensional features, making the function extremely flexible. Gaussian processes are really nice in all ways, but with one catch: like kernel density estimation, your data become your models. Each time you do training or inference, you need to go through all the data. For Gaussian process regression (GPR), this translates into inverting an $n$ by $n$ matrix, where $n$ is the number of training data points. This isn't very practical for many cases in modern machine learning, especially with large datasets.''')
    st.markdown(r'''''')
    st.markdown(r'''Despite their limitations, Gaussian processes are incredibly valuable in astronomy, particularly because we often work with limited training data and low-dimensional observations. Common applications include analyzing time series or examining photometric data, both of which typically involve a small number of features and training data points. In today's lecture, we'll explore star-galaxy separation, which is an excellent example of how Gaussian process classification can be used to distinguish between different types of stars. You'll see that all of this can be accomplished using the methods we'll cover today.''')
    st.markdown(r'''''')
    st.markdown(r'''Gaussian process classification is arguably the best method for many astronomical classification tasks, such as finding metal-poor stars from sky survey data. It's flexible and provides uncertainty estimates. Once you understand Gaussian process classification, it's just a few lines of code to implement one of the best classification methods available.''')
    st.markdown(r'''''')
    st.markdown(r'''However, I should mention that this is probably one of the more technical lectures because teaching Gaussian process classification involves a lot of derivations. It's mostly hardcore derivations, but I'll explain why we need to do them.''')
    st.markdown(r'''''')
    st.markdown(r'''## Revisiting Logistic Regression''')
    st.markdown(r'''''')
    st.markdown(r'''Before we dive into Gaussian processes, let's revisit the logistic regression we covered earlier. Logistic regression is the linear version for classification, and just as Gaussian process regression extends linear regression, Gaussian process classification extends logistic regression. Previously, we didn't discuss the Bayesian version of logistic regression; we only focused on finding the best boundary between classes. Today, we'll characterize the uncertainty of the boundary and explain why that's important.''')
    st.markdown(r'''''')
    st.markdown(r'''We skipped this aspect initially because it requires learning about Laplace approximation, which is quite technical. But once you understand these concepts, along with the linear algebra and linear regression we've covered, applying the kernel trick to create a Gaussian process becomes straightforward.''')
    st.markdown(r'''''')
    st.markdown(r'''So, we'll start with the Bayesian version of logistic regression, then move on to Gaussian process classification (GPC), and finally discuss how to use Laplace approximation to derive the formula for GPC.''')
    st.markdown(r'''''')
    st.markdown(r'''## Bayesian Inference and Model Uncertainty''')
    st.markdown(r'''''')
    st.markdown(r'''The main idea of what we call liberation here is to admit that the model itself is uncertain. Given some training data D and new data X, we want to infer the output Y. This summarizes supervised machine learning: we have data with known ground truth, and we want to infer the outcome for new data. The model is always implicit in machine learning; we need to build a model to make such inferences. The model is parameterized with some parameters, $W$.''')
    st.markdown(r'''''')
    st.markdown(r'''The idea of Bayesian inference is to admit that given finite data, the inference on the parameters is never perfect. You have some posterior of the models. Just like fitting a line, you never have one perfect line; you should have many lines that would fit the data. Because the model itself is uncertain, when making inferences, we need to integrate over the uncertainty of the models.''')
    st.markdown(r'''''')
    st.markdown(r'''Without variation, we're only looking at what we call the MAP (Maximum A Posteriori) of the posterior – the most likely model that describes the data, but not the distribution of models. In linear regression, it's nice because both the posterior and the likelihood are Gaussian. We've shown that if you have two Gaussians and you're marginalizing over one of the parameters, the result is also a Gaussian, and we have an analytic formula we can apply.''')
    st.markdown(r'''''')
    st.markdown(r'''## Challenges in Bayesian Logistic Regression''')
    st.markdown(r'''''')
    st.markdown(r'''Now, what's the problem with applying Bayesian methods to logistic regression? First, let's write down the likelihood. We've talked about why we need some tapering function because we don't want the models to overshoot. You shouldn't treat classification just as regression because then you might overshoot the hyperplane and not get the best model. You need a function that will find the boundary and beyond that is one or zero.''')
    st.markdown(r'''''')
    st.markdown(r'''The likelihood is based on the classes – what's the likelihood that we'll see the classes of the training data? This can be succinctly written as what we call the cross-entropy. Hopefully, this all comes as second nature by now, as we've repeated it quite a few times. The likelihood itself is just the cross-entropy, but $y_n$ is written as sigma of the linear function of the features. We're saying that in the feature space, we can find a linear boundary characterized by $w$, where $w$ determines where you put the hyperplane, and that would give us the best model.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's review some key equations for logistic regression:''')
    st.markdown(r'''''')
    st.markdown(r'''1. The model: $y(x) = \sigma(w^T \phi(x))$, where $\sigma$ is the sigmoid function.''')
    st.markdown(r'''2. Likelihood (cross-entropy): $p(t | w, X) = \prod^N_{n=1} y_n^{t_n} (1 - y_n)^{(1-t_n)}$''')
    st.markdown(r'''3. Prior: $p(w) \equiv \mathcal{N}(w | m_0, S_0)$''')
    st.markdown(r'''4. Posterior: $p(w | t, X) \propto \mathcal{N}(m_0, S_0) \cdot \prod^N_{n=1} y_n^{t_n} (1 - y_n)^{t_n}$''')
    st.markdown(r'''''')
    st.markdown(r'''The likelihood, of course, can assume any prior. Let's say I assume a very simple prior on the models, but nonetheless, you know that your posterior is proportional to the likelihood times the prior. Your likelihood is not a Gaussian, so therefore, you get something that is quite complex. ''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's delve deeper into the challenges of Bayesian logistic regression and introduce a powerful technique called Laplace approximation. Remember, our goal is to compute the predictive distribution, which is given by:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(y^* = 1 | x^*, \mathcal{D}) = \int p(y^* = 1 | x^*, w) p(w | \mathcal{D}) dw$''')
    st.markdown(r'''''')
    st.markdown(r'''Here, $p(y^* = 1 | x^*, w)$ is the sigmoid function $\sigma(w^T \phi(x^*))$, and $p(w | \mathcal{D})$ is our posterior, which is non-Gaussian. It's proportional to $\mathcal{N}(m_0, S_0) \cdot \prod^N_{n=1} y_n^{t_n} (1 - y_n)^{t_n}$.''')
    st.markdown(r'''''')
    st.markdown(r'''When we try to compute this integral, we run into problems. The likelihood is a sigmoid function, and the posterior is even more complex - it's a Gaussian times some cross-entropy terms. This makes the integral very difficult to solve analytically.''')
    st.markdown(r'''''')
    st.markdown(r'''In our first lecture, we simplified this by just finding the Maximum A Posteriori (MAP) estimate - the most likely $w$ that maximizes this term. But today, we're going to tackle this problem head-on and try to account for the uncertainty in $w$.''')
    st.markdown(r'''''')
    st.markdown(r'''## Introduction to Laplace Approximation''')
    st.markdown(r'''''')
    st.markdown(r'''So, how do we approach this? The key idea is to approximate our non-Gaussian posterior with a Gaussian distribution. This is where Laplace approximation comes in.''')
    st.markdown(r'''''')
    st.markdown(r'''Laplace approximation is a technique for finding a Gaussian distribution $q(z)$ that best matches our complex distribution $p(z)$. The "best" here is somewhat ad hoc, but the idea is to match both the mode (the peak) of the distribution and the curvature at that peak.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's formulate this more carefully. Say we have a distribution $p(z)$ that is unnormalized. We only know $f(z)$, meaning we don't know the normalization constant. This is common for posteriors - remember in Bayes' theorem, we often ignore the evidence term as it's just a normalization constant.''')
    st.markdown(r'''''')
    st.markdown(r'''The first step in Laplace approximation is to find the mode of the distribution. This is equivalent to finding where the derivative of $f(z)$ is zero. If our distribution is convex, there will be only one such point.''')
    st.markdown(r'''''')
    st.markdown(r'''Once we've found this maximum point, let's call it $z_0$, we perform a Taylor expansion of $\log f(z)$ around $z_0$. Because $z_0$ is a maximum, the first derivative term in the Taylor expansion will be zero. So we get:''')
    st.markdown(r'''''')
    st.markdown(r'''$\ln f(z) \approx \ln f(z_0) - \dfrac{1}{2}A(z - z_0)^2$''')
    st.markdown(r'''''')
    st.markdown(r'''where $A = -d^2/dz^2 \ln f(z)$ at $z = z_0$. This $A$ term represents the curvature of the log distribution at its peak.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, why is this so important? If we truncate our Taylor expansion at the second order and exponentiate both sides, we get:''')
    st.markdown(r'''''')
    st.markdown(r'''$f(z) \approx f(z_0) \exp\{-\dfrac{A}{2}(z - z_0)^2\}$''')
    st.markdown(r'''''')
    st.markdown(r'''This has the form of a Gaussian distribution! We've managed to approximate our complex distribution with a Gaussian centered at $z_0$ with precision $A$.''')
    st.markdown(r'''''')
    st.markdown(r'''To normalize this distribution, we can use the fact that we know the form of a normalized Gaussian. Our approximating distribution $q(z)$ is then:''')
    st.markdown(r'''''')
    st.markdown(r'''$q(z) = \sqrt{\dfrac{A}{2\pi}} \exp\{-\dfrac{A}{2}(z - z_0)^2\}$''')
    st.markdown(r'''''')
    st.markdown(r'''This is the essence of Laplace approximation. We've taken our complex, non-Gaussian distribution and approximated it with a Gaussian that matches its mode and curvature at that mode.''')
    st.markdown(r'''''')
    st.markdown(r'''In the context of our Bayesian logistic regression problem, this allows us to approximate our complex posterior $p(w | \mathcal{D})$ with a Gaussian distribution. This Gaussian approximation then makes our predictive distribution integral much more tractable.''')
    st.markdown(r'''''')
    st.markdown(r'''## Advantages and Limitations of Laplace Approximation''')
    st.markdown(r'''''')
    st.markdown(r'''Now that we've covered the basics of Laplace approximation, let's discuss why this technique is so crucial, not just for Bayesian logistic regression, but also for Gaussian process classification, which we'll explore later. The Laplace approximation allows us to handle the non-Gaussian nature of classification problems within the elegant framework of Gaussian processes.''')
    st.markdown(r'''''')
    st.markdown(r'''To recap, if we have an unnormalized distribution $f(z)$, which in our case is the posterior (because we're trying to normalize our posterior), we can find a Gaussian approximation by following two steps. First, we find the mode of the distribution, and then we calculate the second derivative at that mode. This approach, while powerful, does have some pros and cons that we should consider.''')
    st.markdown(r'''''')
    st.markdown(r'''One of the major advantages of Laplace approximation is that we don't need to know the normalization constant for $p(z)$. Even if $p(z)$ is not normalized, we can still derive a normalized $q(z)$ because we assume $q(z)$ is Gaussian, where the normalization constant only depends on the factor $A$ (the curvature at the mode). This is particularly useful in Bayesian inference where we often deal with unnormalized posteriors.''')
    st.markdown(r'''''')
    st.markdown(r'''However, the method does have some limitations. We assume that $z$ spans the entire support of $\mathbb{R}^d$. If our distribution is strictly positive, for instance, this might be problematic because our Gaussian approximation will extend from negative infinity to positive infinity. In practice, though, this isn't a severe issue as we can often change variables to make a distribution more Gaussian-like if needed.''')
    st.markdown(r'''''')
    st.markdown(r'''Another limitation is that the approximation is based purely on local information around the mode $z_0$, disregarding global information about $p(z)$. This can be problematic for multimodal distributions. If we have a bimodal distribution and only look at the curvature at one mode, we completely ignore the second mode. Fortunately, in many practical applications, including our logistic regression problem, we don't often encounter such multimodal posteriors.''')
    st.markdown(r'''''')
    st.markdown(r'''## Why Laplace Approximation Works for Logistic Regression''')
    st.markdown(r'''''')
    st.markdown(r'''Now, you might be wondering, given these limitations, why does Laplace approximation work so well for our Bayesian logistic regression problem? Let's examine this in more detail.''')
    st.markdown(r'''''')
    st.markdown(r'''Recall our posterior for logistic regression:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(w | t, X) = \mathcal{N}(m_0, S_0) \cdot \prod^N_{n=1} \sigma(w^T \phi(x_n))^{t_n} \{1 - \sigma(w^T \phi(x_n))\}^{(1-t_n)}$''')
    st.markdown(r'''''')
    st.markdown(r'''At first glance, this looks very complex. Let's consider what happens as we increase the number of data points. If we have only one data point ($N = 1$), the approximation doesn't work well. In fact, you need at least two data points to draw a boundary between two classes. With a very small number of data points, you'll get something quite skewed because you have a Gaussian multiplied by some tapering function.''')
    st.markdown(r'''''')
    st.markdown(r'''However, as we increase the number of observations, something interesting happens. Each additional data point ($x$ and $t$) shifts the decision boundary slightly. As you keep increasing the number of observations, two things occur: the posterior becomes tighter because with more data, we become more certain about our boundary, and the many data points average out where the boundary should be placed.''')
    st.markdown(r'''''')
    st.markdown(r'''This is where the central limit theorem comes into play. The central limit theorem tells us that the sum of a large number of independent random variables tends towards a Gaussian distribution, regardless of the underlying distribution of the variables. In our case, each data point contributes to the posterior in a way that can be thought of as an independent random variable. As we add more and more data points, their collective effect on the posterior starts to resemble a Gaussian distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''Let me illustrate this with an example. Imagine we have two classes. With very few data points, we might be able to draw many different lines that separate the two classes. But as we get more and more data points, we become more certain about where the boundary should be, and the distribution of possible boundaries becomes more Gaussian-like. This is essentially the central limit theorem in action.''')
    st.markdown(r'''''')
    st.markdown(r'''What this means for us is that even though our posterior looks very strange in its mathematical form, as the number of data points increases, it naturally starts to look more and more like a Gaussian. This is why the Laplace approximation becomes increasingly accurate as we gather more data.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's look at a concrete example. With $N = 1$, our Gaussian approximation is suboptimal. The true posterior is skewed and doesn't match well with a Gaussian. But if we increase $N$ to 500, we see that the Gaussian approximation becomes quite good. The true posterior is now much more symmetric and closely resembles a Gaussian distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''It's also worth noting that this function is concave and has a unique maximum, which you can prove as an exercise. This property ensures that our Laplace approximation will always find a single, well-defined mode to expand around.''')
    st.markdown(r'''''')
    st.markdown(r'''While Laplace approximation has its limitations, it works remarkably well for our Bayesian logistic regression problem due to the nature of our posterior and the effects of the central limit theorem as we increase our dataset size. This is why we can confidently use Laplace approximation to find an approximated Gaussian form to represent our complex posterior. As the number of data points increases, the approximation becomes increasingly accurate, allowing us to handle the non-Gaussian nature of classification problems within the Gaussian process framework.''')
    st.markdown(r'''''')
    st.markdown(r'''## Multidimensional Laplace Approximation''')
    st.markdown(r'''''')
    st.markdown(r'''In many cases, we're approximating the posterior of the decision boundary, which is characterized by the parameter vector $w$. This $w$ is technically the vector showing the tangent hyperplane of the boundary. Importantly, $w$ is often multidimensional. Even in a 2D case, if you have features $x_1$ and $x_2$, the boundary line is characterized by a vector of two numbers.''')
    st.markdown(r'''''')
    st.markdown(r'''So, we're not just approximating a 1D Gaussian; we need to approximate a multi-dimensional Gaussian. Fortunately, this extension is straightforward. Instead of finding $df/dz = 0$, we now look for $\nabla f(z) = 0$ at $z = z_0$ to find the stationary point (local maximum). Then, we calculate the Hessian matrix $A = -\nabla\nabla\ln f(z)$ at $z = z_0$.''')
    st.markdown(r'''''')
    st.markdown(r'''The resulting multidimensional Gaussian approximation is given by:''')
    st.markdown(r'''''')
    st.markdown(r'''$q(z) = \sqrt{\dfrac{|A|}{(2\pi)^M}} \exp\{-\dfrac{1}{2}(z - z_0)^T A(z - z_0)\} = \mathcal{N}(z | z_0, A^{-1})$''')
    st.markdown(r'''''')
    st.markdown(r'''Here, $A$ is an $\mathbb{R}^{m\times m}$ matrix, where $m$ is the number of dimensions. This approximation is valid provided that $A$ is positive definite, ensuring that $z_0$ is indeed a local maximum.''')
    st.markdown(r'''''')
    st.markdown(r'''Any questions so far? I know this material can be quite dense, but hopefully, when we get to the end of the derivation, you'll start to see what it actually means in practice.''')
    st.markdown(r'''''')
    st.markdown(r'''## Deriving the Log Posterior''')
    st.markdown(r'''''')
    st.markdown(r'''So far, we've shown that the strange-looking posterior for logistic regression can, in principle, be approximated by a Gaussian distribution. To find this approximation, we need to determine the Hessian matrix and the maximum point. This means we need to derive the log of $f(z)$.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's break down the log posterior:''')
    st.markdown(r'''''')
    st.markdown(r'''$-\ln p(w | t, X) = \dfrac{1}{2}(w - m_0)^T S_0^{-1}(w - m_0) - \sum^N_{n=1} \{t_n \ln y_n + (1 - t_n)\ln(1 - y_n)\} + const.$''')
    st.markdown(r'''''')
    st.markdown(r'''The first term is from the Gaussian prior, and the sum is the cross-entropy term from the likelihood. Note that $w$ is implicit in this equation because $y_n$ depends on $w$: $y_n = \sigma(w^T \phi(x_n))$.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's derive the Hessian. The quadratic term is straightforward - when you differentiate it twice, even in multiple dimensions, you get back the matrix. The second part is more involved, but after doing the derivation once and then twice, we get:''')
    st.markdown(r'''''')
    st.markdown(r'''$S = S_0^{-1} + \sum^N_{n=1} y_n(1 - y_n)\phi_n \phi_n^T$''')
    st.markdown(r'''''')
    st.markdown(r'''This gives us the curvature of our approximating Gaussian. We can also find the maximum point $w_{MAP}$ by setting the gradient to zero:''')
    st.markdown(r'''''')
    st.markdown(r'''$\nabla_w \ln p(w | t, X) = S_0^{-1}(m_0 - w) + \sum^N_{n=1} (t_n - y_n)\phi_n = 0$''')
    st.markdown(r'''''')
    st.markdown(r'''Solving this equation (often numerically) gives us $w_{MAP}$.''')
    st.markdown(r'''''')
    st.markdown(r'''Now we have all the pieces to write our Gaussian approximation to the posterior:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(w | t, X) \approx q(w) = \mathcal{N}(w | w_{MAP}, S)$''')
    st.markdown(r'''''')
    st.markdown(r'''## The Challenge of Integration''')
    st.markdown(r'''''')
    st.markdown(r'''Let's take a step back and consider what we're trying to achieve. Our ultimate goal is to compute the predictive distribution:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(y^* = 1 | x^*, \mathcal{D}) = \int p(y^* = 1 | x^*, w) p(w | \mathcal{D}) dw$''')
    st.markdown(r'''''')
    st.markdown(r'''We needed to approximate the posterior $p(w | \mathcal{D})$ because it's non-Gaussian for logistic regression. We've made this Gaussian, but it still doesn't guarantee integrability. If you have a sigmoid function (from $p(y^* = 1 | x^*, w)$) and a Gaussian (our approximated posterior), you generally wouldn't know how to do the integration analytically.''')
    st.markdown(r'''''')
    st.markdown(r'''## The Probit Function: A Clever Workaround''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's address the challenge we encountered earlier. We want to compute an integral that involves a sigmoid function and a Gaussian, but this integral isn't directly solvable. However, there's a clever workaround: we can approximate the sigmoid function with another function that looks very similar but makes the integration possible. This function is called the probit function.''')
    st.markdown(r'''''')
    st.markdown(r'''The probit function is closely related to the error function (erf). Mathematically, it's defined as:''')
    st.markdown(r'''''')
    st.markdown(r'''$\Phi(x) \equiv \int^x_{-\infty} \mathcal{N}(x | 0,1) dx$''')
    st.markdown(r'''''')
    st.markdown(r'''In other words, if you have a standard Gaussian distribution and do the indefinite integral from negative infinity to $x$, you get the probit function. Visually, the probit function looks very much like the sigmoid function we're familiar with from logistic regression.''')
    st.markdown(r'''''')
    st.markdown(r'''The key advantage of the probit function is that when you multiply it by a Gaussian, the resulting expression is integrable. Specifically:''')
    st.markdown(r'''''')
    st.markdown(r'''$\int \Phi(\lambda a) \mathcal{N}(a | \mu, \sigma^2) da = \Phi(\dfrac{\mu}{\sqrt{\lambda^{-2} + \sigma^2}})$''')
    st.markdown(r'''''')
    st.markdown(r'''This result is crucial for our purposes. While it's good to see the proof once, it's more important that you understand how to implement it rather than the details of the derivation. Nevertheless, let's walk through the proof briefly.''')
    st.markdown(r'''''')
    st.markdown(r'''Consider two Gaussian random variables, $X$ and $Y$:''')
    st.markdown(r'''''')
    st.markdown(r'''$X \sim \mathcal{N}(0,\omega^2)$, $Y \sim \mathcal{N}(\mu, \sigma^2)$''')
    st.markdown(r'''''')
    st.markdown(r'''If we subtract these two random variables, we get:''')
    st.markdown(r'''''')
    st.markdown(r'''$Z \equiv X - Y \sim \mathcal{N}(-\mu, \omega^2 + \sigma^2)$''')
    st.markdown(r'''''')
    st.markdown(r'''This is a mathematical way of saying that if you have a ground truth with zero mean and $\omega^2$ variance, and you add noise, the result still has a mean of $\mu$ but the variance is the sum of the variances.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, if we want to know the probability of $X$ being smaller than $Y$, we can approach it in two ways:''')
    st.markdown(r'''''')
    st.markdown(r'''1. We can move $Y$ to the left-hand side: $P(X \leq Y) = P(Z \leq 0)$. Since $Z$ is a Gaussian distribution, this probability is equivalent to evaluating the probit function:''')
    st.markdown(r'''''')
    st.markdown(r'''   $P(Z \leq 0) = \Phi(\dfrac{\mu}{\sqrt{\omega^2 + \sigma^2}})$''')
    st.markdown(r'''''')
    st.markdown(r'''2. Alternatively, we can consider drawing $Y$ from its distribution and asking what the chances are that $X$ is smaller than this drawn value, integrating over all possible draws of $Y$:''')
    st.markdown(r'''''')
    st.markdown(r'''   $P(X \leq Y) = \int^{\infty}_{(-\infty)} P(X \leq a)\mathcal{N}(a | \mu, \sigma^2) da = \int^{\infty}_{(-\infty)} \Phi(a/\omega)\mathcal{N}(a | \mu, \sigma^2) da$''')
    st.markdown(r'''''')
    st.markdown(r'''These two approaches must yield the same result, which gives us our key equation.''')
    st.markdown(r'''''')
    st.markdown(r'''To recap, we want to compute an integral involving a sigmoid function and a Gaussian, but we can't do this directly. However, if we switch to a probit function, it becomes integrable. A probit times a Gaussian is integrable, with the outcome being an evaluation of the probit function.''')
    st.markdown(r'''''')
    st.markdown(r'''## Approximating the Sigmoid with the Probit Function''')
    st.markdown(r'''''')
    st.markdown(r'''In practice, we can approximate the sigmoid function with the probit function quite accurately. The approximation is given by:''')
    st.markdown(r'''''')
    st.markdown(r'''$\sigma(a) \approx \Phi(\lambda a)$, where $\lambda^2 = \pi/8$''')
    st.markdown(r'''''')
    st.markdown(r'''This approximation is not arbitrary but carefully chosen to maximize accuracy. To illustrate this, let's consider a visual comparison between the logistic sigmoid function and its probit approximation.''')
    st.markdown(r'''''')
    st.markdown(r'''Imagine a graph with two curves: a solid red line representing the logistic sigmoid function $\sigma(a)$, and a dashed blue line showing the scaled probit function $\Phi(\lambda a)$. What's striking about this graph is the remarkable alignment between these two curves. The scaled probit function, with its precisely chosen scaling factor $\lambda$, provides an exceptionally accurate approximation of the sigmoid function.''')
    st.markdown(r'''''')
    st.markdown(r'''The choice of $\lambda$ such that $\lambda^2 = \pi/8$ is deliberate and mathematically motivated. This specific value ensures that the derivatives of both functions are equal at $a = 0$. As a result, the approximation is most accurate in the critical middle region of the curve, where the function is most sensitive to changes in input. This region is particularly important because it's where small changes in input lead to the largest changes in output probability.''')
    st.markdown(r'''''')
    st.markdown(r'''This close alignment between the sigmoid and probit functions is crucial for our purposes. It allows us to replace the sigmoid function in our integrals with the probit function, making previously intractable calculations possible while maintaining a high degree of accuracy in our probabilistic model.''')
    st.markdown(r'''''')
    st.markdown(r'''By using our probit approximation, we can transform this integral into a form that we know how to solve. Specifically, this integral can be approximated as:''')
    st.markdown(r'''''')
    st.markdown(r'''$\int \sigma(a)\mathcal{N}(a | \mu, \sigma^2)da \approx \sigma(\kappa(\sigma^2) \mu)$''')
    st.markdown(r'''''')
    st.markdown(r'''where $\kappa(\sigma^2) = (1 + \pi\sigma^2/8)^{(-1/2)}$''')
    st.markdown(r'''''')
    st.markdown(r'''## Deriving the Predictive Distribution''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's put all the pieces together and derive the predictive distribution for our Bayesian logistic regression model. Thanks to our approximations, we can now integrate a sigmoid times a Gaussian distribution, which was previously intractable.''')
    st.markdown(r'''''')
    st.markdown(r'''Recall that our goal is to compute:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(y^* = 1 | x^*, \mathcal{D}) = \int p(y^* = 1 | x^*, w) p(w | \mathcal{D}) dw$''')
    st.markdown(r'''''')
    st.markdown(r'''We've approximated $p(w | \mathcal{D})$ with a Gaussian $q(w) = \mathcal{N}(w | w_{MAP}, S)$ using Laplace approximation. The term $p(y^* = 1 | x^*, w)$ is our sigmoid function $\sigma(w^T \phi(x^*))$.''')
    st.markdown(r'''''')
    st.markdown(r'''To simplify our calculations, let's make a change of variable. We'll define $a = w^T \phi(x^*)$. This transforms our integral into:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(y^* | x^*, \mathcal{D}) = \int \sigma(a) p(a) da$''')
    st.markdown(r'''''')
    st.markdown(r'''Now, $p(a)$ is also a Gaussian distribution $\mathcal{N}(a | \mu_a, \sigma_a^2)$, where:''')
    st.markdown(r'''''')
    st.markdown(r'''$\mu_a = w_{MAP}^T \phi(x^*)$''')
    st.markdown(r'''$\sigma_a^2 = \phi^T(x^*) S \phi(x^*)$''')
    st.markdown(r'''''')
    st.markdown(r'''Using our probit approximation for the sigmoid, we can now solve this integral. The result is:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(y^* | x^*, \mathcal{D}) \approx \sigma((1 + \pi\sigma_a^2/8)^{(-1/2)} \mu_a)$''')
    st.markdown(r'''''')
    st.markdown(r'''This formula is crucial because it allows us to compute the predictive distribution in our Bayesian logistic regression model, accounting for the uncertainty in our parameters.''')
    st.markdown(r'''''')
    st.markdown(r'''## Interpreting the Results''')
    st.markdown(r'''''')
    st.markdown(r'''Let's interpret this result and understand its implications. If we compare this to standard logistic regression, we can see some important differences:''')
    st.markdown(r'''''')
    st.markdown(r'''1. In standard logistic regression, we would simply have $p(y^* | x^*, w_{MAP}) = \sigma(w_{MAP}^T \phi(x^*))$. This gives us a linear decision boundary in the feature space.''')
    st.markdown(r'''''')
    st.markdown(r'''2. In our Bayesian approach, we have an additional term $(1 + \pi\sigma_a^2/8)^{(-1/2)}$ which acts as a softening factor. This term depends on $x^*$ through $\sigma_a^2$, which measures the uncertainty in our prediction for this particular input.''')
    st.markdown(r'''''')
    st.markdown(r'''3. As $x^*$ moves away from our training data, $\sigma_a^2$ increases, which in turn decreases the argument to the sigmoid. This has the effect of pushing our predictions towards 0.5 (uncertainty) in regions where we have little or no training data.''')
    st.markdown(r'''''')
    st.markdown(r'''To visualize this, imagine a plot where the white line represents a 50-50 split between classes 0 and 1. In standard logistic regression, you'd see straight dashed lines moving away from this boundary, representing increasing certainty as you move away from the decision boundary. However, this isn't quite right - we shouldn't be that certain for areas with no data points.''')
    st.markdown(r'''''')
    st.markdown(r'''The Bayesian approach gives us a more nuanced picture. Near our training data, the predictions look similar to standard logistic regression. But as we move into regions with little or no training data, our certainty decreases. The decision boundary becomes less sharp, reflecting our increased uncertainty.''')
    st.markdown(r'''''')
    st.markdown(r'''This property is particularly important when dealing with extrapolation, which is common in many astronomical applications. For instance, if you have a biased or small sample in some parameter space, traditional logistic regression would make strong predictions even for regions without data points. This can lead to overconfident and potentially misleading results.''')
    st.markdown(r'''''')
    st.markdown(r'''The Bayesian approach, on the other hand, naturally reflects the uncertainty in regions without training data. The softening term $\sigma_a^2$ increases as $x^*$ moves away from your training data, leading to more uncertain predictions in these regions.''')
    st.markdown(r'''''')
    st.markdown(r'''This ability to quantify uncertainty is crucial for many classification tasks, especially in scientific applications where understanding the reliability of our predictions is as important as the predictions themselves.''')
    st.markdown(r'''''')
    st.markdown(r'''Are there any questions before we move on to even more technical aspects of this approach?''')
    st.markdown(r'''''')
    st.markdown(r'''## Beyond Linear Boundaries: Gaussian Process Classification''')
    st.markdown(r'''''')
    st.markdown(r'''Now that we've covered Bayesian logistic regression, let's address a remaining issue: the limitation of linear boundaries. Even with Bayesian logistic regression, we're still constrained to linear separations in our feature space. This is because the logistic regression likelihood is a sigmoid function of $w \cdot \phi(x)$, which inherently defines a linear boundary.''')
    st.markdown(r'''''')
    st.markdown(r'''But what if we need more flexibility? Many real-world problems require nonlinear boundaries. Let me illustrate this with an example inspired by an undergraduate project that evolved into an honors thesis. This project, interestingly, can be implemented in just five lines of code, yet it tackles a significant astronomical problem.''')
    st.markdown(r'''''')
    st.markdown(r'''Imagine we have data points representing galaxies and stars. Astronomers are trying to separate stars from galaxies using data from Gaia, based on color features like BP, RP and G. While it's theoretically possible to devise features that make blue and red points linearly separable, it's extremely challenging in practice. What if we could avoid this feature engineering step altogether? What if we could use whatever features we have and let the data determine the boundary? Even better, what if we could not only find the best boundary but also quantify its uncertainty?''')
    st.markdown(r'''''')
    st.markdown(r'''This is where Gaussian Process Classification (GPC) comes into play. GPC allows us to create flexible, nonlinear boundaries and quantify uncertainty in different regions. In the plot from our example, you can see that some areas become highly uncertain due to a lack of data points. The softening terms become larger in these sparse regions. Conversely, we're more certain in areas with abundant training data.''')
    st.markdown(r'''''')
    st.markdown(r'''This approach opens up countless possibilities for astronomical applications. To be honest, I'm not sure if I've seen many papers explicitly using GPC for astronomy, but its potential is immense.''')
    st.markdown(r'''''')
    st.markdown(r'''## The Core Idea of Gaussian Process Classification''')
    st.markdown(r'''''')
    st.markdown(r'''So, how do we make our boundary more flexible? How can we avoid being restricted to linear boundaries or needing to define specific features? This is the core idea behind GPC.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's start by recalling our discussion on Gaussian Process Regression (GPR). In GPR, we assumed that the output $t$ follows a joint Gaussian distribution: $p(t | X) = \mathcal{N}(0, K)$. However, this assumption doesn't work well for classification because our target values are typically binary (e.g., $t_n \in \{0,1\}$).''')
    st.markdown(r'''''')
    st.markdown(r'''The key idea in GPC is to use a Gaussian process as an intermediate step. Remember in logistic regression, we had $y(x) = \sigma(w^T \phi(x))$? In GPC, we replace this with $y(x) = \sigma(a(x))$, where $a(x)$ itself is a Gaussian process. Mathematically, we're saying:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(a | X) = \mathcal{N}(0, K)$''')
    st.markdown(r'''''')
    st.markdown(r'''And then we "squash" this Gaussian process with a sigmoid function:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(t_n | x_n) = \sigma(a(x_n))^{t_n} (1 - \sigma(a(x_n)))^{1-t_n}$''')
    st.markdown(r'''''')
    st.markdown(r'''This approach is logical because we're extending linear regression by substituting the linear models on features with a kernelized version of regression. In simpler terms, we're allowing the Gaussian process to have a very flexible functional form, and we're adding a tapering function to constrain it between 0 and 1.''')
    st.markdown(r'''''')
    st.markdown(r'''To visualize this process:''')
    st.markdown(r'''''')
    st.markdown(r'''1. We start with input variables $[x_1, x_2, ..., x_n]$.''')
    st.markdown(r'''2. We pass these through a Gaussian Process to get intermediate variables $a = [a_1, a_2, ..., a_n]$.''')
    st.markdown(r'''3. Finally, we apply a response function (like a sigmoid) to get our target variables $t = [t_1, t_2, ..., t_n]$.''')
    st.markdown(r'''''')
    st.markdown(r'''While this concept is straightforward, deriving the predictive distribution is challenging. We start with the assumption that our intermediate variable $A$ follows a joint Gaussian distribution. For any new data point $x^*$, $a^*$ will follow a Gaussian with a specific form, similar to what we discussed in GPR.''')
    st.markdown(r'''''')
    st.markdown(r'''Our ultimate goal is to infer $t^*$ given $x^*$, $t$, and $X$. However, we can't calculate this directly. The derivation involves several important mathematical tricks, which, while somewhat complex, are crucial for understanding this type of problem-solving approach. These techniques will be particularly important when we discuss MCMC and sampling from posteriors in future studies.''')
    st.markdown(r'''''')
    st.markdown(r'''## Deriving the Predictive Distribution''')
    st.markdown(r'''''')
    st.markdown(r'''Let's dive deeper into the derivation of Gaussian Process Classification (GPC), ensuring we cover all the crucial details. Our strategy is to break down complex distributions into manageable pieces, often using dummy variables and leveraging Bayes' theorem.''')
    st.markdown(r'''''')
    st.markdown(r'''We start with our goal: calculating the predictive distribution $p(t^* = 1 | x^*, X, t)$. This represents the probability of a new data point $x^*$ belonging to class 1, given our training data $X$ and $t$. To make this more tractable, we introduce an intermediate variable $a^*$:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(t^* = 1 | x^*, X, t) = \int p(t^* = 1 | x^*, X, t, a^*) p(a^* | x^*, X, t) da^*$''')
    st.markdown(r'''''')
    st.markdown(r'''This is an improvement because $p(t^* = 1 | x^*, X, t, a^*)$ is simply $\sigma(a^*)$, where $\sigma$ is the sigmoid function. However, we still don't know $p(a^* | x^*, X, t)$.''')
    st.markdown(r'''''')
    st.markdown(r'''To address this, we introduce another dummy variable $a$ and integrate over it:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(a^* | x^*, X, t) = \int p(a^* | x^*, X, a) p(a | X, t) da$''')
    st.markdown(r'''''')
    st.markdown(r'''The term $p(a^* | x^*, X, a)$ we know because $a$ follows a Gaussian process. For $p(a | X, t)$, we apply Bayes' theorem:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(a | X, t) \propto p(t | a, X) p(a | X)$''')
    st.markdown(r'''''')
    st.markdown(r'''Now we have terms we know:''')
    st.markdown(r'''- $p(t | a, X) = \prod^N_{n=1} \sigma(a_n)^{t_n} (1 - \sigma(a_n))^{(1-t_n)}$''')
    st.markdown(r'''- $p(a | X) = \mathcal{N}(0, K)$, where $K$ is the kernel matrix''')
    st.markdown(r'''''')
    st.markdown(r'''However, $p(a | X, t)$ is not Gaussian, so we can't integrate it analytically. This is where Laplace approximation comes in. We'll find a Gaussian $q(a)$ that approximates $p(a | X, t)$.''')
    st.markdown(r'''''')
    st.markdown(r'''## Applying Laplace Approximation''')
    st.markdown(r'''''')
    st.markdown(r'''To apply Laplace approximation, we need to find the mode $a_{MAP}$ and the Hessian $H$ at this mode. Let's start with finding $a_{MAP}$. We need to solve:''')
    st.markdown(r'''''')
    st.markdown(r'''$\nabla_a \ln p(a | X, t) = t - \sigma(a) - K^{-1}a = 0$''')
    st.markdown(r'''''')
    st.markdown(r'''This equation doesn't have a closed-form solution. Instead, we need to solve it iteratively. We can rewrite it as:''')
    st.markdown(r'''''')
    st.markdown(r'''$a = K(t - \sigma(a))$''')
    st.markdown(r'''''')
    st.markdown(r'''This is a stationary point problem of the form $a = F(a)$. To solve this, we can use Newton's method or other iterative techniques. In practice, many numerical optimization libraries can handle this type of problem efficiently.''')
    st.markdown(r'''''')
    st.markdown(r'''Once we've found $a_{MAP}$, we need to calculate the Hessian:''')
    st.markdown(r'''''')
    st.markdown(r'''$H = -\nabla_a\nabla_a \ln p(a | X, t) = \mathrm{diag}\{\sigma(a_{MAP})(1 - \sigma(a_{MAP}))\} + K^{-1}$''')
    st.markdown(r'''''')
    st.markdown(r'''With $a_{MAP}$ and $H$, we can approximate $p(a | X, t)$ as a Gaussian:''')
    st.markdown(r'''''')
    st.markdown(r'''$q(a) \approx p(a | X, t) = \mathcal{N}(a | a_{MAP}, H^{-1})$''')
    st.markdown(r'''''')
    st.markdown(r'''## Final Steps and Key Results''')
    st.markdown(r'''''')
    st.markdown(r'''Now we can return to our original integral. We're integrating the product of two Gaussians:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(a^* | x^*, X, t) = \int \mathcal{N}(a^* | m(x^*), \sigma^2(x^*)) \mathcal{N}(a | a_{MAP}, H^{-1}) da$''')
    st.markdown(r'''''')
    st.markdown(r'''where''')
    st.markdown(r'''''')
    st.markdown(r'''$m(x^*) = k(x^*, X)K^{-1}a$ ''')
    st.markdown(r'''''')
    st.markdown(r'''and ''')
    st.markdown(r'''''')
    st.markdown(r'''$\sigma^2(x^*) = k(x^*, x^*) - k(x^*, X)K^{-1}k(X, x^*)$.''')
    st.markdown(r'''''')
    st.markdown(r'''This integral results in another Gaussian:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(a^* | x^*, X, t) = \mathcal{N}(a^* | c, d^2)$''')
    st.markdown(r'''''')
    st.markdown(r'''Where:''')
    st.markdown(r'''''')
    st.markdown(r'''$c = k(x^*, X)(t - \sigma(a_{MAP}))$''')
    st.markdown(r'''''')
    st.markdown(r'''$d^2 = k(x^*, x^*) - k(x^*, X)(\mathrm{diag}\{\sigma(a_{MAP})(1 - \sigma(a_{MAP}))\}^{-1} + K)^{-1}k(X, x^*)$''')
    st.markdown(r'''''')
    st.markdown(r'''Finally, to get our predictive probability, we need to integrate:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(t^* = 1 | x^*, X, t) = \int \sigma(a^*) \mathcal{N}(a^* | c, d^2) da^*$''')
    st.markdown(r'''''')
    st.markdown(r'''This integral doesn't have a closed-form solution, but we can approximate it using the probit function, similar to what we did in Bayesian logistic regression:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(t^* = 1 | x^*, X, t) \approx \sigma(\dfrac{c}{\sqrt{1 + \pi d^2/8}})$''')
    st.markdown(r'''''')
    st.markdown(r'''This final approximation gives us our predictive probability for the new point $x^*$, taking into account the uncertainty in our model.''')
    st.markdown(r'''''')
    st.markdown(r'''## Interpreting the Results and Practical Implications''')
    st.markdown(r'''''')
    st.markdown(r'''The derivation we've just covered is complex but crucial. It demonstrates how to calculate a posterior that we don't have direct access to, including the use of dummy variables. These are techniques you might encounter in research, so understanding them is valuable.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's recap the key result of GPC. The predictive distribution is given by:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(t^* = 1 | x^*, X, t) = \sigma(\dfrac{c}{\sqrt{1 + \pi d^2/8}})$''')
    st.markdown(r'''''')
    st.markdown(r'''where:''')
    st.markdown(r'''''')
    st.markdown(r'''$c = k(x^*, X)(t - \sigma(a_{MAP}))$''')
    st.markdown(r'''''')
    st.markdown(r'''$d^2 = k(x^*, x^*) - k(x^*, X)(\mathrm{diag}\{\sigma(a_{MAP})(1 - \sigma(a_{MAP}))\}^{-1} + k(X, X))^{-1}k(X, x^*)$''')
    st.markdown(r'''''')
    st.markdown(r'''This formula might look intimidating, but it's the result of all the approximations and tricks we've discussed. The $c$ term here is quite interesting. $a_{MAP}$ represents the best boundary, and you can see that we adjust our boundary when our target contradicts the best models. This captures an idea similar to support vector machines, which we didn't cover in detail. It means we only need to determine the boundary based on the regions where there's contradictory information.''')
    st.markdown(r'''''')
    st.markdown(r'''The $d^2$ term is a softening factor that depends on how similar the new point $x^*$ is to our training data $X$. If $x^*$ is far from our training data, we get a larger softening term, reflecting increased uncertainty. This is crucial for classification tasks where we want to quantify the uncertainty of the classification.''')
    st.markdown(r'''''')
    st.markdown(r'''What makes this formula beautiful is how it captures the essence of Bayesian classification. It provides a flexible boundary without explicitly defining features, thanks to the kernel trick. It quantifies uncertainty based on the density of training data. In regions with sparse data, the predictions become less certain (closer to 0.5). And it focuses on contentious regions where class boundaries are unclear.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's look at a practical example. In a plot of class probabilities, you might see sharp transitions (0 to 1) in areas with dense, clearly separated training data. But in regions where the classes start to blend together, you'll see more gradual transitions, maybe from 0.3 to 0.7 over a larger range. And in areas far from any training data, the predictions will stay close to 0.5, reflecting our uncertainty.''')
    st.markdown(r'''''')
    st.markdown(r'''This behavior is exactly what we want in a Bayesian classifier. It's not just finding the best boundary, but characterizing the uncertainty of that boundary based on our training data. This is particularly important when extrapolating, such as in astronomical applications where you might have a biased or small sample in some parameter space. Traditional logistic regression would make strong predictions even for regions without data points, which can be problematic.''')
    st.markdown(r'''''')
    st.markdown(r'''The beauty of GPC is that it's often the best method for low-dimensional classification problems, which are common in astronomy. If you're working with, say, five photometric bands and want to classify objects into a few categories, GPC is ideal. There's often no need to resort to more complex methods like neural networks for these types of problems.''')
    st.markdown(r'''''')
    st.markdown(r'''This approach to GPC is a rare case where we can use these approximation tricks and still get excellent results. In many other scenarios, these integrals become impossible to solve analytically, which is why we'll be learning about sampling and MCMC in future studies. MCMC isn't just a theoretical concept. It's developed because as you move from simple linear regression to more complex models like Gaussian processes, you encounter these challenging integrals that you have no way to solve analytically, even when you're just trying to compute a posterior or predictive distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''In conclusion, GPC is a powerful tool that provides flexible decision boundaries and uncertainty estimates, making it ideal for many astronomical classification tasks. It excels at finding boundaries without explicitly defining features, thanks to the kernel trick. And it gives us a Bayesian version of classification, allowing us to quantify the uncertainty of the boundary based on the density of the training data. Understanding its derivation gives you insights into handling complex statistical problems, even if in practice you'll mostly be using the final formula we derived today.''')
    st.markdown(r'''''')

#if __name__ == '__main__':
show_page()