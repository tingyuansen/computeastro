import streamlit as st
import streamlit.components.v1 as components

def show_page():

    # Page Title
    st.title('Gaussian Process - Regression')

    # Embed the external HTML page
    # st.info('''
    # Course Slides
    # ''')
    # slideshare_embed_code = """
    #     <iframe src="https://www.slideshare.net/slideshow/embed_code/key/5zt3C3VpEusqIM" 
    #             width="700" height="394" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" 
    #             style="border:0px solid #000; border-width:0px; margin-bottom:0px; max-width: 100%;" allowfullscreen> 
    #     </iframe> 
    # """
    # components.html(slideshare_embed_code, height=412)

    # st.markdown('---')

    st.markdown(r'''Welcome, everyone. Let's begin our lecture. Today, we're going to recap what we've learned in the last two lectures about neural networks and then move on to a new topic. ''')
    st.markdown(r'''''')
    st.markdown(r'''In our previous sessions, we discussed neural networks. To put this in context, let's remind ourselves why we use neural networks in the first place. We began this course with four lectures on the basics of machine learning. You can think of these fundamental techniques as learning to play basic instruments - guitar, piano, or violin. While these methods are useful, they each have their limitations.''')
    st.markdown(r'''''')
    st.markdown(r'''When we introduced neural networks, it was like jumping into the world of K-pop - exciting and capable of overcoming some limitations of simpler methods. However, neural networks themselves have an important limitation: they lack a strong statistical background. This is improving over time as researchers begin to understand the statistical properties of neural networks better, but it remains a challenge.''')
    st.markdown(r'''''')
    st.markdown(r'''To draw a parallel with music, you might love K-pop, but you can also appreciate classical music like Bach or Mozart. That's where we're heading today with Gaussian processes. I believe Gaussian processes represent one of the most beautiful pieces of mathematics in machine learning, and they're extremely well understood. This is our 'classical music' - we know the theory, we know how to implement it, and while it has some limitations, it's truly elegant math that can do things neural networks still struggle with.''')
    st.markdown(r'''''')
    st.markdown(r'''Remember, only format equations and mathematical variables. Do not make any other changes to the text structure or formatting.''')
    st.markdown(r'''''')
    st.markdown(r'''However, like classical music, Gaussian processes can be hard to appreciate at first. It took me years to fully internalize their concepts. I want to warn you that this lecture and the next one will be among the most challenging in this course. The topic is demanding, but the algorithms themselves are quite straightforward. For exams and assignments, you should find the practical aspects manageable. The real challenge lies in developing an appreciation for Gaussian processes, which is what I'll try to convey today.''')
    st.markdown(r'''''')
    st.markdown(r'''Don't be discouraged if the math seems daunting at first. The key is to grasp the main concepts as you start implementing these methods. You'll begin to appreciate their beauty as you work with them more.''')
    st.markdown(r'''''')
    st.markdown(r'''To understand Gaussian processes, we need to first revisit the limitations of linear regression. There are two main approaches to overcoming these limitations. First, neural networks try to learn the features by passing data through multiple layers, with only the final layer performing linear regression. Second, Gaussian processes take a different approach, eliminating the need to define features explicitly while retaining the beneficial properties of linear regression.''')
    st.markdown(r'''''')
    st.markdown(r'''Gaussian processes use what we call the 'kernel trick'. This allows us to work with potentially infinite-dimensional features without explicitly defining them. It's a powerful concept that's still relevant in modern deep learning.''')
    st.markdown(r'''''')
    st.markdown(r'''Today, we'll focus on Gaussian processes for regression tasks. We'll explore two perspectives: the weight-space perspective and the function-space perspective. Then we'll discuss implementation. While I usually don't recommend additional textbooks, for Gaussian processes, I highly recommend reading the first chapter and initial sub-chapters of 'Gaussian Processes for Machine Learning' by Rasmussen and Williams. It's a beautifully written, timeless textbook that I found invaluable when I was a student.''')
    st.markdown(r'''''')
    st.markdown(r'''Remember, only format equations and mathematical variables. Do not make any other changes to the text structure or formatting.''')
    st.markdown(r'''''')
    st.markdown(r'''There will be a lot of mathematics ahead, more than you've seen in previous lectures. But before we dive in, it's important to understand why we need this level of mathematical depth. Let's begin with a recap of linear regression and then move on to the kernel trick and Gaussian processes.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's delve into the applications of Gaussian processes in astronomy. It's worth noting that Gaussian processes are applied very frequently in this field. In fact, some researchers have built entire careers around using Gaussian processes in astronomical research. While the concept itself isn't particularly difficult, gaining an intuitive understanding of the algorithms can be challenging.''')
    st.markdown(r'''''')
    st.markdown(r'''In astronomy, Gaussian processes are primarily applied to light curves of various celestial objects. Let's consider a few examples to illustrate this.''')
    st.markdown(r'''''')
    st.markdown(r'''First, let's look at exoplanet transits. In this case, we observe the flux of a star as a function of time. The resulting light curve can take various forms. The challenge lies in describing such a light curve mathematically. For exoplanet transits, it might be possible to use an analytic formula because it's a relatively simple geometrical scenario. However, in many other cases, if you have a complex light curve, you can't rely on simple linear regression or basic analytic formulas to describe it accurately.''')
    st.markdown(r'''''')
    st.markdown(r'''If you look at the image I'm showing now, you can see an artist's rendering of an exoplanet transit, courtesy of NASA. It gives you a visual idea of what's happening during these events. Now, let me show you some actual data. This graph here is from a recent study by Gordon and colleagues in 2020. What you're seeing is the raw data from an exoplanet transit, and overlaid on that are samples drawn from a fitted Gaussian process. Notice how the Gaussian process captures the overall trend of the data while also accounting for the uncertainty and variability.''')
    st.markdown(r'''''')
    st.markdown(r'''A more compelling example is the accretion of black holes, also known as quasar accretion. You might recall the spectra example from assignment three, but another way to study black holes is by examining their light curves. This is, in fact, one of the primary tasks of sky surveys.''')
    st.markdown(r'''''')
    st.markdown(r'''When studying quasars, astronomers observe changes in luminosity over time. We know that the brightness of quasars varies because the accretion disk surrounding the quasar is not static. The underlying physics and particular time scales determine the light curve we observe. However, similar to the stock market, there are numerous complexities involved. It would be impossible to model this accurately using simple linear regression.''')
    st.markdown(r'''''')
    st.markdown(r'''Let me show you what I mean. Here's another image from NASA, giving you an artist's impression of a quasar. And now, look at this plot. This is from a study by Kelly and others in 2011. What you're seeing here is the log luminosity of a quasar plotted against time. The blue dots are the actual data points, and the red line is the fitted Gaussian process. See how it captures the overall trend and the complex variations? This is exactly the kind of scenario where Gaussian processes excel.''')
    st.markdown(r'''''')
    st.markdown(r'''Moreover, even with identical underlying physics, the process is fundamentally stochastic. This means that each light curve is essentially a draw from a distribution. Even with the same physical conditions, the realization of the light curve will not be identical each time. This is what we refer to as a stochastic process, and Gaussian processes are a type of stochastic process particularly well-suited to modeling such phenomena.''')
    st.markdown(r'''''')
    st.markdown(r'''Another application of Gaussian processes in astronomy involves studying the light curves of stars. The brightness of stars also changes over time, and researchers use Gaussian processes to model these variations as well. This field, known as asteroseismology, focuses on studying stellar oscillations and can provide valuable insights into the internal structure and evolution of stars.''')
    st.markdown(r'''''')
    st.markdown(r'''Here's an interesting visualization related to asteroseismology. This image gives you an idea of what we're looking at when we study stellar oscillations. And in this context, I want to mention a paper by Pereira and colleagues from 2019. They've done some fascinating work applying Gaussian processes to asteroseismology data.''')
    st.markdown(r'''''')
    st.markdown(r'''It's important to note that many of these applications are relatively recent developments. Despite the straightforward nature of Gaussian processes, their application in astronomy is not yet widespread beyond a few specific examples. However, the potential for broader application is significant.''')
    st.markdown(r'''''')
    st.markdown(r'''The key advantage of Gaussian processes becomes apparent when dealing with complex light curves for which we cannot write down a precise model. This is a crucial point. In most of our previous work, we've focused on writing down models and optimizing them with respect to some loss function. But what if we don't even know how to formulate the model in the first place? This is where Gaussian processes shine, providing a flexible framework for modeling complex, stochastic processes without requiring a precise underlying model.''')
    st.markdown(r'''''')
    st.markdown(r'''Today, we'll address Gaussian processes in the context of machine learning. This chart effectively summarizes the key concepts in classical machine learning that we've covered and will explore further.''')
    st.markdown(r'''''')
    st.markdown(r'''Remember, only format equations and mathematical variables. Do not make any other changes to the text structure or formatting.''')
    st.markdown(r'''''')
    st.markdown(r'''We began with linear regression and progressed to Bayesian linear regression, represented by the yellow line in our chart. The Bayesian approach acknowledges that with finite data, our model inherently carries uncertainty. Instead of seeking a single optimal set of parameters ($W^*$), we aim to determine the distribution of $W$. This distribution of models leads to a predictive distribution, allowing us to quantify uncertainty in our predictions for new data points.''')
    st.markdown(r'''''')
    st.markdown(r'''The chart also illustrates the duality between regression and classification tasks, shown by the green line connecting linear regression to logistic regression. In your recent assignment, you identified the optimal decision boundary for classification, but we didn't address the uncertainty of this boundary—a crucial consideration given finite data.''')
    st.markdown(r'''''')
    st.markdown(r'''The blue line in our chart represents the kernel trick, which we'll explore today. This technique is particularly useful when we lack a clear model or don't want to explicitly define features. The kernel trick bridges linear regression and Gaussian process regression, offering a powerful alternative approach.''')
    st.markdown(r'''''')
    st.markdown(r'''Importantly, this chart encapsulates the core of classical machine learning, providing a comprehensive overview that's rare in specialized courses. While we cover substantial mathematical ground, our goal is to offer a broad perspective on how these methods interconnect, rather than an exhaustive treatment of each technique.''')
    st.markdown(r'''''')
    st.markdown(r'''There's also a connection to neural networks, though we won't delve deeply into this. Research has shown that Gaussian processes are equivalent to infinitely wide, single-layer neural networks. This relationship is noted in the chart, with a reference to Bishop's textbook (section 6.4.7) for those interested in further exploration.''')
    st.markdown(r'''''')
    st.markdown(r'''You might wonder why we're dedicating significant time to Gaussian processes when neural networks dominate current machine learning discourse. A critical limitation of traditional neural networks is their treatment of uncertainty. In backpropagation, we optimize weights without considering their distribution or uncertainty. This presents a significant challenge, especially with models containing billions of parameters.''')
    st.markdown(r'''''')
    st.markdown(r'''Bayesian neural networks attempt to address this by estimating weight distributions rather than point estimates, but this remains a complex problem in practice. For many applications, such as image classification, this uncertainty might not be critical. However, in scientific contexts, understanding model uncertainty is often crucial.''')
    st.markdown(r'''''')
    st.markdown(r'''This is where Gaussian processes excel. They offer comparable flexibility to neural networks while providing a full Bayesian treatment, allowing us to quantify model uncertainty. Our chart highlights this advantage, noting that Gaussian processes are "Statistically rigorous, easy to give full Bayesian treatments — as opposed to neural networks".''')
    st.markdown(r'''''')
    st.markdown(r'''The chart also outlines our course progression, indicating when we'll cover various aspects of Gaussian processes and related concepts. This structure helps situate Gaussian processes within the broader context of our machine learning curriculum.''')
    st.markdown(r'''''')
    st.markdown(r'''With this motivation established, we'll proceed to explore the mathematical foundations of Gaussian processes. Understanding these concepts will provide you with a powerful tool for modeling complex systems while maintaining a rigorous statistical framework.''')
    st.markdown(r'''''')
    st.markdown(r'''Remember, only format equations and mathematical variables. Do not make any other changes to the text structure or formatting.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's recall linear regression, using notation that aligns closely with Gaussian process literature. In linear regression, we assume the output $y$ is a linear function of the features $\phi(x)$: $y(x) = \phi(x)^T w$. Here, $w \in \mathbb{R}^m$ represents the weights, and $m$ is the number of features. For $N$ training data points, we can construct a design matrix $\Phi \in \mathbb{R}^{N\times m}$ containing all our input features, and a target vector $t \in \mathbb{R}^N$ for the corresponding outputs.''')
    st.markdown(r'''''')
    st.markdown(r'''In the non-Bayesian approach, where we're solely interested in finding the best-fit line without considering weight uncertainty, the solution is given by: $w* = (\Phi^T \Phi)^{-1} \Phi^T t$. This formula should be familiar to you by now; I hope you can recite it in your sleep!''')
    st.markdown(r'''''')
    st.markdown(r'''However, this method has a significant limitation: the need to define features beforehand. In many real-world scenarios, determining appropriate features is challenging. This limitation motivates us to explore methods that can adapt features to the data. While neural networks offer one approach to this problem, we'll focus on another powerful technique: kernel methods.''')
    st.markdown(r'''''')
    st.markdown(r'''So, what if we don't know how to define the features? What assumptions can we make? A naive assumption might be that we should only consider neighboring points. For example, if we want to predict the weather at Stromlo, instead of writing a complex function relating various variables to temperature, we might simply estimate it based on readings from neighboring suburbs like Weston and Belconnen. This approach, similar to K-nearest neighbors, assumes some continuity in the target function based on similarity.''')
    st.markdown(r'''''')
    st.markdown(r'''The crucial point here is that there are no explicit models. We don't need to construct a model about how temperature changes as a function of all variables. Instead, the training data itself becomes our model. This is the essence of kernel methods.''')
    st.markdown(r'''''')
    st.markdown(r'''If this seems abstract, consider Kernel Density Estimation (KDE), a technique many of you have likely used in your research. KDE is a density estimation method based on kernel techniques. Let's say we want to describe a distribution of $X$ given a set of data points $X_n$, but we don't know how to write down the distribution. If we knew the distribution, we could simply maximize the likelihood. But what if we can't formulate the function?''')
    st.markdown(r'''''')
    st.markdown(r'''One straightforward approach is to place a Gaussian kernel around each data point and sum their contributions. The formula for KDE is: $p(x) = (1/N) \sum_{n=1}^N [1/((2\pi h^2)^{1/2}) * \exp(-(‖x - x_n‖^2)/(2h^2))]$. Here, $N$ is the number of data points, and $h$ is a smoothing parameter. For astronomers, this concept of a smoothing scale should be familiar from N-body simulations and other contexts.''')
    st.markdown(r'''''')
    st.markdown(r'''Importantly, this equation requires no training. Once we write it down, our model is complete. The only parameter we need to specify is $h$, the smoothing scale. If we choose a small $h$, our density estimate becomes quite spiky. A larger $h$ results in a smoother distribution. This is equivalent to deciding how many neighbors we should trust or, in our weather prediction analogy, how large an influence sphere we should consider.''')
    st.markdown(r'''''')
    st.markdown(r'''This approach has several notable characteristics. First, it's fundamentally different from what we've discussed so far: there are no explicit models, and the data itself serves as the model. There's no training phase; we always just look for the nearest neighbors and perform a weighted sum. However, there's also a significant drawback: this method can be extremely slow. For a dataset with a billion points, making a prediction for a new data point $X$ requires summing over all billion points, as in principle, all data points contribute to the density. Of course, you can truncate this sum based on a chosen influence field, but the computational intensity remains a challenge.''')
    st.markdown(r'''''')
    st.markdown(r'''The choice of $h$, our smoothing parameter, is crucial and remains a key aspect to specify in this method. This hyperparameter effectively quantifies the smoothness of our function and determines how much we allow our model to adapt to local variations in the data.''')
    st.markdown(r'''''')
    st.markdown(r'''Now that we've discussed kernel methods for density estimation, let's shift our focus to their application in regression tasks. The fundamental question is: How can we apply kernel methods to regression?''')
    st.markdown(r'''''')
    st.markdown(r'''In supervised learning, we have labeled training data $\{x_i, t_i\}^N_{i=1}$. Our goal is to find a predictive function that can generate outputs for new input points. Ideally, this function would take the form:''')
    st.markdown(r'''''')
    st.markdown(r'''$y(x) = \Sigma^N_{i=1} k(x, x_i)a_i$''')
    st.markdown(r'''''')
    st.markdown(r'''Here, $k(x, x_i)$ is the kernel function, which measures the similarity between the new point $x$ and each training point $x_i$. The term $a_i$ is an implicit model that depends solely on the training data. This formulation allows us to make predictions without explicitly defining the underlying function.''')
    st.markdown(r'''''')
    st.markdown(r'''The key insight here is that we're not specifying the function directly. Instead, we're looking at the nearest neighbors within an influence sphere, similar to our approach in density estimation. The crucial difference is that we're now applying this concept to a regression task.''')
    st.markdown(r'''''')
    st.markdown(r'''Given this framework, our first task is to define the form of similarity. How do we determine if two input points are close, especially in high-dimensional spaces? One obvious choice, which you might recall from last week's colloquium, is the dot product.''')
    st.markdown(r'''''')
    st.markdown(r'''If we have feature vectors $\phi(x_n)$ and $\phi(x_m)$, their dot product gives us information about the angle between these vectors. Aligned vectors (indicating similar data points) will have a larger dot product. This is also known as cosine similarity. Mathematically, we can express this as:''')
    st.markdown(r'''''')
    st.markdown(r'''$K_{nm} = \phi(x_n)^T \phi(x_m)$''')
    st.markdown(r'''''')
    st.markdown(r'''For our $N$ training data points, we construct an $N×N$ matrix $K$, where each entry $K_{nm}$ represents the pairwise similarity between training points $x_n$ and $x_m$. In matrix notation:''')
    st.markdown(r'''''')
    st.markdown(r'''$K = Φ(x)Φ(x)^T ∈ ℝ^{N×N}$''')
    st.markdown(r'''''')
    st.markdown(r'''This matrix $K$ is what we call the kernel matrix. It encapsulates all the pairwise similarities in our training set.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, you might wonder: Why are we doing this? The goal is to rewrite our linear regression solution in terms of this kernel matrix $K$, rather than the feature matrix $Φ$. This approach allows us to work with similarities between data points directly, without needing to explicitly define or compute the feature vectors.''')
    st.markdown(r'''''')
    st.markdown(r'''For simplicity, let's assume we're working without regularization and noise ($λ = 0$, $β^{-1} = 0$). The general proof with these terms included is similar and can be found in standard textbooks.''')
    st.markdown(r'''''')
    st.markdown(r'''The key question now is: Can we reformulate our familiar linear regression solution using this kernel representation instead of explicit features? This transition from feature space to kernel space is at the heart of kernel methods and will lead us to Gaussian Process regression.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's apply the kernel trick to linear regression. We know that in linear regression, the best weights are given by $w* = (Φ^T Φ)^{-1} Φ^T t$. For any new data point $x$, the prediction is $y(x) = ϕ(x)^T w*$. Substituting $w*$, we get:''')
    st.markdown(r'''''')
    st.markdown(r'''$y(x) = ϕ(x)^T (Φ^T Φ)^{-1} Φ^T t$''')
    st.markdown(r'''''')
    st.markdown(r'''However, this isn't in the form we want because our kernel is $ϕϕ^T$, not $ϕ^T ϕ$. Can we reformulate this in terms of $ϕϕ^T$? The answer is yes.''')
    st.markdown(r'''''')
    st.markdown(r'''First, we use the matrix inversion lemma: $(AB)^{-1} = B^{-1} A^{-1}$. We can see that these two terms will cancel out. But we still don't see $ϕϕ^T$. So we'll inject $ϕϕ^T$ here, injecting an identity. We'll inject both $ϕϕ^T$ as well as $(ϕϕ^T)^{-1}$. This is just injecting an identity; we're doing nothing. But this term will cancel out. Grouping all these terms, we arrive at:''')
    st.markdown(r'''''')
    st.markdown(r'''$y(x) = (Φϕ(x))^T (ΦΦ^T)^{-1} t$''')
    st.markdown(r'''''')
    st.markdown(r'''Let's walk through what this form means. $\phi$ itself is a row matrix. If we have a row matrix and dot it with the transpose, this term is calculating the cosine similarity between any two points, taking the row, dot with the column, taking the row, dot with the columns. What about this term? This term is taking that row matrix and dotting it with a single column matrix. But a single column matrix is the features of your test data point, that one test data point that you want to predict.''')
    st.markdown(r'''''')
    st.markdown(r'''This can be rewritten in kernel notation as:''')
    st.markdown(r'''''')
    st.markdown(r'''$y(x) = k(x, X)k(X, X)^{(-1)} t$''')
    st.markdown(r'''''')
    st.markdown(r'''Or in summation form:''')
    st.markdown(r'''''')
    st.markdown(r'''$y(x) = \sum^N_{i=1} a_i k(x, x_i)$''')
    st.markdown(r'''''')
    st.markdown(r'''If you squint your eyes a little bit, this is exactly the form we want. We want to say, without defining the features, can we find a solution of $y$ that only depends on $K$, where $K$, on the one hand, calculates how close our test data point is with respect to the training data points, and then with some weights, the weights themselves depend only on the training data.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's take a step to marvel at this part. You might think that we've really done nothing, right? It seems like we've just done some injection of identity and changed the form from $\phi^T \phi$ to $\phi\phi^T$. What's the big deal? If anything, you might think that we're making things worse. Remember, $\phi\phi^T$ is an $N$ by $N$ matrix where $N$ is the number of data points. So if you have a million data points, this is a million by million matrix. Instead of inverting an $M$ by $M$ matrix, where $M$ is the number of features, we're taking the opportunity to invert the million by million matrix. You've seen some of this trade-off in your PCA, right? We can have the duality. You can choose to do the diagonalization on one or the other.''')
    st.markdown(r'''''')
    st.markdown(r'''Yes, we are losing in a way, right? We're doing a lot more calculation, just like in the case of KDE. But we gain one thing - we're getting rid of the features. If you write down this form, it means that as long as we can define the kernel, we do not need to define $\phi$. We'll see that this is really advantageous. As I said, this seems dumb. Instead of inverting a small matrix, you invert a big one. But now you get rid of $\phi$. And remember, what is the definition of a kernel? A kernel is just saying that we need to define a bivariate function that tells us how close two points are. And this is much more flexible than defining the features because it's much easier to say whether or not Stromlo is close to Belconnen than defining the models of the temperature for the whole of Australia.''')
    st.markdown(r'''''')
    st.markdown(r'''But there's one catch here, right? We know that given the features, we can find the kernel. But we also want to make sure that any kernel we define can correspond to some feature space. You cannot define any bivariate function and call it a kernel, because for the argument to work, we need to make sure that we can go from $\phi$ to $k$ and $k$ back to $\phi$. This is the hard part of math that we won't talk about in detail, but it turns out that you need the kernel function to satisfy two things:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Symmetry: $k(x,y) = k(y,x)$ for all $x$ and $y$. This is intuitive; the similarity between Belconnen and Stromlo should be the same as between Stromlo and Belconnen.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Positive semi-definiteness: For any finite set of points $\{x_1, ..., x_n\}$, the kernel matrix $K$ with $K_{ij} \equiv k(x_i, x_j)$ must be positive semi-definite. In other words, for any vector $u \in \mathbb{R}^N$, $u^T Ku \geq 0$.''')
    st.markdown(r'''''')
    st.markdown(r'''If these two conditions are satisfied, then by Mercer's theorem, there exists a feature map $\phi$ such that the kernel is the dot product of these features. This guarantees the duality between feature space and kernel space.''')
    st.markdown(r'''''')
    st.markdown(r'''I know this is a bit much, but we won't quiz you on it. It just means that you can't find any bivariate function and call it a similarity function. You need to satisfy these two things. If you satisfy these two things, then by Mercer's condition, there exists a $\phi$ such that the dot product of $\phi$ is exactly the kernel.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's explore some specific kernel functions and their corresponding feature spaces. This will help us understand the power and flexibility of kernel methods.''')
    st.markdown(r'''''')
    st.markdown(r'''First, consider a simple kernel function defined as $k(x_m, x_n) = x_m^T x_n$ for any input $m$ and $n$. This kernel is equivalent to a feature map $\phi(x) = x$, because the dot product of these features is exactly the kernel. So you can either define $\phi$ or define $k$; they're equivalent.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's look at a slightly more complex example. If we square the previous kernel, we get $k(x_m, x_n) = (x_m^T x_n)^2$. In two dimensions, this corresponds to a feature map $\phi(x) = (x_1^2, \sqrt{2}x_1x_2, x_2^2)$. You can verify that $\phi(x)\phi(x)^T$ will give you exactly this form in the kernel. Again, you can choose to work with either the feature space or the kernel space.''')
    st.markdown(r'''''')
    st.markdown(r'''At this point, you might think, "This seems rather pointless. If I define it in the $\phi$ space, I'm only inverting a 3x3 matrix for these three features. But if I have 100 million data points and define it in the kernel space, I'm inverting a million-by-million dimensional matrix. Why would I choose the latter?"''')
    st.markdown(r'''''')
    st.markdown(r'''The beauty of kernel methods becomes apparent when we consider kernels that correspond to infinitely long feature vectors. A prime example is the famous radial basis function (RBF) kernel, also known as the Gaussian kernel:''')
    st.markdown(r'''''')
    st.markdown(r'''$k(x_m, x_n) = exp(-(\|x_n - x_m\|^2)/(2\sigma^2))$''')
    st.markdown(r'''''')
    st.markdown(r'''This kernel has an intuitive interpretation: it measures the similarity between points $x_n$ and $x_m$ based on their distance, modulated by a length scale $\sigma$. If we're comparing the similarity between Belconnen and Stromlo, for instance, we'd take the distance between the two and adjust it by some length scale $\sigma$, which defines the "influence sphere".''')
    st.markdown(r'''''')
    st.markdown(r'''While this kernel seems straightforward to write down, it actually corresponds to an infinite-dimensional feature space. We can prove that this is a valid kernel (symmetric and positive definite), so by Mercer's theorem, there exists a feature map $\phi$ such that this duality works. But why is this kernel equivalent to an infinite-dimensional feature space?''')
    st.markdown(r'''''')
    st.markdown(r'''The key lies in the Taylor series expansion of the exponential function:''')
    st.markdown(r'''''')
    st.markdown(r'''$\exp(x) = 1 + x + x^2/2! + x^3/3! + ...$''')
    st.markdown(r'''''')
    st.markdown(r'''Each term in this expansion corresponds to a different order of features. The first-order term corresponds to one feature, the second-order to a set of features, and so on. By including all terms in the infinite sum, we're effectively working with an infinite-dimensional feature space.''')
    st.markdown(r'''''')
    st.markdown(r'''Take a moment to appreciate the implications of this. We're still solving a linear regression problem, but by working in the kernel space, we gain tremendous flexibility. We can easily define the kernel, yet it corresponds to an infinite number of features in the primal space. Yes, we now need to invert larger matrices, but we gain the ability to fit our data with infinite-dimensional freedom.''')
    st.markdown(r'''''')
    st.markdown(r'''This is why Gaussian processes are so appealing to statisticians. We're not explicitly defining a model, yet it's as if we've defined a model with infinite-dimensional freedom to fit our data. That's the power of kernel methods and Gaussian processes.''')
    st.markdown(r'''''')
    st.markdown(r'''It's natural if this seems overwhelming at first. It took me a long time to fully internalize these concepts. But as we progress, you'll see how these ideas come together to form a powerful framework for machine learning.''')
    st.markdown(r'''''')
    st.markdown(r'''We've kernelized linear regression, but we're only halfway there. We haven't yet kernelized Bayesian linear regression. Remember, in the Bayesian version, we don't just want $y^*$; we want the distribution of $y^*$. This is because our weights are uncertain, and we need to integrate over this uncertainty.''')
    st.markdown(r'''''')
    st.markdown(r'''In Bayesian linear regression, the prediction of $y$ follows a distribution where the mean is the best prediction we've just discussed. However, $y^*$ also has uncertainty stemming from two sources: observational error and uncertainty in the weights themselves. This is where neural networks often struggle - they focus on finding the best fit $w$, rarely considering the distribution of $w$, the posterior of $w$.''')
    st.markdown(r'''''')
    st.markdown(r'''For simplicity, we'll ignore observational noise, though the derivation would be similar if we included it. Pictorially, in linear regression, because we have finite data points, our line itself is uncertain. When predicting $y^*$ for a new $x^*$, we must account for this weight uncertainty.''')
    st.markdown(r'''''')
    st.markdown(r'''We've discussed the best prediction of $y^*$ based on kernelized regression: $k(x^*,X)k(X,X)^{-1}t$. But can we also express the uncertainty in this kernelized form? ''')
    st.markdown(r'''''')
    st.markdown(r'''The magic here is similar to what we've seen before. We know that $m'$ (the mean of the predictive distribution) is $\phi^T(x^*)(Φ^T Φ)^{-1}Φ^T t$. Using the identity tricks we've learned, this simplifies to the kernel form we've just described.''')
    st.markdown(r'''''')
    st.markdown(r'''For a new data point $x^*$, we first evaluate its similarity to all training data, similar to KDE, where the weights depend only on the training data. The covariance term $S'$ follows a similar pattern. In linear regression, it's expressed as $\phi^T S_N \phi$. Applying our kernel trick, we can rewrite this in terms of $\phi\phi^T$ instead of $\phi^T \phi$.''')
    st.markdown(r'''''')
    st.markdown(r'''This leads us to the key result of Gaussian Process regression. Given a dataset $\mathcal{D}$ consisting of training inputs $X$ and outputs $t$, for any new input $x^*$, we can predict $y^*$ as:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(y^* | x^*, \mathcal{D}) = \mathcal{N}(y^* | m', S')$''')
    st.markdown(r'''''')
    st.markdown(r'''where:''')
    st.markdown(r'''$m' = k(x^*, X) k(X, X)^{-1} t$''')
    st.markdown(r'''$S' = k(x^*, x^*) - k(x^*, X) k(X, X)^{-1} k(X, x^*)$''')
    st.markdown(r'''''')
    st.markdown(r'''Let's break down what each term means:''')
    st.markdown(r'''- $k(x^*,x^*)$ is the similarity of $x^*$ with itself.''')
    st.markdown(r'''- $k(x^*,X)$ is the similarity between $x^*$ and all training points.''')
    st.markdown(r'''- $k(X,X)$ is the similarity among all training data points.''')
    st.markdown(r'''- $k(X,x^*)$ is the transpose of $k(x^*,X)$.''')
    st.markdown(r'''''')
    st.markdown(r'''Take a moment to appreciate the elegance of this result. We've done nothing more than define a kernel function, yet we can now predict not just $y^*$, but its entire predictive distribution. This is the power of Gaussian Processes.''')
    st.markdown(r'''''')
    st.markdown(r'''It's not too surprising when you think about it. If you want to predict the temperature at a new location, you might find the nearest neighbors and calculate some form of average. This is an extension of that idea, but with a rigorous probabilistic foundation.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's delve into the function-space perspective of Gaussian Processes (GPs), which offers an intuitive way to understand this powerful concept.''')
    st.markdown(r'''''')
    st.markdown(r'''First, why do we call it a Gaussian process? A GP is a stochastic process defined as a probability distribution over functions. For any scalar field $Y$ and any given $X$ in a $D$-dimensional space, we output a scalar. Formally, for any set of points $\{x_1, ..., x_n\}$, the joint distribution of $\{y(x_1), ..., y(x_n)\}$ follows a multivariate Gaussian distribution:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(y(x_1), ..., y(x_n)) = \mathcal{N}(0, K)$''')
    st.markdown(r'''''')
    st.markdown(r'''We typically set the mean to zero, assuming no prior knowledge of $y(x)$. The process is fully specified by its covariance matrix $K$.''')
    st.markdown(r'''''')
    st.markdown(r'''This definition might seem strange because we're saying a function itself is Gaussian. We can't evaluate this at infinite points, but for any finite set of points, if $y$ evaluated at $x1$ to $xn$ forms a Gaussian distribution, we call this function a Gaussian process.''')
    st.markdown(r'''''')
    st.markdown(r'''In our discussion, we'll assume a zero-mean Gaussian. This means for any set of X, P(Y, X1 to Xn) follows a zero-mean Gaussian defined by the covariance matrices. The covariance matrix is an RnxRn matrix, which we call K, tying back to the kernel we've discussed.''')
    st.markdown(r'''''')
    st.markdown(r'''The function-space perspective views a function as an infinite-dimensional vector. Before observing any data, we can draw many functions from this distribution, constrained by the kernel which represents the function's smoothness. Once we observe data points, we restrict our functional class to only those functions that pass through these points.''')
    st.markdown(r'''''')
    st.markdown(r'''To visualize this, imagine a zero-mean Gaussian before any conditioning. With two points, $x1$ and $x2$ (corresponding to $t1$ and $t2$), $T1$ and $T2$ initially form a centered Gaussian. Observing $T1$ determines the distribution of $T2$, depending on the Gaussian's tilt (covariance), which represents the function's smoothness.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, consider a set of data points Y, including our training data T and a point we want to predict, $Y*$. Assuming these form a Gaussian process, we have an $(N+1)x(N+1)$ Gaussian joint distribution. When we observe T and want to know the posterior of $Y*$, we can derive the conditional distribution analytically:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(y* | t) = \mathcal{N}(m', S')$''')
    st.markdown(r'''''')
    st.markdown(r'''Where:''')
    st.markdown(r'''$m' = k(x*, X) k(X, X)^{(-1)} t$''')
    st.markdown(r'''$S' = k(x*, x*) - k(x*, X) k(X, X)^{(-1)} k(X, x*)$''')
    st.markdown(r'''''')
    st.markdown(r'''This is identical to the formula from the weight-space view, demonstrating the equivalence of these perspectives.''')
    st.markdown(r'''''')
    st.markdown(r'''The beauty of this approach is that we never explicitly deal with features. We define the functional class as a Gaussian process and can evaluate new test points directly.''')
    st.markdown(r'''''')
    st.markdown(r'''But how can we constrain an infinite-dimensional functional space with finite data? The key lies in the properties of Gaussian processes. Although we're dealing with infinite functions, we always have finite training data and evaluate at finite test points. At any evaluation, we're only considering N+M dimensions at most, where N is the training data size and M is the number of test points.''')
    st.markdown(r'''''')
    st.markdown(r'''GPs make a strong assumption: any finite subset of points will form a joint Gaussian distribution. This allows us to work with manageable, finite-dimensional distributions even when our underlying space is infinite-dimensional.''')
    st.markdown(r'''''')
    st.markdown(r'''This approach extends kernel density estimation (KDE) to regression tasks within a fully probabilistic framework. Like KDE, we consider nearby points for predictions, but with well-calibrated uncertainty estimates.''')
    st.markdown(r'''''')
    st.markdown(r'''The equations have intuitive properties. If $x^*$ is close to a training point $x_i$, the kernel assigns more weight to that point. Our uncertainty decreases when the test point is close to training data and increases as we move away.''')
    st.markdown(r'''''')
    st.markdown(r'''To recap, we can view GPs from two perspectives:''')
    st.markdown(r'''''')
    st.markdown(r'''Weight-space: We start with Bayesian linear regression and kernelize it.''')
    st.markdown(r'''Function-space: We define a prior over functions and condition on observed data.''')
    st.markdown(r'''''')
    st.markdown(r'''Both lead to the same result, offering a powerful, flexible framework for regression with uncertainty quantification. The function-space view, in particular, gives us a way to work with infinite-dimensional spaces using finite computations, all thanks to the properties of Gaussian processes.''')
    st.markdown(r'''''')
    st.markdown(r'''We haven't yet discussed a crucial aspect: how do we choose which kernel to use? There are many kernels to choose from, so let's delve into the intuitive understanding of kernels.''')
    st.markdown(r'''''')
    st.markdown(r'''Consider this generic kernel, which is both positive and symmetric definite:''')
    st.markdown(r'''''')
    st.markdown(r'''$k(x_n, x_m) = \theta_0 \exp(-\theta_1/2 ||x_n - x_m||^2) + \theta_2 + \theta_3 x_n^T x_m$''')
    st.markdown(r'''''')
    st.markdown(r'''This is essentially an RBF (radial basis function) plus a linear kernel. It's worth noting that if two functions are kernel functions, you can add them to create a new valid kernel.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's break down the effects of each parameter:''')
    st.markdown(r'''''')
    st.markdown(r'''1. $\theta_0$ is just a normalization factor, not doing much on its own.''')
    st.markdown(r'''''')
    st.markdown(r'''2. $\theta_1$ affects the 'jaggedness' of the function. As we increase $\theta_1$, the functions we draw from the prior become more jagged. Why? Because as $\theta_1$ increases, as long as $x_n$ is not the same as $x_m$, this term becomes very small. We're effectively decreasing the length scale, resulting in a more jagged function. Conversely, decreasing $\theta_1$ leads to smoother functions, starting with a more flexible functional class.''')
    st.markdown(r'''''')
    st.markdown(r'''3. $\theta_2$ is a constant term. If we increase $\theta_2$, we increase the correlation at all lengths. No matter what $x_m$ and $x_n$ are, they're always more correlated.''')
    st.markdown(r'''''')
    st.markdown(r'''4. $\theta_3$ is the coefficient of the linear term. Increasing it will expand the similarity as points become more aligned.''')
    st.markdown(r'''''')
    st.markdown(r'''These parameters have important consequences. As shown here, if we fix different length scales. Imagine we always observe the white crosses in our training data. If we choose the optimal length scale (we'll discuss what makes it optimal later), we get a function that makes sense. But if we choose an length scale that's too large or too small, we'll get predictions that don't make much sense - we'll either overfit or underfit.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's recap what we've learned about Gaussian processes and compare them to Bayesian linear regression:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Gaussian processes and Bayesian linear regression are two sides of the same coin. You're either looking at the feature space or the kernel space.''')
    st.markdown(r'''''')
    st.markdown(r'''2. In many cases for linear regression, the number of features ($m$) is smaller than the number of data points ($N$). This is why linear regression became popular - you only need to invert an $m \times m$ matrix. But the downside is that you're restricting yourself to a specific functional class, defining features by hand.''')
    st.markdown(r'''''')
    st.markdown(r'''3. Gaussian processes, on the other hand, look at the entire distribution of functions. If you define an RBF kernel, you're effectively working with infinite-dimensional features. So implicitly, you're assuming that the number of features is much larger than the training data. Even if you have a trillion data points, your $m$ is still larger - it's infinity.''')
    st.markdown(r'''''')
    st.markdown(r'''4. This is why, even though inverting an $N×N$ matrix for GP seems bad, it would be worse to invert an $m×m$ matrix when $m$ is infinite. You have a lot of freedom with GPs, but you still have an $N^3$ complexity, which becomes problematic for large datasets.''')
    st.markdown(r'''''')
    st.markdown(r'''5. This computational complexity is usually what limits GPs. If you have more than one or two-dimensional inputs, it becomes really hard to use GPs. This is why we often apply GPs to light curves in astronomy - they're 1D inputs with finite length, making them more manageable.''')
    st.markdown(r'''''')
    st.markdown(r'''The beauty of Gaussian processes lies in their flexibility and their ability to work with potentially infinite-dimensional feature spaces. However, this comes at the cost of computational complexity, which can be a significant limitation for large or high-dimensional datasets.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's set aside all the intuition and the mind-blowing beauty we've discussed. Let's focus on how you would actually apply Gaussian Processes in practice. The application is surprisingly simple and boils down to this key formula:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(y* | x*, X, t, \beta^{(-1)}) = \mathcal{N}(m', S')$''')
    st.markdown(r'''''')
    st.markdown(r'''Where:''')
    st.markdown(r'''$m' = k(x*, X) k(X, X)^{(-1)} t$''')
    st.markdown(r'''$S' = k(x*, x*) - k(x*, X) k(X, X)^{(-1)} k(X, x*)$''')
    st.markdown(r'''''')
    st.markdown(r'''We've seen this many times. It's straightforward: you have some training data $X$ and $t$. For now, we're assuming no noise, but in the tutorial, we'll discuss how to include noise in the model.''')
    st.markdown(r'''''')
    st.markdown(r'''When you have a new data point $x*$, you can evaluate all of this because you've predefined what $K$ is. The main challenge is optimally inverting $k(X, X)$. If you do this naively, it's always an $O(N^3)$ operation, but there's always a slightly better $N^3$ than a very bad $N^3$.''')
    st.markdown(r'''''')
    st.markdown(r'''The best approach is to use Cholesky decomposition. Since $k(X, X)$ is a symmetric positive definite matrix, you can decompose it as:''')
    st.markdown(r'''''')
    st.markdown(r'''$k(X, X) = L \cdot L^T$''')
    st.markdown(r'''''')
    st.markdown(r'''Where $L$ is a lower triangular matrix. Why is this good? Because Cholesky decomposition requires $N^3/3$ operations. It's still $O(N^3)$, but it's much better than other methods. For example, other ways to do the inverse usually take about two times $N^3/3$ operations. So Cholesky is still the best you can do if you want to blindly invert the matrix. If you have a choice between one hour or three hours of computation, you'll choose one hour, right? It's not a massive gain, but it's significant.''')
    st.markdown(r'''''')
    st.markdown(r'''Once you have the Cholesky decomposition, the term $k(X, X)^{(-1)} t$ becomes $(L \cdot L^T)^{(-1)} t$. This is equivalent to solving a linear equation. In many programming languages, this is denoted by a backslash operator, meaning we're solving the linear equation:''')
    st.markdown(r'''''')
    st.markdown(r'''$L \cdot L^T \cdot x = t$''')
    st.markdown(r'''''')
    st.markdown(r'''There are good numerical libraries, like NumPy, that can solve this efficiently. This operation is computationally inexpensive compared to the decomposition.''')
    st.markdown(r'''''')
    st.markdown(r'''After the Cholesky decomposition, the remaining operations are $O(N^2)$ or less. Here's how we can break it down step by step:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Compute the Cholesky decomposition: $L \equiv Cholesky(k(X, X))$''')
    st.markdown(r'''2. Solve for $\alpha$: $\alpha \equiv L^T \ (L \ t)$''')
    st.markdown(r'''3. Compute the mean: $m(x*) = k(x*, X) \cdot \alpha$''')
    st.markdown(r'''4. Compute $v$: $v = L \ k(X, x*)$''')
    st.markdown(r'''5. Compute the variance: $\sigma^2(x*) = k(x*, x*) - v^T \cdot v$''')
    st.markdown(r'''''')
    st.markdown(r'''The hardest part is the matrix inversion, which we handle through Cholesky decomposition. Everything else is just matrix multiplication or solving triangular systems, which are much faster.''')
    st.markdown(r'''''')
    st.markdown(r'''In the tutorial, you'll see how to implement this. But the key takeaway is that it's really just about how to best perform the matrix inversion. Even if you don't optimize it perfectly, you can still get good results. Define the kernel matrix, do the linear algebra operations, and you'll be fine.''')
    st.markdown(r'''''')
    st.markdown(r'''Remember, the main limitation is still the $O(N^3)$ complexity. This is why Gaussian Processes can become challenging for very large datasets. But for many practical applications, especially in fields like astronomy where we often deal with one-dimensional time series data, this approach can be very powerful and effective.''')
    st.markdown(r'''''')
    st.markdown(r'''There's still one crucial aspect we need to address: how do we learn the best function class? We know we need to choose a length scale $\ell$, and visually, we can see that some choices look more correct than others. But how can we statistically determine that, say, $\ell = 1$ is indeed better than $\ell = 0.3$ or $\ell = 3$? This is where we apply something you've learned before: maximizing the likelihood.''')
    st.markdown(r'''''')
    st.markdown(r'''We start with our data $t$, which forms a joint distribution. What we want to find is the $\theta$ (our hyperparameters) that best captures this distribution. Mathematically, we're looking at:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(t | X, \theta) = \mathcal{N}(0, K)$''')
    st.markdown(r'''''')
    st.markdown(r'''This is the marginal distribution of all the data. We're essentially finding the best Gaussian that can capture this data. If you have $N$ data points, $t$ is in $N$ dimensions, so you're optimizing your kernel to best capture all the data points.''')
    st.markdown(r'''''')
    st.markdown(r'''The log-likelihood we want to maximize is:''')
    st.markdown(r'''''')
    st.markdown(r'''$\ln p(t | X, \theta) = -1/2 \ln |K| - 1/2 t^T K^{-1} t - N/2 \ln(2\pi)$''')
    st.markdown(r'''''')
    st.markdown(r'''This is just a Gaussian distribution, so you've seen this form many times. You have the normalization terms, the quadratic terms, and the determinant terms.''')
    st.markdown(r'''''')
    st.markdown(r'''How do you optimize this? There are two main approaches:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Automatic differentiation: You can simply plug this into an auto-diff library and let it handle the optimization. This is often the most practical approach in modern machine learning frameworks.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Analytic form: There's also an analytic form for the gradient, which you can use for gradient descent:''')
    st.markdown(r'''''')
    st.markdown(r'''$\dfrac{\partial}{\partial \theta_i} \ln p(t | X, \theta) = -1/2 \mathrm{Tr}(K^{-1} \dfrac{\partial K}{\partial \theta_i}) + 1/2 t^T K^{-1} \dfrac{\partial K}{\partial \theta_i} K^{-1} t$''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's look more closely at how we can simplify the likelihood calculation using the Cholesky decomposition. Remember, we start with:''')
    st.markdown(r'''''')
    st.markdown(r'''ln p(t | X, θ) = $-1/2 \ln |K| - 1/2 t^T K^{-1} t - N/2 \ln(2\pi)$''')
    st.markdown(r'''''')
    st.markdown(r'''We can simplify this using the Cholesky decomposition $K = LL^T$. Here's how:''')
    st.markdown(r'''''')
    st.markdown(r'''1. The determinant: $|K| = |LL^T| = |L||L^T| = |L|^2$''')
    st.markdown(r'''   Since $L$ is triangular, its determinant is the product of its diagonal elements.''')
    st.markdown(r'''   So, $\ln |K| = 2 \sum_i \ln L_{ii}$''')
    st.markdown(r'''''')
    st.markdown(r'''2. The quadratic term: $t^T K^{-1} t$''')
    st.markdown(r'''   We can rewrite this as $t^T (LL^T)^{-1} t = (L^{-T} t)^T (L^{-1} t)$''')
    st.markdown(r'''   Let's define $\alpha \equiv L^T \backslash (L \backslash t)$, which is equivalent to $L^{-T} (L^{-1} t)$''')
    st.markdown(r'''   Then $t^T K^{-1} t = \alpha^T t$''')
    st.markdown(r'''''')
    st.markdown(r'''Putting this together, we get:''')
    st.markdown(r'''''')
    st.markdown(r'''$\ln p(t | X, \theta) = -\sum_i \ln L_{ii} - 1/2 t^T \alpha - N/2 \ln(2\pi)$''')
    st.markdown(r'''''')
    st.markdown(r'''This form is computationally more efficient and stable, especially for large datasets. ''')
    st.markdown(r'''Recall that when you do the inference, you would have already calculated $L$ and $L^T$ first, and all this can be reused to calculate the likelihood for this particular choice of kernel. You can do a grid search, meaning that you can choose different hyperparameters and calculate the joint likelihood, finding the one that is the best. We will implement this approach in the tutorial.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's summarize what we've learned about Gaussian Processes:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Gaussian Processes provide a flexible, non-parametric approach to regression and classification tasks.''')
    st.markdown(r'''''')
    st.markdown(r'''2. They can be viewed from two perspectives:''')
    st.markdown(r'''   - Weight-space: As a generalization of Bayesian linear regression''')
    st.markdown(r'''   - Function-space: As a distribution over functions''')
    st.markdown(r'''''')
    st.markdown(r'''3. The key component is the kernel function, which defines the similarity between data points and implicitly determines the properties of the functions we're modeling.''')
    st.markdown(r'''''')
    st.markdown(r'''4. GPs naturally provide uncertainty estimates, which is a significant advantage in many applications.''')
    st.markdown(r'''''')
    st.markdown(r'''5. The main limitation is computational complexity, scaling as $O(N^3)$ with the number of data points.''')
    st.markdown(r'''''')
    st.markdown(r'''6. Hyperparameter learning can be done through maximum likelihood estimation, often using gradient-based optimization or grid search.''')
    st.markdown(r'''''')
    st.markdown(r'''7. While the mathematical foundation may seem complex, the practical implementation often boils down to a few key formulas and algorithms.''')
    st.markdown(r'''''')
    st.markdown(r'''In the tutorial, we'll walk through a practical implementation of these ideas. You'll see that while the underlying theory is complex, the actual application can be quite straightforward. The key is to understand the core concepts and how they translate into practical algorithms for machine learning tasks.''')
    st.markdown(r'''''')
    st.markdown(r'''Remember, developing an intuitive understanding of Gaussian Processes takes time. Don't worry if some aspects still feel unclear – practice and application will help solidify these concepts. The beauty of GPs lies in their elegant mathematical formulation and their power in capturing complex patterns in data, all while providing well-calibrated uncertainty estimates.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's walk through the tutorial a bit. We'll spend some time on this specifically because it's crucial for understanding the practical applications of Gaussian Processes.''')
    st.markdown(r'''''')
    st.markdown(r'''We start by revisiting what a Gaussian distribution is and how sampling from a joint distribution relates to sampling from a conditional distribution. Remember, a Gaussian process assumes that all variables, including both the training and test sets, come from a joint Gaussian. When we condition on the training set to make predictions, we're working with the conditional distribution.''')
    st.markdown(r'''''')
    st.markdown(r'''In the tutorial, we begin with some toy data. It's a rather nasty dataset, but that's what makes it interesting. The beauty of Gaussian Processes is that you can fit a model without explicitly defining the function. In this case, the true function is $x^2 \times \sin(x) \times \tanh(x)$, but in real-world scenarios, you often don't know the functional form. With Gaussian Processes, as long as you can define the kernel, you can fit the function extremely well. And not only that, you can also predict the uncertainty of your predictions.''')
    st.markdown(r'''''')
    st.markdown(r'''Initially, we fix the hyperparameters. But we can go further and optimize over different kernels by calculating the likelihood for various parameter values. When we use the best-fit kernel, we get a much more reasonable function than our initial attempt.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's marvel at this for a moment. You have no access to the true function. You only have five data points. You do nothing except assume continuity and define the closeness of data points. Yet, you're able to fit the data and quantify the uncertainty of the fit. This is the essence of machine learning.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, you might ask, "What does this have to do with astronomy?" Well, let's look at a real-world example: a quasar light curve from Kepler. We can fit this light curve, optimize for the best kernel, and get a good fit. Interestingly, the RBF (Radial Basis Function) kernel isn't actually the best for quasar light curves. There's another class of kernels that performs even better. But the RBF kernel is still useful because it gives us insight into the timescale of the black hole accretion.''')
    st.markdown(r'''''')
    st.markdown(r'''By finding the best-fit $\sigma$ for each light curve and plotting a histogram of these values, we can determine the characteristic timescale of quasar accretion. This is quite remarkable. Even though the RBF isn't the optimal kernel, it still allows us to extract meaningful information. Quasar timescales can range from a day to a year, and there's no way to define an analytic function that captures this variation. Even a neural network would struggle here because we don't have labels. But Gaussian Processes give us a way to extract this timescale information by optimizing the kernel to best describe the data.''')
    st.markdown(r'''''')
    st.markdown(r'''Alright, that's all for today. We've covered a lot of ground, from the theoretical foundations of Gaussian Processes to their practical applications in astronomy. Remember, it's okay if some of this still feels unclear – developing intuition for these methods takes time and practice. Any questions from those online? No? Okay, I'll see you next Tuesday at the same time. Bye-bye!''')
    st.markdown(r'''''')
    st.markdown(r'''''')

if __name__ == '__main__':
    show_page()