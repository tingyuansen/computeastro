import streamlit as st
import streamlit.components.v1 as components

def show_page():

    # Page Title
    st.title('Supervised Learning: Classification - Logistic Regression')

    # Embed the external HTML page
    # st.info('''
    # Course Slides
    # ''')
    # slideshare_embed_code = """
    #     <iframe src="https://www.slideshare.net/slideshow/embed_code/key/qF5DOvLfPX6vsM" 
    #             width="700" height="394" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" 
    #             style="border:0px solid #000; border-width:0px; margin-bottom:0px; max-width: 100%;" allowfullscreen> 
    #     </iframe> 
    # """
    # components.html(slideshare_embed_code, height=412)

    # st.markdown('---')
    
    st.markdown(r'''''')
    st.markdown(r'''## Introduction to Classification''')
    st.markdown(r'''''')
    st.markdown(r'''Welcome to our discussion on classification and logistic regression. In our previous session, we explored linear regression, where we built a linear model for a supervised task to make inferences on continuous variable labels. Today, we're extending our discussion of linear models to a different supervised task: classification.''')
    st.markdown(r'''''')
    st.markdown(r'''Our plan for today is to explore logistic regression, which, despite its name, is actually a classification task rather than a regression task. The key difference between regression and classification is that classification deals with discrete variables. Instead of mapping to continuous values like a black hole's mass, we're categorizing objects into discrete classes, such as galaxies or stars.''')
    st.markdown(r'''''')
    st.markdown(r'''Some of you may already be familiar with logistic regression and might have used it in your research. This lecture aims to provide you with the mathematical foundation behind it. Logistic regression serves as an excellent model to introduce the concepts of discriminative and generative models.''')
    st.markdown(r'''''')
    st.markdown(r'''In any supervised task, we can approach the problem from both discriminative and generative perspectives. We'll examine how these approaches are linked through Bayes' theorem. Logistic regression is a unique case where we can rigorously derive the relationship between the generative and discriminative models.''')
    st.markdown(r'''''')
    st.markdown(r'''We'll also discuss why we can't simply treat classification as a regression task. As an undergraduate, I made the mistake of applying regression techniques to discrete labels, using MSE loss or chi-square. We'll explore why this approach is problematic and introduce the concept of the sigmoid function as a solution.''')
    st.markdown(r'''''')
    st.markdown(r'''You may have heard terms like "cross-entropy loss" in the context of neural networks. Both the sigmoid function (also known as the perceptron) and cross-entropy loss are widely used in neural networks. While these are often assumed as objective functions in neural networks, we can derive them rigorously through logistic regression and Bayes' theorem.''')
    st.markdown(r'''''')
    st.markdown(r'''Unlike linear regression, where we had analytic solutions for maximum likelihood estimation, posterior weights, and predictive distribution, classification introduces non-convexity due to the sigmoid function. This necessitates numerical optimization methods, such as stochastic gradient descent, which will be crucial when we discuss neural networks later.''')
    st.markdown(r'''''')
    st.markdown(r'''We'll start with the simple case of binary classification (e.g., cats vs. dogs) and then extend our discussion to multi-class problems.''')
    st.markdown(r'''''')
    st.markdown(r'''## Understanding Classification''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's delve into the concept of classification. As we discussed earlier, the main difference between today's topic and our previous discussion is that we're dealing with a classification task. Like regression, classification is a supervised task, but instead of predicting continuous values, we're assigning input data to discrete classes.''')
    st.markdown(r'''''')
    st.markdown(r'''In classification, our goal is to map an input x to one of K discrete classes, where K can be any number from two upwards. In principle, K can be very large. This concept extends to more complex applications like large language models. For instance, ChatGPT essentially performs a classification task: given the input text, it predicts the next word from a discrete set of thousands of possible tokens.''')
    st.markdown(r'''''')
    st.markdown(r'''For a smaller number of classes, it's intuitive to think of classification as dividing the input space into different regions. Let's consider the classic example of classifying flower species. If we have two features, $X_1$ (petal length) and $X_2$ (sepal length), we might find that different flower species cluster in distinct regions of this two-dimensional feature space. Our goal in classification is to find the boundaries between these regions. For linear models, we're specifically looking for linear separators in the feature space.''')
    st.markdown(r'''''')
    st.markdown(r'''## Applications in Astronomy''')
    st.markdown(r'''''')
    st.markdown(r'''Classification is widely used in astronomy, particularly for survey target selection. Given limited observational resources, astronomers need to pre-select objects of interest. For example, if you're planning a metal-poor star survey, you'd want to pre-select stars likely to be in that category. Similarly, for spectroscopic surveys, you need to distinguish between galaxies and stars based on some preliminary features before collecting spectra. The aim is to find boundaries between classes using easily obtainable features, allowing efficient follow-up observations of specific object classes.''')
    st.markdown(r'''''')
    st.markdown(r'''In many real-world scenarios, classes may not be linearly separable in the original feature space. However, with domain knowledge, you can often identify or construct features that separate classes more effectively. For instance, in star-galaxy separation, the WISE color has proven to be a useful index.''')
    st.markdown(r'''''')
    st.markdown(r'''The field has evolved significantly with new data sources. The Gaia mission, for example, has revolutionized star-galaxy separation by providing precise astrometric data. Stars move relative to Earth, while distant galaxies appear stationary. This astrometric information has become a powerful tool for classification. In fact, when Gaia's first data release (DR1) came out, there was a flurry of papers on improved star-galaxy separation techniques.''')
    st.markdown(r'''''')
    st.markdown(r'''Beyond simple binary classification, there's ongoing research in more complex categorizations. For instance, classifying galaxies based on their morphology remains a hot topic. Researchers might want to identify specific types of galaxies, like ring galaxies, for focused studies. Similarly, classifying galaxies according to the Hubble sequence is another active area of research.''')
    st.markdown(r'''''')
    st.markdown(r'''## Formulating Classification''')
    st.markdown(r'''''')
    st.markdown(r'''Now that we've discussed the motivation behind classification, let's formulate it more rigorously. As mentioned earlier, we're dealing with discrete variables. In the simplest case of binary classification, we can encode the classes using 0 and 1. For instance, if we're classifying cats and dogs, we might designate cats as class 0 and dogs as class 1.''')
    st.markdown(r'''''')
    st.markdown(r'''However, when we move to multi-class problems, the encoding becomes more complex. You might think of using integers from 0 to K-1 for K classes, but this approach has a significant drawback. It implicitly suggests an ordering or relationship between classes that may not exist. For example, if we're classifying cats, dogs, and humans, assigning them values 0, 1, and 2 respectively could incorrectly imply that dogs are more similar to humans than to cats.''')
    st.markdown(r'''''')
    st.markdown(r'''To avoid this issue, we typically use one-hot encoding for multi-class problems. In this scheme, if we have K classes, we represent each class as a K-dimensional vector. The vector has a 1 in the position corresponding to the class and 0s everywhere else. For instance, in a five-class problem, an object belonging to the second class would be represented as $[0,1,0,0,0]$.''')
    st.markdown(r'''''')
    st.markdown(r'''It's important to note that while these are the ground truth labels, our model's output (which we often denote as Y) doesn't need to be binary. It can take any value between 0 and 1, which has a natural probabilistic interpretation. For example, if our model outputs $[0.95, 0.05, 0, 0, 0]$ for a five-class problem, we can interpret this as saying there's a 95% chance the input belongs to class 1 and a 5% chance it belongs to class 2. This probabilistic output is much more informative than a single integer encoding.''')
    st.markdown(r'''''')
    st.markdown(r'''## Discriminative vs. Generative Models''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's focus on binary classification and introduce the concepts of discriminative and generative models. These are two approaches to the same problem, linked by Bayes' theorem.''')
    st.markdown(r'''Recall Bayes' theorem: ''')
    st.markdown(r'''''')
    st.markdown(r'''$P(Y|X) = \dfrac{P(X|Y) \cdot P(Y)}{P(X)}$''')
    st.markdown(r'''''')
    st.markdown(r'''In our classification context, $X$ represents our input features and $C_k$ represents our class labels.''')
    st.markdown(r'''''')
    st.markdown(r'''A discriminative model directly learns $P(C_k|X)$ - the probability of a class given the input. This approach focuses on finding the decision boundary in the input space. It's generally simpler and often sufficient for many classification tasks.''')
    st.markdown(r'''''')
    st.markdown(r'''On the other hand, a generative model learns $P(X|C_k)$ - the probability of observing the input given a particular class. This is a more complex approach but can be more powerful in certain scenarios.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's delve deeper into the concept of generative models. While discriminative models focus on finding the decision boundary, generative models take a different approach. Instead of directly learning $P(C_k|X)$, generative models aim to learn $P(X|C_k)$ - the distribution of feature inputs given each class.''')
    st.markdown(r'''''')
    st.markdown(r'''This approach is more challenging because $X$ is typically high-dimensional. For instance, if we're dealing with images, $X$ represents all the pixels, making the distribution of images much more complex to describe than just the boundary between classes.''')
    st.markdown(r'''''')
    st.markdown(r'''To illustrate this, consider learning languages. It's easier to distinguish between German and Portuguese by listening to many sentences and learning the boundary between them (discriminative approach) than to actually learn to speak both languages fluently (generative approach). The discriminative approach only requires you to recognize the differences, while the generative approach requires a much deeper understanding of the language's structure and rules.''')
    st.markdown(r'''''')
    st.markdown(r'''You might wonder, "If I only need to find the boundary for classification, why bother with the more difficult generative approach?" The answer lies in the additional capabilities that generative models provide. While discriminative models are often sufficient for classification tasks, generative models offer more flexibility and power.''')
    st.markdown(r'''''')
    st.markdown(r'''For example, if you only know the boundary between German and Portuguese, you might struggle to identify a sentence that belongs to neither language. However, if you have a generative model of both languages, you can recognize when a sentence doesn't fit either distribution. This ability to detect outliers is crucial in many fields, including astronomy.''')
    st.markdown(r'''''')
    st.markdown(r'''It's worth noting that when you hear terms like "generative AI" or "generative models" in the media, especially in the context of tools like ChatGPT, this is what they're referring to. These models don't just generate outputs; they've learned to model the statistical properties of the data they're trained on, whether that's text, images, or other types of data.''')
    st.markdown(r'''''')
    st.markdown(r'''## Logistic Regression and the Origin of the Sigmoid Function''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's delve into logistic regression. As mentioned earlier, the term "logistic regression" is somewhat misleading because we're actually performing classification, not regression. We'll start by explaining the origin of the term "logistic" and then focus on binary classification with two classes: 0 and 1.''')
    st.markdown(r'''''')
    st.markdown(r'''The goal in logistic regression is to find a discriminant function - a linear boundary in the feature space that separates class 1 from class 0. You might be tempted to approach this like we did with linear regression, using a simple linear model $y = w^T x$ to predict labels. However, this approach has significant drawbacks in classification tasks.''')
    st.markdown(r'''''')
    st.markdown(r'''To understand why, let's visualize a linear model. In a two-dimensional feature space ($X_1$ and $X_2$), a linear model creates a hyperplane. The direction of this hyperplane is determined by the weights $w$. For instance, if $w = [1, 2]$, meaning $y = 1X_1 + 2X_2$, the hyperplane of constant $y$ values will be perpendicular to the vector $w$.''')
    st.markdown(r'''''')
    st.markdown(r'''This visualization helps illustrate why treating classification as a simple regression task is problematic. Consider a scenario with two classes and two features. The optimal boundary between classes might be represented by a purple line. However, if we treat this as a regression problem, we might end up with a suboptimal solution, represented by a green line.''')
    st.markdown(r'''''')
    st.markdown(r'''Why does this happen? In a regression approach, as we move away from the optimal boundary (purple line), our predicted values might overshoot. For instance, if the purple line represents a value of 0.5, points far from this line might be assigned values like 2 or -1, which don't make sense for binary classification. To minimize error in a regression framework, the model might settle on the green line, which is not the correct classification boundary.''')
    st.markdown(r'''''')
    st.markdown(r'''The fundamental issue is that linear models grow indefinitely in the direction of $w$, which doesn't align with the binary nature of our classification problem.''')
    st.markdown(r'''''')
    st.markdown(r'''To address this, we need a "tapering" function - a function that squeezes all values from negative infinity to positive infinity back into the range [0, 1]. We want all points on one side of our boundary to approach 1, and all points on the other side to approach 0, regardless of how far they are from the boundary.''')
    st.markdown(r'''''')
    st.markdown(r'''This is where the sigmoid function comes into play.''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's explore the origin and significance of the sigmoid function in logistic regression.''')
    st.markdown(r'''''')
    st.markdown(r'''The sigmoid function has a characteristic S-shaped curve. Its analytic form is:''')
    st.markdown(r'''''')
    st.markdown(r'''$\sigma(z) = \dfrac{1}{1 + e^{-z}}$''')
    st.markdown(r'''''')
    st.markdown(r'''This function has a crucial property: it "squashes" any input value to the range between 0 and 1. Values much larger than 1 are mapped close to 1, while values much smaller than -1 are mapped close to 0. This behavior makes it an ideal choice for binary classification.''')
    st.markdown(r'''''')
    st.markdown(r'''But where does this function come from? It's not just an arbitrary choice made by some genius. In fact, the sigmoid function has a deep connection to probability theory and Bayes' theorem.''')
    st.markdown(r'''''')
    st.markdown(r'''The inverse of the sigmoid function is known as the logit function, which is where the term "logistic" in logistic regression originates. The logit function is defined as:''')
    st.markdown(r'''''')
    st.markdown(r'''$\mathrm{logit}(p) = \ln(\dfrac{p}{1-p})$''')
    st.markdown(r'''''')
    st.markdown(r'''where $p$ is a probability.''')
    st.markdown(r'''''')
    st.markdown(r'''To understand the origin of the sigmoid function, let's revisit Bayes' theorem in the context of binary classification. For two classes $C_1$ and $C_2$, we can write:''')
    st.markdown(r'''''')
    st.markdown(r'''$P(C_1 | x) = \dfrac{P(x | C_1) \cdot P(C_1)}{P(x)}$''')
    st.markdown(r'''''')
    st.markdown(r'''We can expand the denominator $P(x)$ as:''')
    st.markdown(r'''''')
    st.markdown(r'''$P(x) = P(x | C_1) \cdot P(C_1) + P(x | C_2) \cdot P(C_2)$''')
    st.markdown(r'''''')
    st.markdown(r'''This is analogous to calculating the probability of an event (like bringing an umbrella) by considering all possible scenarios (raining or not raining).''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's introduce the concept of "odds". In our context, the odds are defined as:''')
    st.markdown(r'''''')
    st.markdown(r'''$\mathrm{odds} = \dfrac{P(x | C_1) \cdot P(C_1)}{P(x | C_2) \cdot P(C_2)}$''')
    st.markdown(r'''''')
    st.markdown(r'''This ratio compares the likelihood of observing $x$ given that it belongs to class $C_1$ versus class $C_2$. In the umbrella example, it would be the ratio of the probability of bringing an umbrella when it rains to the probability of bringing an umbrella when it doesn't rain.''')
    st.markdown(r'''''')
    st.markdown(r'''If we take the logarithm of these odds, we get what's known as the "log odds" or "logit":''')
    st.markdown(r'''''')
    st.markdown(r'''$\mathrm{logit} = \ln(\mathrm{odds}) = \ln(\dfrac{P(x | C_1) \cdot P(C_1)}{P(x | C_2) \cdot P(C_2)})$''')
    st.markdown(r'''''')
    st.markdown(r'''Now, if we manipulate this expression, we can derive the sigmoid function:''')
    st.markdown(r'''''')
    st.markdown(r'''$P(C_1 | x) = \dfrac{1}{1 + e^{-\mathrm{logit}}}$''')
    st.markdown(r'''''')
    st.markdown(r'''This is precisely the form of the sigmoid function!''')
    st.markdown(r'''''')
    st.markdown(r'''So, the sigmoid function naturally arises when we study the log odds of an event in a binary classification problem. It's not an arbitrary choice, but a direct consequence of applying Bayes' theorem to our classification task.''')
    st.markdown(r'''''')
    st.markdown(r'''## Generative Model and Linear Discriminant Function''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's explore the conditions under which our generative model leads to a logistic regression with a linear discriminant function.''')
    st.markdown(r'''''')
    st.markdown(r'''The key assumption is that the distribution of our feature vector $x$, given any class, follows a Gaussian distribution. Moreover, we assume that all classes have the same covariance matrix $\Sigma$, differing only in their mean values. Mathematically, we can express this as:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(x | C_k) = \dfrac{1}{(2\pi)^{D/2} |\Sigma|^{1/2}} \exp\bigg(-\dfrac{1}{2} (x - \mu_k)^T \Sigma^{-1} (x - \mu_k)\bigg)$''')
    st.markdown(r'''''')
    st.markdown(r'''Under this assumption, we can derive that the log-odds function $a(x)$ becomes a linear function of $x$:''')
    st.markdown(r'''''')
    st.markdown(r'''$a(x) = (\mu_1 - \mu_2)^T \Sigma^{-1} x + \dfrac{1}{2} (\mu_2^T \Sigma^{-1} \mu_2 - \mu_1^T \Sigma^{-1} \mu_1) + \ln\bigg(\dfrac{P(C_1)}{P(C_2)}\bigg)$''')
    st.markdown(r'''''')
    st.markdown(r'''This results in the familiar form of logistic regression:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(C_1 | x) = \sigma(w^T x)$''')
    st.markdown(r'''''')
    st.markdown(r'''where $\sigma$ is the sigmoid function and $w$ is a linear combination of the model parameters.''')
    st.markdown(r'''''')
    st.markdown(r'''### Implications and Limitations of Assumptions''')
    st.markdown(r'''''')
    st.markdown(r'''It's important to understand the implications of these assumptions. We're essentially saying that for each class, the data points form a Gaussian "cloud" in the feature space, with these clouds having the same shape (covariance) but centered at different points (means).''')
    st.markdown(r'''''')
    st.markdown(r'''You might wonder if this is a realistic assumption. In many real-world scenarios, different classes might have different covariance structures. For instance, in our flower classification example, it seems unlikely that the distribution of petal lengths and sepal lengths would have the same covariance for all species.''')
    st.markdown(r'''''')
    st.markdown(r'''However, this model remains powerful for several reasons:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Feature engineering: Just as with linear regression, we're not limited to using raw input features. We can apply transformations $\phi(x)$ to our input data. With clever feature engineering, we might be able to transform our data to better fit these assumptions.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Robustness: Even when the assumptions aren't perfectly met, logistic regression often performs well in practice.''')
    st.markdown(r'''''')
    st.markdown(r'''3. Interpretability: The linear nature of the model makes it easy to interpret the importance of different features.''')
    st.markdown(r'''''')
    st.markdown(r'''4. Foundation for more complex models: Understanding logistic regression provides a basis for understanding more advanced techniques.''')
    st.markdown(r'''''')
    st.markdown(r'''If manually engineering features seems daunting, don't worry. In future discussions, we'll explore methods that can automatically learn useful feature representations:''')
    st.markdown(r'''''')
    st.markdown(r'''1. Neural Networks: These can be thought of as learning the feature transformations $\phi(x)$ automatically from the data.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Gaussian Processes: Instead of explicitly defining features, these methods work by defining similarity measures between data points (kernels).''')
    st.markdown(r'''''')
    st.markdown(r'''These approaches extend beyond linear models, providing a richer toolkit for supervised learning tasks.''')
    st.markdown(r'''''')
    st.markdown(r'''Understanding the strengths and limitations of logistic regression is crucial. While it's a powerful tool that can handle many tasks, recognizing its assumptions helps you understand when to apply it and when to consider more complex models. It's like learning to play the piano – mastering the basics allows you to appreciate and effectively use more advanced techniques when needed.''')
    st.markdown(r'''''')
    st.markdown(r'''## Feature Engineering and Deep Learning''')
    st.markdown(r'''''')
    st.markdown(r'''Let's consider another perspective on linear separation in classification problems. Imagine we have two classes plotted in a two-dimensional space defined by features $X_1$ and $X_2$. The distribution of these classes might be such that no linear boundary can perfectly separate them. However, we're not constrained to work only with these original features.''')
    st.markdown(r'''''')
    st.markdown(r'''For instance, if we consider the origin as the center point, we might notice that all points of one class fall within a certain radius, while points of the other class lie beyond this radius. This observation suggests that by transforming our original features $X_1$ and $X_2$ into a new feature space $\phi(x)$, we might be able to achieve linear separation.''')
    st.markdown(r'''''')
    st.markdown(r'''This process of creating new, more informative features is called feature engineering. It has been a crucial aspect of machine learning, especially before the rise of deep learning. In fields like neuroscience, researchers have traditionally relied heavily on domain knowledge to identify relevant features that allow classical models like logistic or linear regression to perform well.''')
    st.markdown(r'''''')
    st.markdown(r'''However, the advent of deep learning has somewhat changed this landscape. With sufficient data and computational power, neural networks can automatically learn useful feature representations. This capability has sparked debates about the necessity of manual feature engineering.''')
    st.markdown(r'''''')
    st.markdown(r'''### The Balance Between Domain Knowledge and Data-Driven Approaches''')
    st.markdown(r'''''')
    st.markdown(r'''For example, there are reports of companies hiring scientists without traditional domain backgrounds, believing that with enough data and computational power, all necessary features can be learned directly from the data. However, this approach might not be universally applicable, especially in fields where data acquisition is often expensive and time-consuming.''')
    st.markdown(r'''''')
    st.markdown(r'''In many scientific disciplines, where data (especially simulations) can be costly to obtain, leveraging domain knowledge for feature engineering is still valuable. As we discussed previously, there's rarely a pure modeling or pure learning approach – the balance depends on data availability. In scenarios with virtually unlimited data, like some language modeling tasks, pure learning approaches can be effective. But with limited data, domain insights become crucial.''')
    st.markdown(r'''''')
    st.markdown(r'''### Advancements in Deep Learning and Generative Models''')
    st.markdown(r'''''')
    st.markdown(r'''An exciting development in recent years has been the ability of deep learning models to understand complex distributions in high-dimensional spaces. This is evident in generative AI models like Midjourney or DALL-E 2, which can create images or text that demonstrate a deep understanding of complex distributions.''')
    st.markdown(r'''''')
    st.markdown(r'''This capability is opening up new possibilities in various scientific fields. Traditionally, we've been limited to feature engineering, but now we can potentially work with more complex, learned generative models. This approach, known as simulation-based inference (SBI) or previously as likelihood-free inference (LFI), is becoming increasingly important in scientific research. This is a topic that we will study in the future when we discuss neural networks.''')
    st.markdown(r'''''')
    st.markdown(r'''It's worth noting that the term "likelihood-free" is somewhat misleading – we're still working within a Bayesian framework and thus still have a likelihood. The key difference is that we don't need to explicitly parametrize this likelihood.''')
    st.markdown(r'''''')
    st.markdown(r'''## Objective Function for Logistic Regression''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's return to logistic regression and focus on defining the objective function we need to optimize. As always, our foundation is Bayes' theorem.''')
    st.markdown(r'''''')
    st.markdown(r'''Our goal is to find the best decision boundary. In Bayesian terms, we're looking for the boundary that maximizes the likelihood of observing our data. We don't know the correct boundary a priori, but we can propose different boundaries and see which one best explains our observations.''')
    st.markdown(r'''''')
    st.markdown(r'''What do we observe? We have our input features $X$ and our ground truth labels $T$ from our training data. We want to find the best weights $w$ that maximize the probability of observing these $(X, T)$ pairs.''')
    st.markdown(r'''''')
    st.markdown(r'''Let's break this down:''')
    st.markdown(r'''''')
    st.markdown(r'''1. If $t_n = 1$ (the nth object belongs to class 1), the probability of observing $x_n$ given it belongs to class 1 is:''')
    st.markdown(r'''   $p(C_1 | x_n) = \sigma(w^T x_n)$''')
    st.markdown(r'''''')
    st.markdown(r'''2. If $t_n = 0$ (the nth object belongs to class 2), the probability of observing $x_n$ given it belongs to class 2 is:''')
    st.markdown(r'''   $p(C_2 | x_n) = 1 - \sigma(w^T x_n)$''')
    st.markdown(r'''''')
    st.markdown(r'''Assuming all data points are independent, the joint likelihood of observing all the data is the product of individual likelihoods. We can write this compactly as our objective function:''')
    st.markdown(r'''''')
    st.markdown(r'''$L = \prod_{n=1}^N y_n^{t_n} (1 - y_n)^{1-t_n}$''')
    st.markdown(r'''''')
    st.markdown(r'''where $y_n \equiv \sigma(w^T x_n)$''')
    st.markdown(r'''''')
    st.markdown(r'''This elegant formulation automatically handles both classes: when $t_n = 1$, it uses $y_n$, and when $t_n = 0$, it uses $(1 - y_n)$.''')
    st.markdown(r'''''')
    st.markdown(r'''### Cross-Entropy Loss Function''')
    st.markdown(r'''''')
    st.markdown(r'''To make this easier to work with, we typically take the natural logarithm of this likelihood, which gives us the cross-entropy loss function:''')
    st.markdown(r'''''')
    st.markdown(r'''$E(w) = -\ln L = -\sum_{n=1}^N [t_n \ln y_n + (1 - t_n) \ln(1 - y_n)]$''')
    st.markdown(r'''''')
    st.markdown(r'''This cross-entropy function is fundamental in machine learning, particularly for classification tasks.''')
    st.markdown(r'''''')
    st.markdown(r'''The function $y_n$ is actually a representation of $\sigma(w^T x_n)$, which is a nonlinear function. Consequently, the entire loss function is nonlinear and likely non-convex (though this requires verification). This nonlinearity means we cannot derive an analytic solution for the optimal point of the loss function.''')
    st.markdown(r'''''')
    st.markdown(r'''## Optimization Techniques''')
    st.markdown(r'''''')
    st.markdown(r'''To find a solution, we must employ numerical optimization techniques. While using optimization libraries like SciPy is a pragmatic approach, understanding the underlying principles, particularly gradient descent, is valuable. SciPy itself utilizes gradient descent in some of its algorithms.''')
    st.markdown(r'''''')
    st.markdown(r'''### Gradient Descent''')
    st.markdown(r'''''')
    st.markdown(r'''Gradient descent is an iterative optimization algorithm. When an analytic solution is unavailable, we can use gradient descent to find the optimal point of a function. The method involves proposing an initial $w$ and then iteratively moving in the negative direction of the gradient. This process leads to a local minimum, which may or may not be the global minimum.''')
    st.markdown(r'''''')
    st.markdown(r'''Visually, gradient descent can be likened to descending a hill. Starting from any point on a function $f(x)$, we calculate $df/dx$ and take steps in the negative direction of this gradient. The step size, denoted by the hyperparameter $\eta$ (eta), requires careful tuning. An excessively large step size may cause overshooting, while a too small step size will result in slow convergence.''')
    st.markdown(r'''''')
    st.markdown(r'''### Stochastic Gradient Descent (SGD)''')
    st.markdown(r'''''')
    st.markdown(r'''A challenge arises when dealing with large datasets. If our loss function involves summing over billions of data points, each step in gradient descent becomes computationally expensive. This is where stochastic gradient descent (SGD) proves beneficial.''')
    st.markdown(r'''''')
    st.markdown(r'''SGD estimates the gradient using a randomly chosen subset of the data, assuming this subset provides a representative gradient of the loss function. This approach significantly reduces computation time and allows for more frequent updates to the parameters.''')
    st.markdown(r'''''')
    st.markdown(r'''Moreover, SGD introduces beneficial randomness into the optimization process. This stochasticity can help the algorithm escape local minima, as the estimated gradient may vary slightly for different subsets of data.''')
    st.markdown(r'''''')
    st.markdown(r'''### Analytic Gradient for Logistic Regression''')
    st.markdown(r'''''')
    st.markdown(r'''For logistic regression, there's an additional advantage. Consider our objective function:''')
    st.markdown(r'''''')
    st.markdown(r'''$E(w) = -\sum_{n=1}^N [t_n \ln y_n + (1 - t_n) \ln(1 - y_n)]$''')
    st.markdown(r'''''')
    st.markdown(r'''While the optimal point for $w$ lacks an analytic solution, the gradient $dE/dw$ is analytically derivable. The $w$ is implicit in $y_n$, which is $\sigma(w^T x_n)$. Applying the chain rule to $dE/dw$ involves $d\sigma/dw$, which simplifies to:''')
    st.markdown(r'''''')
    st.markdown(r'''$\dfrac{d\sigma}{dw} = \sigma(1 - \sigma)$''')
    st.markdown(r'''''')
    st.markdown(r'''Through calculus, we arrive at an analytic form for the gradient:''')
    st.markdown(r'''''')
    st.markdown(r'''$\nabla E(w) = \sum_{n=1}^N (y_n - t_n)x_n$''')
    st.markdown(r'''''')
    st.markdown(r'''This gradient depends only on $x$, $t$, and $y$, values that are already necessary for other calculations. Thus, we obtain the gradient without additional computational cost.''')
    st.markdown(r'''''')
    st.markdown(r'''While this analytic gradient might seem trivial now, its significance becomes apparent in the context of neural networks and automatic differentiation, topics we'll explore in future discussions. In those cases, all functions are differentiable, but gradients are computed algorithmically. For logistic regression, however, we have this convenient closed-form solution.''')
    st.markdown(r'''''')
    st.markdown(r'''''')
    st.markdown(r'''## Multi-Class Logistic Regression''')
    st.markdown(r'''''')
    st.markdown(r'''Now, let's discuss extending logistic regression to multiple classes.''')
    st.markdown(r'''''')
    st.markdown(r'''In many scenarios, we have more than two classes. We use one-hot encoding because there's no natural ordering of classes. Instead of encoding classes as a scalar, we use a vector with the same dimension as the number of classes to denote the probability of belonging to each class.''')
    st.markdown(r'''''')
    st.markdown(r'''The extension from binary to multi-class is straightforward. Remember how we derived the sigmoid function using Bayes' theorem? For multiple classes, it's similar, but we sum over all classes $j$ instead of just two:''')
    st.markdown(r'''''')
    st.markdown(r'''$p(C_k | x) = \dfrac{p(x | C_k) p(C_k)}{\sum_j p(x | C_j) p(C_j)} \equiv \dfrac{\exp(a_k)}{\sum_j \exp(a_j)}$''')
    st.markdown(r'''''')
    st.markdown(r'''where $a_k \equiv \ln(p(x | C_k)p(C_k)) \equiv w_k^T x$''')
    st.markdown(r'''''')
    st.markdown(r'''We define $a_k$ as the log-likelihood of observing $x$ given class $C_k$. This is what we want to optimize. Unlike binary classification where we only needed one decision boundary, in multi-class problems, we need multiple boundaries. For instance, with three classes, we might need three lines to separate them (like a Mercedes-Benz logo).''')
    st.markdown(r'''''')
    st.markdown(r'''The degree of freedom increases from $n$ (features) to $k * n$, where $k$ is the number of classes. Each class has its own weight vector $w_k$.''')
    st.markdown(r'''''')
    st.markdown(r'''### The Softmax Function''')
    st.markdown(r'''''')
    st.markdown(r'''This leads us to the softmax function. The term "softmax" is one of many jargon terms in machine learning. It's important to understand these terms, but also to recognize that sometimes they're just fancy names for simple concepts.''')
    st.markdown(r'''''')
    st.markdown(r'''The softmax function has a nice property: the probabilities for all classes sum to 1, allowing us to interpret the output as probabilities of belonging to each class.''')
    st.markdown(r'''''')
    st.markdown(r'''For each example $i$ and class $k$, we have an output $y_{ik}$, which represents the probability that example $i$ belongs to class $k$:''')
    st.markdown(r'''''')
    st.markdown(r'''$y_{ik} = \dfrac{\exp(a_k(x_i, w_k))}{\sum_j \exp(a_j(x_i, w_j))}$''')
    st.markdown(r'''''')
    st.markdown(r'''Our loss function becomes:''')
    st.markdown(r'''''')
    st.markdown(r'''$E(w_1, w_2, ..., w_K) = -\dfrac{1}{N} \sum_{i=1}^N \sum_{k=1}^K t_{ik} \log(y_{ik})$''')
    st.markdown(r'''''')
    st.markdown(r'''Remember, for any $t_i$, there's only one value of $k$ where this is non-zero because of one-hot encoding. The likelihood that an example belongs to $t_{ik}$ is exactly $y_{ik}$.''')
    st.markdown(r'''''')
    st.markdown(r'''This is a natural extension of the binary case. If this seems unclear, think about how it relates to the binary case and how it extends to multiple classes.''')
    st.markdown(r'''''')
    st.markdown(r'''The optimization process for multi-class logistic regression is similar to the binary case, but now we're optimizing multiple weight vectors simultaneously. We can still use techniques like stochastic gradient descent, just with a more complex gradient calculation.''')
    st.markdown(r'''''')
    st.markdown(r'''### Interpreting Multi-Class Output''')
    st.markdown(r'''''')
    st.markdown(r'''One important point to consider is how we interpret the output $y_{ik}$. This is a vector of $K$ dimensions that can take any values, as long as they sum to one. For example, even if the ground truth label is $[0, 1, 0, 0, 0, 0, 0]$, $y_{ik}$ might be $[0.95, 0.05, 0, 0, 0, 0, 0]$.''')
    st.markdown(r'''''')
    st.markdown(r'''The simplest way to classify an object is to take the maximum value of $y_{ik}$. In this example, we'd classify it as belonging to the first class. However, this approach isn't always ideal, especially when we need to be more conservative in our predictions.''')
    st.markdown(r'''''')
    st.markdown(r'''#### Dealing with Uncertainty in Classification''')
    st.markdown(r'''''')
    st.markdown(r'''Consider a medical scenario: if your doctor says you have a 95% chance of not having cancer and a 5% chance of having cancer, you might not want to simply declare yourself cancer-free. In such cases, we can't tolerate false negatives.''')
    st.markdown(r'''''')
    st.markdown(r'''So, how we project $y_{ik}$ into classes is a decision we must make carefully. It often depends on what we call the confusion matrix, which shows how our predicted classes compare to the ground truth labels on a test set.''')
    st.markdown(r'''''')
    st.markdown(r'''In some cases, we can't tolerate false positives, while in others, we can't tolerate false negatives. This boundary depends on the specific problem.''')
    st.markdown(r'''''')
    st.markdown(r'''#### Addressing Class Imbalance''')
    st.markdown(r'''''')
    st.markdown(r'''Another consideration is that your training sample might be skewed. For instance, in cancer detection, there are typically many more non-cancerous cases than cancerous ones. The likelihood function treats all samples democratically, which might lead to favoring the majority class. This could result in more false negatives for the minority class.''')
    st.markdown(r'''''')
    st.markdown(r'''To address this, we can tweak the loss function. For rare classes, we might want to give higher weight to their samples:''')
    st.markdown(r'''''')
    st.markdown(r'''$E(w_1, w_2, ..., w_K) = -\dfrac{1}{N} \sum_{i=1}^N \sum_{k=1}^K \mathrm{weight}_i * t_{ik} \log(y_{ik})$''')
    st.markdown(r'''''')
    st.markdown(r'''This weighting is more of an art than a science, depending on the decision you want to make.''')
    st.markdown(r'''''')
    st.markdown(r'''''')
    st.markdown(r'''Another approach is to introduce an "abstain" option. In some cases, you might say, "I don't know if you're cancerous or not. Do another test." This is another way to deal with uncertainty at the decision boundary.''')
    st.markdown(r'''''')
    st.markdown(r'''This is still an active area of research. For example, there are papers introducing concepts like "conservative prediction" or "confidence minimization," suggesting that we should only make predictions when we're very confident.''')
    st.markdown(r'''''')
    st.markdown(r'''### Summary''')
    st.markdown(r'''''')
    st.markdown(r'''To summarize what we've learned today:''')
    st.markdown(r'''''')
    st.markdown(r'''1. We extended linear models from regression to classification, introducing logistic regression.''')
    st.markdown(r'''''')
    st.markdown(r'''2. Logistic regression serves as a great example of the interplay between generative and discriminative models.''')
    st.markdown(r'''''')
    st.markdown(r'''3. We saw that logistic regression, while a discriminative model, is equivalent to a generative model where all classes have the same covariance.''')
    st.markdown(r'''''')
    st.markdown(r'''4. We discussed the origin of the sigmoid function from log odds and Bayes' theorem:''')
    st.markdown(r'''''')
    st.markdown(r'''   $\sigma(z) = \dfrac{1}{1 + e^{-z}}$''')
    st.markdown(r'''''')
    st.markdown(r'''5. We derived the cross-entropy loss function from the principle of maximizing the likelihood of our observations:''')
    st.markdown(r'''''')
    st.markdown(r'''   $E(w) = -\sum_{n=1}^N [t_n \ln y_n + (1 - t_n) \ln(1 - y_n)]$''')
    st.markdown(r'''''')
    st.markdown(r'''6. Due to the non-linearity introduced by the sigmoid function, we discussed numerical optimization techniques like gradient descent.''')
    st.markdown(r'''''')
    st.markdown(r'''7. Finally, we extended these concepts to multi-class classification, introducing the softmax function:''')
    st.markdown(r'''''')
    st.markdown(r'''   $y_{ik} = \dfrac{\exp(a_k(x_i, w_k))}{\sum_j \exp(a_j(x_i, w_j))}$''')
    st.markdown(r'''''')
    st.markdown(r'''Remember, while logistic regression is a powerful tool, it's important to understand its assumptions and limitations. In many cases, it can serve as an excellent starting point for your analysis, providing interpretable results and a foundation for more complex models if needed.''')
    st.markdown(r'''''')
    st.markdown(r'''In future discussions, we'll delve into unsupervised learning techniques, exploring how we can find patterns and structure in data without explicit labels. This will include topics like clustering and dimensionality reduction, which are crucial in many scientific applications.''')
    st.markdown(r'''''')

if __name__ == '__main__':
    show_page()